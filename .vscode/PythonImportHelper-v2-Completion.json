[
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "feature_engineer",
        "importPath": "feature_engineering",
        "description": "feature_engineering",
        "isExtraImport": true,
        "detail": "feature_engineering",
        "documentation": {}
    },
    {
        "label": "remove_ocd_meds",
        "importPath": "feature_engineering",
        "description": "feature_engineering",
        "isExtraImport": true,
        "detail": "feature_engineering",
        "documentation": {}
    },
    {
        "label": "run_feature_engineering",
        "importPath": "feature_engineering",
        "description": "feature_engineering",
        "isExtraImport": true,
        "detail": "feature_engineering",
        "documentation": {}
    },
    {
        "label": "listofknown_medications",
        "importPath": "word_lists",
        "description": "word_lists",
        "isExtraImport": true,
        "detail": "word_lists",
        "documentation": {}
    },
    {
        "label": "conditions",
        "importPath": "word_lists",
        "description": "word_lists",
        "isExtraImport": true,
        "detail": "word_lists",
        "documentation": {}
    },
    {
        "label": "stop",
        "importPath": "word_lists",
        "description": "word_lists",
        "isExtraImport": true,
        "detail": "word_lists",
        "documentation": {}
    },
    {
        "label": "biasing_terms",
        "importPath": "word_lists",
        "description": "word_lists",
        "isExtraImport": true,
        "detail": "word_lists",
        "documentation": {}
    },
    {
        "label": "biasing_terms",
        "importPath": "word_lists",
        "description": "word_lists",
        "isExtraImport": true,
        "detail": "word_lists",
        "documentation": {}
    },
    {
        "label": "stop",
        "importPath": "word_lists",
        "description": "word_lists",
        "isExtraImport": true,
        "detail": "word_lists",
        "documentation": {}
    },
    {
        "label": "listofknown_medications",
        "importPath": "word_lists",
        "description": "word_lists",
        "isExtraImport": true,
        "detail": "word_lists",
        "documentation": {}
    },
    {
        "label": "run_preprocess_data",
        "importPath": "preprocessing",
        "description": "preprocessing",
        "isExtraImport": true,
        "detail": "preprocessing",
        "documentation": {}
    },
    {
        "label": "run_modeling",
        "importPath": "modeling",
        "description": "modeling",
        "isExtraImport": true,
        "detail": "modeling",
        "documentation": {}
    },
    {
        "label": "stop",
        "importPath": "modeling",
        "description": "modeling",
        "isExtraImport": true,
        "detail": "modeling",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "CountVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "DecisionTreeClassifier",
        "importPath": "sklearn.tree",
        "description": "sklearn.tree",
        "isExtraImport": true,
        "detail": "sklearn.tree",
        "documentation": {}
    },
    {
        "label": "RandomForestClassifier",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "AdaBoostClassifier",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "GradientBoostingClassifier",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "GridSearchCV",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "GridSearchCV",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "cross_val_score",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "LogisticRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "plot_roc_curve",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "sklearn.pipeline",
        "description": "sklearn.pipeline",
        "isExtraImport": true,
        "detail": "sklearn.pipeline",
        "documentation": {}
    },
    {
        "label": "XGBClassifier",
        "importPath": "xgboost",
        "description": "xgboost",
        "isExtraImport": true,
        "detail": "xgboost",
        "documentation": {}
    },
    {
        "label": "nbformat",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nbformat",
        "description": "nbformat",
        "detail": "nbformat",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "scripts.data_exploration",
        "description": "scripts.data_exploration",
        "peekOfCode": "df = pd.read_csv(\"data/reddit_threads.csv\") # The combined data\n# # Explore the data\n# 1. Top Bigrams in the post selftext for r/Autism\n# 2. Top Bigrams in the post selftext for r/OCD\n# 3. Top Trigrams in the post selftext for r/Autism\n# 4. Top Trigrams in the post selftext for r/OCD\n# 5. Top Words in the post selftext for r/Autism\n# 6. Top Words in the post selftext for r/OCD\n# 7. Top Users posting on r/Autism in the data\n# 8. Top Users posting on r/OCD in the data",
        "detail": "scripts.data_exploration",
        "documentation": {}
    },
    {
        "label": "remove_ocd_meds",
        "kind": 2,
        "importPath": "scripts.feature_engineering",
        "description": "scripts.feature_engineering",
        "peekOfCode": "def remove_ocd_meds(text,listofknown_medications):\n    global meds\n    if len(meds)> 0:\n        log_string = f'The latest medication mentioned is: {meds[-1]}'\n        logging.info(log_string)\n    wordsintext = text.split(' ')\n    for word in wordsintext:\n        if word in listofknown_medications:\n            meds.append(word)\n            text = text.replace(word,' ')",
        "detail": "scripts.feature_engineering",
        "documentation": {}
    },
    {
        "label": "feature_engineer",
        "kind": 2,
        "importPath": "scripts.feature_engineering",
        "description": "scripts.feature_engineering",
        "peekOfCode": "def feature_engineer(df,listofknown_medications):\n    \"\"\"\n    summary: This function takes in a dataframe and a list of known medications and returns a dataframe with new features.\n    List of Features that will be created:\n    1. Number of words in the post (word_count)\n    2. Number of unique words in the post (unique_words)\n    2. Length of the post (number of characters)\n    4. unique words in the post (unique_word_count)\n    5. If the selftext of the post contains a known medication (medication_mentioned) (list of the meds mentioned)\n    Args:",
        "detail": "scripts.feature_engineering",
        "documentation": {}
    },
    {
        "label": "binarize_target_feature",
        "kind": 2,
        "importPath": "scripts.feature_engineering",
        "description": "scripts.feature_engineering",
        "peekOfCode": "def binarize_target_feature(df):\n    df[\"is_autism\"] = df[\"subreddit\"].apply(lambda x: 1 if x == \"autism\" else 0)\n    #? Drop the 'subreddit' column\n    #note: not sure if the subreddit column is needed anymore but I will keep it for now.\n    return df\ndef run_feature_engineering(df):\n    #& Importing the Data and Engineering Features\n    df = pd.read_csv('./data/reddit_threads.csv')\n    logging.info(f'Beginning feature engineering...')\n    # Run the functions in this script to create new features for the model.",
        "detail": "scripts.feature_engineering",
        "documentation": {}
    },
    {
        "label": "run_feature_engineering",
        "kind": 2,
        "importPath": "scripts.feature_engineering",
        "description": "scripts.feature_engineering",
        "peekOfCode": "def run_feature_engineering(df):\n    #& Importing the Data and Engineering Features\n    df = pd.read_csv('./data/reddit_threads.csv')\n    logging.info(f'Beginning feature engineering...')\n    # Run the functions in this script to create new features for the model.\n    #& binarize the target feature\n    df = binarize_target_feature(df) # binarize the target feature (i.e. 'subreddit')\n    #& create new features\n    df = feature_engineer(df,listofknown_medications)\n    # save the dataframe with the new features to a csv file 'df_after_feature_engineering.csv' in the data folder.",
        "detail": "scripts.feature_engineering",
        "documentation": {}
    },
    {
        "label": "OCD_Posts_With_Meds",
        "kind": 5,
        "importPath": "scripts.feature_engineering",
        "description": "scripts.feature_engineering",
        "peekOfCode": "OCD_Posts_With_Meds = 0 # initialized count of posts mentioning meds from the OCD subreddit to zero.\nAutism_Posts_With_Meds = 0 # initialized count of posts mentioning meds from the Autism subreddit to zero.\nTotal_Posts_With_Meds = 0 # the total number of posts that mention medications.\nmedications_mentioned = [] # the list of all medications that are mentioned in the posts.\n# import the lists of medications and conditions from word_lists.py\nfrom word_lists import listofknown_medications, conditions\nimport pandas as pd\nimport numpy as np\n# Logging Setup\nimport logging",
        "detail": "scripts.feature_engineering",
        "documentation": {}
    },
    {
        "label": "Autism_Posts_With_Meds",
        "kind": 5,
        "importPath": "scripts.feature_engineering",
        "description": "scripts.feature_engineering",
        "peekOfCode": "Autism_Posts_With_Meds = 0 # initialized count of posts mentioning meds from the Autism subreddit to zero.\nTotal_Posts_With_Meds = 0 # the total number of posts that mention medications.\nmedications_mentioned = [] # the list of all medications that are mentioned in the posts.\n# import the lists of medications and conditions from word_lists.py\nfrom word_lists import listofknown_medications, conditions\nimport pandas as pd\nimport numpy as np\n# Logging Setup\nimport logging\n# set the logfile to be 'logs/feature_engineering.log'",
        "detail": "scripts.feature_engineering",
        "documentation": {}
    },
    {
        "label": "Total_Posts_With_Meds",
        "kind": 5,
        "importPath": "scripts.feature_engineering",
        "description": "scripts.feature_engineering",
        "peekOfCode": "Total_Posts_With_Meds = 0 # the total number of posts that mention medications.\nmedications_mentioned = [] # the list of all medications that are mentioned in the posts.\n# import the lists of medications and conditions from word_lists.py\nfrom word_lists import listofknown_medications, conditions\nimport pandas as pd\nimport numpy as np\n# Logging Setup\nimport logging\n# set the logfile to be 'logs/feature_engineering.log'\nlogging.basicConfig(filename='logs/feature_engineering.log')",
        "detail": "scripts.feature_engineering",
        "documentation": {}
    },
    {
        "label": "medications_mentioned",
        "kind": 5,
        "importPath": "scripts.feature_engineering",
        "description": "scripts.feature_engineering",
        "peekOfCode": "medications_mentioned = [] # the list of all medications that are mentioned in the posts.\n# import the lists of medications and conditions from word_lists.py\nfrom word_lists import listofknown_medications, conditions\nimport pandas as pd\nimport numpy as np\n# Logging Setup\nimport logging\n# set the logfile to be 'logs/feature_engineering.log'\nlogging.basicConfig(filename='logs/feature_engineering.log')\ndef remove_ocd_meds(text,listofknown_medications):",
        "detail": "scripts.feature_engineering",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "df = pd.read_csv(\"data/reddit_threads.csv\") # The combined data\nprint(f'Completed Step One. The shape of the data is {df.shape}')\n#& Step Two. Preprocess the data\ndf_preprocessed = run_preprocess_data(df)\nprint(f'Completed Step Two. The shape of the data is {df_preprocessed.shape}')\n#& Step Three. Run the feature engineering functions on the data\n# Run the feature engineering script\n# selftext is converted to string now.\ndf = run_feature_engineering(df_preprocessed)\nprint(f'Completed Step Three. The shape of the data is {df.shape}')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "df_preprocessed",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "df_preprocessed = run_preprocess_data(df)\nprint(f'Completed Step Two. The shape of the data is {df_preprocessed.shape}')\n#& Step Three. Run the feature engineering functions on the data\n# Run the feature engineering script\n# selftext is converted to string now.\ndf = run_feature_engineering(df_preprocessed)\nprint(f'Completed Step Three. The shape of the data is {df.shape}')\n#& Step Four. Run the modeling script to test the models\n# Run the modeling script\nrun_modeling(df)",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "scripts.main",
        "description": "scripts.main",
        "peekOfCode": "df = run_feature_engineering(df_preprocessed)\nprint(f'Completed Step Three. The shape of the data is {df.shape}')\n#& Step Four. Run the modeling script to test the models\n# Run the modeling script\nrun_modeling(df)\nprint(f'Completed Step Four. The shape of the data is {df.shape}, and we generated our model results.')",
        "detail": "scripts.main",
        "documentation": {}
    },
    {
        "label": "my_models",
        "kind": 6,
        "importPath": "scripts.modeling",
        "description": "scripts.modeling",
        "peekOfCode": "class my_models:\n    def __init__(self, df):\n        # params grid for logistic regression.\n        self.params_logreg = {\n            \"cvec__max_features\": [3000,4000,5000],\n            \"cvec__min_df\": [2,3,4],\n            \"cvec__max_df\": [0.9,1,0.1],\n            \"cvec__ngram_range\": [(1, 1)],\n            \"logreg__penalty\": [\"l1\",\"l2\"],\n            \"logreg__C\": [1, 2, 3],",
        "detail": "scripts.modeling",
        "documentation": {}
    },
    {
        "label": "run_modeling",
        "kind": 2,
        "importPath": "scripts.modeling",
        "description": "scripts.modeling",
        "peekOfCode": "def run_modeling():\n    \"\"\"\n    Problem Statement:\n    A wealthy donor with a track record of philanthropic contributions to both Autism and OCD research organizations contacted our organization, asking for a model that they can utilize to identify post characteristics on Reddit.\n    The purposes of this study (towards those ends) are to:\n    1) Use Pushshift API to scrape Reddit posts from the Autism and OCD subreddits.\n    2) To build a predictive model that can accurately predict whether a post is from the Autism or OCD subreddit\n    To accomplish these goals, we hypothesize that count vectorization, and Logistic Regression, Adaboost, or Decision Trees can be used to build a model that accurately can predict whether a post is from the Autism or OCD subreddit. Success in this study would mean that our model has a misclassification rate of less than 10 percent and an accuracy score of greater than 90 percent on the test data set.\n    \"\"\"\n    # Control Flow:",
        "detail": "scripts.modeling",
        "documentation": {}
    },
    {
        "label": "process_dataframe",
        "kind": 2,
        "importPath": "scripts.preprocessing",
        "description": "scripts.preprocessing",
        "peekOfCode": "def process_dataframe(df):\n    # Quick Eliminations\n    if \"selftext\" in df.columns:\n        df = df[\n            [\"title\", \"selftext\", \"subreddit\", \"author\", \"created_utc\"]\n        ]  # Eliminate the Unused Columns\n    else:  # selftext has already been renamed to selftext\n        df = df[[\"title\", \"selftext\", \"subreddit\", \"author\", \"created_utc\"]]\n    # Rename 'selftext' to 'selftext'\n    df = df.rename(columns={\"selftext\": \"selftext\"})",
        "detail": "scripts.preprocessing",
        "documentation": {}
    },
    {
        "label": "remove_overly_biasing_terms",
        "kind": 2,
        "importPath": "scripts.preprocessing",
        "description": "scripts.preprocessing",
        "peekOfCode": "def remove_overly_biasing_terms(df):\n    # adapted from source: https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n    # remove the overly biasing terms from the selftext in the dataframe\n    df[\"selftext\"] = df[\"selftext\"].apply(\n        lambda x: \" \".join(x for x in x.split() if x not in biasing_terms)\n    )\n    return df\ndef merge_text(df):\n    \"\"\"\n    Merges text in the title and selftext columns into the selftext column. This is done to increase the amount of text in the selftext column and eliminate the issues that could come up with title-heavy posts.",
        "detail": "scripts.preprocessing",
        "documentation": {}
    },
    {
        "label": "merge_text",
        "kind": 2,
        "importPath": "scripts.preprocessing",
        "description": "scripts.preprocessing",
        "peekOfCode": "def merge_text(df):\n    \"\"\"\n    Merges text in the title and selftext columns into the selftext column. This is done to increase the amount of text in the selftext column and eliminate the issues that could come up with title-heavy posts.\n    Args:\n        df (dataframe): The dataframe to be processed.\n    Raises:\n        Exception: If the dataframe does not have a title and selftext column.\n    Returns:\n        df: The processed dataframe.\n    \"\"\"",
        "detail": "scripts.preprocessing",
        "documentation": {}
    },
    {
        "label": "preprocessing_function",
        "kind": 2,
        "importPath": "scripts.preprocessing",
        "description": "scripts.preprocessing",
        "peekOfCode": "def preprocessing_function(df):\n    \"\"\"\n    Run the process_dataframe and merge_text functions on the dataframe.\n    Args:\n        df (dataframe): The dataframe to be processed.\n    Returns:\n        dataframe: The processed dataframe.\n    \"\"\"\n    df = merge_text(df)  # introduce new features\n    df = process_dataframe(df)  # process the dataframe",
        "detail": "scripts.preprocessing",
        "documentation": {}
    },
    {
        "label": "data_exploration",
        "kind": 2,
        "importPath": "scripts.preprocessing",
        "description": "scripts.preprocessing",
        "peekOfCode": "def data_exploration(df):\n    \"\"\"\n    Perform data exploration on the dataframe.\n    Args:\n        df (dataframe): The dataframe to be explored.\n    \"\"\"\n    # We want to explore the data to see if there is anything we can do to improve the model.\n    # We want to see if there is any correlation between the length of the title and the length of the selftext\n    # and the subreddit that the post is from.\n    sns.scatterplot(x=\"title_length\", y=\"selftext_length\", hue=\"is_autism\", data=df)",
        "detail": "scripts.preprocessing",
        "documentation": {}
    },
    {
        "label": "run_preprocess_data",
        "kind": 2,
        "importPath": "scripts.preprocessing",
        "description": "scripts.preprocessing",
        "peekOfCode": "def run_preprocess_data(df):\n    # ^ File I/O\n    # If the df_cleaned.csv file has not been generated yet, then we want to generate it with the preprocessing_function.\n    # If the df_cleaned.csv file has been generated, then we want to read it in and use it for the model.\n    if not os.path.isfile(\"data/df_cleaned.csv\"):\n        logging.info(\"The df_cleaned.csv file has not been generated yet, so we are generating it now.\")\n        # Create df by concatenating the dataframes from the two subreddits\n        #& Reading in both Reddit threads as csv files.\n        df1 = pd.read_csv(\"./data/ocd_thread.csv\")  # read in the ocd threads\n        df2 = pd.read_csv(\"./data/autism_thread.csv\")  # read in the autism threads",
        "detail": "scripts.preprocessing",
        "documentation": {}
    },
    {
        "label": "parse_table_of_contents",
        "kind": 2,
        "importPath": "scripts.report_generator_from_readme",
        "description": "scripts.report_generator_from_readme",
        "peekOfCode": "def parse_table_of_contents(readme_text):\n    \"\"\"\n    parse_table_of_contents takes a string of text and returns a list of tuples\n    Parameters\n    :param readme_text: a string of text\n    :type readme_text: str\n    :return: a list of tuples\n    :rtype: list\n    \"\"\"\n    # parse the table of contents from the readme.md file",
        "detail": "scripts.report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "startup",
        "kind": 2,
        "importPath": "scripts.report_generator_from_readme",
        "description": "scripts.report_generator_from_readme",
        "peekOfCode": "def startup():\n    # read the readme.md file\n    with open(path, \"r\") as f:\n        readme_text = f.read()\n    # parse the table of contents\n    table_of_contents = parse_table_of_contents(readme_text)\n    return table_of_contents, readme_text\ndef process_flow_controller():\n    # from start to finish, examine the readme and end with a populated, fully functional report jupyter notebook that can be tweaked and then presented to the end-user as the final report.\n    table_of_contents, readme_text = startup() # read the readme.md file and parse the table of contents",
        "detail": "scripts.report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "process_flow_controller",
        "kind": 2,
        "importPath": "scripts.report_generator_from_readme",
        "description": "scripts.report_generator_from_readme",
        "peekOfCode": "def process_flow_controller():\n    # from start to finish, examine the readme and end with a populated, fully functional report jupyter notebook that can be tweaked and then presented to the end-user as the final report.\n    table_of_contents, readme_text = startup() # read the readme.md file and parse the table of contents\n    # using the table of contents, create the sections of the report notebook\n    generate_report_notebook(table_of_contents,readme_text)\n    return\n# What are the expected sections in a data science report notebook?\n# The answer is:\n# 1. Introduction\n# 2. Table of Contents",
        "detail": "scripts.report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "create_data_section",
        "kind": 2,
        "importPath": "scripts.report_generator_from_readme",
        "description": "scripts.report_generator_from_readme",
        "peekOfCode": "def create_data_section(report_notebook):\n    global readme_text\n    # generate the data section of the report notebook\n    # the data section is a markdown cell with the text from the readme.md file for the Data Section.\n    # This section is about explaining what kinds of data are in the dataset and how the data was collected.\n    # find the line that starts with \"Data\" and\n    return\n# A markdown cell for the Data Cleaning Section (with the text from the readme.md file for the Data Cleaning Section).\n# This section is about explaining how the data was cleaned.\n# what kinds of dirtiness was found in the data and how it was cleaned?",
        "detail": "scripts.report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "get_section_text",
        "kind": 2,
        "importPath": "scripts.report_generator_from_readme",
        "description": "scripts.report_generator_from_readme",
        "peekOfCode": "def get_section_text(section_name, markdown_text):\n    \"\"\"\n    get_section_text takes a section name and a string of markdown text and returns a string of text\n    Parameters\n    :param section_name: a string of text\n    :type section_name: str\n    :param markdown_text: a string of text\n    :type markdown_text: str\n    :return: a string of text\n    :rtype: str",
        "detail": "scripts.report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "pandify_readme",
        "kind": 2,
        "importPath": "scripts.report_generator_from_readme",
        "description": "scripts.report_generator_from_readme",
        "peekOfCode": "def pandify_readme(readme_text, table_of_contents):\n    # split the readme into sections by their headernames and put those into a pandas dataframe\n    # the dataframe has two columns: section_name, section_text\n    # section_name is a string\n    # section_text is a string\n    readme_df = pd.DataFrame(columns=[\"section_name\", \"section_text\"])\n    # take the table of contents and make a list of section names\n    section_names = [section[0] for section in table_of_contents]\n    # populate the dataframe with the section names\n    readme_df[\"section_name\"] = section_names",
        "detail": "scripts.report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "generate_report_notebook",
        "kind": 2,
        "importPath": "scripts.report_generator_from_readme",
        "description": "scripts.report_generator_from_readme",
        "peekOfCode": "def generate_report_notebook(table_of_contents,readme_text):\n    # global readme_text\n    # generate a jupyter notebook based on the table of contents in the readme.md file.\n    # the pattern it uses is:\n    # A header markdown cell with the project name\n    # For each section in the table of contents:\n    #   A markdown cell with the section name\n    #   A markdown cell with the text from the readme.md file for that section\n    #   add a code cell below this markdown cell for the user to add any code they want to the section.\n    # create a jupyter notebook object",
        "detail": "scripts.report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "scripts.report_generator_from_readme",
        "description": "scripts.report_generator_from_readme",
        "peekOfCode": "path = 'readme.md' # same directory as this script\n# read a readme.md file and make a report jupyter notebook that has the appropriate sections (from the table of contents in the readme.md file)\ndef parse_table_of_contents(readme_text):\n    \"\"\"\n    parse_table_of_contents takes a string of text and returns a list of tuples\n    Parameters\n    :param readme_text: a string of text\n    :type readme_text: str\n    :return: a list of tuples\n    :rtype: list",
        "detail": "scripts.report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "model_iterator",
        "kind": 2,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "def model_iterator(model, pipe_params, model_names, lemmatized_bool):\n    \"\"\"\n    Run a model with a set of parameters\n    Args:\n        model (str): alias and name of the model\n        pipe_params (dict): _description_\n        lemmatized_bool (bool): whether the model is lemmatized or not\n    Returns:\n        model: _description_\n    \"\"\"",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "model_names",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "model_names = {\n    \"logreg\": LogisticRegression(),\n    \"dt\": DecisionTreeClassifier(),\n    \"adaboost\": AdaBoostClassifier(),\n    \"rf\": RandomForestClassifier()\n}\n### Count Vectorizer\n# Instantiate the CountVectorizer\nvectorizer = CountVectorizer()\nprint(df[\"selftext\"].isnull().sum())",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "vectorizer",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "vectorizer = CountVectorizer()\nprint(df[\"selftext\"].isnull().sum())\ncorpus = df[\"selftext\"]\n# source: https://stackoverflow.com/a/39308809/12801757 for below\ncvec = vectorizer.fit(corpus)  # fit and transform the data to the self-text column\n##! section one. Nonlemmatized fields\nX = df[\"selftext\"]  # post\ny = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "corpus",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "corpus = df[\"selftext\"]\n# source: https://stackoverflow.com/a/39308809/12801757 for below\ncvec = vectorizer.fit(corpus)  # fit and transform the data to the self-text column\n##! section one. Nonlemmatized fields\nX = df[\"selftext\"]  # post\ny = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\ndef model_iterator(model, pipe_params, model_names, lemmatized_bool):",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "cvec",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "cvec = vectorizer.fit(corpus)  # fit and transform the data to the self-text column\n##! section one. Nonlemmatized fields\nX = df[\"selftext\"]  # post\ny = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\ndef model_iterator(model, pipe_params, model_names, lemmatized_bool):\n    \"\"\"\n    Run a model with a set of parameters",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "X = df[\"selftext\"]  # post\ny = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\ndef model_iterator(model, pipe_params, model_names, lemmatized_bool):\n    \"\"\"\n    Run a model with a set of parameters\n    Args:\n        model (str): alias and name of the model",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "y = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\ndef model_iterator(model, pipe_params, model_names, lemmatized_bool):\n    \"\"\"\n    Run a model with a set of parameters\n    Args:\n        model (str): alias and name of the model\n        pipe_params (dict): _description_",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "lemmatizedBoolean",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "lemmatizedBoolean = False\n# 1. Logistic Regression\n# * Original Params\n# pipe_params_sent_len = {\n#     'cvec__max_features': [1000, 2000, 3000],\n#     'cvec__min_df': [2, 3],\n#     'cvec__max_df': [.9, .95],\n#     'cvec__ngram_range': [(1,1), (1,2)],\n#     'logreg__penalty': ['l1','l2'],\n#     'logreg__C': [1, 2, 3]",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "pipe_params",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "pipe_params = {\n    \"cvec__max_features\": [3000],\n    \"cvec__min_df\": [2],\n    \"cvec__max_df\": [0.9],\n    \"cvec__ngram_range\": [(1, 1)],\n    \"logreg__penalty\": [\"l2\"],\n    \"logreg__C\": [1, 2, 3],\n}\nlogregmodel = model_iterator(\"logreg\", pipe_params, model_names, lemmatizedBoolean)\n# 2. Decision Tree",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "logregmodel",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "logregmodel = model_iterator(\"logreg\", pipe_params, model_names, lemmatizedBoolean)\n# 2. Decision Tree\npipe_params_sent_len = {\n    \"cvec__max_features\": [1000, 2000, 3000],\n    \"cvec__min_df\": [2, 3],\n    \"cvec__max_df\": [0.9, 0.95],\n    \"cvec__ngram_range\": [(1, 1), (1, 2)],\n    \"dt__max_depth\": [None, 2, 3, 4],\n    \"dt__min_samples_split\": [2, 3, 4],\n    \"dt__min_samples_leaf\": [1, 2, 3],",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "pipe_params_sent_len",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "pipe_params_sent_len = {\n    \"cvec__max_features\": [1000, 2000, 3000],\n    \"cvec__min_df\": [2, 3],\n    \"cvec__max_df\": [0.9, 0.95],\n    \"cvec__ngram_range\": [(1, 1), (1, 2)],\n    \"dt__max_depth\": [None, 2, 3, 4],\n    \"dt__min_samples_split\": [2, 3, 4],\n    \"dt__min_samples_leaf\": [1, 2, 3],\n}\n# Running the model",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "dt_model",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "dt_model = model_iterator(\"dt\", pipe_params_sent_len, model_names, lemmatizedBoolean)\n# 3. AdaBoost Model\npipe_params_sent_len = {\n    \"cvec__max_features\": [1000, 2000, 3000],\n    \"cvec__min_df\": [2, 3],\n    \"cvec__max_df\": [0.9, 0.95],\n    \"cvec__ngram_range\": [(1, 1), (1, 2)],\n    \"ada__n_estimators\": [50, 100, 150],\n    \"ada__learning_rate\": [0.1, 0.5, 1],\n}",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "pipe_params_sent_len",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "pipe_params_sent_len = {\n    \"cvec__max_features\": [1000, 2000, 3000],\n    \"cvec__min_df\": [2, 3],\n    \"cvec__max_df\": [0.9, 0.95],\n    \"cvec__ngram_range\": [(1, 1), (1, 2)],\n    \"ada__n_estimators\": [50, 100, 150],\n    \"ada__learning_rate\": [0.1, 0.5, 1],\n}\n# Running the model\nada_model = model_iterator(\"ada\", pipe_params_sent_len, model_names, lemmatizedBoolean)",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "ada_model",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "ada_model = model_iterator(\"ada\", pipe_params_sent_len, model_names, lemmatizedBoolean)\n#! section two. Lemmatized fields\nlemmatizedBoolean = True\nX = df[\"selftext_lemmatized\"]  # post\ny = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n# 1. Logistic Regression\n# params",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "lemmatizedBoolean",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "lemmatizedBoolean = True\nX = df[\"selftext_lemmatized\"]  # post\ny = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n# 1. Logistic Regression\n# params\nipe_params = {\n    \"cvec__max_features\": [3000],",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "X = df[\"selftext_lemmatized\"]  # post\ny = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n# 1. Logistic Regression\n# params\nipe_params = {\n    \"cvec__max_features\": [3000],\n    \"cvec__min_df\": [2],",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "y = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n# 1. Logistic Regression\n# params\nipe_params = {\n    \"cvec__max_features\": [3000],\n    \"cvec__min_df\": [2],\n    \"cvec__max_df\": [0.9],",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "ipe_params",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "ipe_params = {\n    \"cvec__max_features\": [3000],\n    \"cvec__min_df\": [2],\n    \"cvec__max_df\": [0.9],\n    \"cvec__ngram_range\": [(1, 1)],\n    \"logreg__penalty\": [\"l2\"],\n    \"logreg__C\": [1, 2, 3],\n}\nlogregmodel_lemmatized = model_iterator(\n    \"logreg\", pipe_params, model_names, lemmatizedBoolean",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "logregmodel_lemmatized",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "logregmodel_lemmatized = model_iterator(\n    \"logreg\", pipe_params, model_names, lemmatizedBoolean\n)\n# 2. Decision Tree\npipe_params_sent_len = {\n    \"cvec__max_features\": [1000, 2000, 3000],\n    \"cvec__min_df\": [2, 3],\n    \"cvec__max_df\": [0.9, 0.95],\n    \"cvec__ngram_range\": [(1, 1), (1, 2)],\n    \"dt__max_depth\": [None, 2, 3, 4],",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "pipe_params_sent_len",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "pipe_params_sent_len = {\n    \"cvec__max_features\": [1000, 2000, 3000],\n    \"cvec__min_df\": [2, 3],\n    \"cvec__max_df\": [0.9, 0.95],\n    \"cvec__ngram_range\": [(1, 1), (1, 2)],\n    \"dt__max_depth\": [None, 2, 3, 4],\n    \"dt__min_samples_split\": [2, 3, 4],\n    \"dt__min_samples_leaf\": [1, 2, 3],\n}\n# Running the model",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "dt_model_lemmatized",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "dt_model_lemmatized = model_iterator(\n    \"dt\", pipe_params_sent_len, model_names, lemmatizedBoolean\n)\n# 3. AdaBoost Model\npipe_params_sent_len = {\n    \"cvec__max_features\": [1000, 2000, 3000],\n    \"cvec__min_df\": [2, 3],\n    \"cvec__max_df\": [0.9, 0.95],\n    \"cvec__ngram_range\": [(1, 1), (1, 2)],\n    \"ada__n_estimators\": [50, 100, 150],",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "pipe_params_sent_len",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "pipe_params_sent_len = {\n    \"cvec__max_features\": [1000, 2000, 3000],\n    \"cvec__min_df\": [2, 3],\n    \"cvec__max_df\": [0.9, 0.95],\n    \"cvec__ngram_range\": [(1, 1), (1, 2)],\n    \"ada__n_estimators\": [50, 100, 150],\n    \"ada__learning_rate\": [0.1, 0.5, 1],\n}\n# Running the model\nada_model_lemmatized = model_iterator(",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "ada_model_lemmatized",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "ada_model_lemmatized = model_iterator(\n    \"ada\", pipe_params_sent_len, model_names, lemmatizedBoolean\n)\n#! Fitting Models (Takes Time)\n# fit all the models on the training data\nprint(f\"Fitting Models on unlemmatized data\")\nlogregmodel.fit(X_train, y_train)\nprint(\"Logistic Regression Model Fitted\")\ndt_model.fit(X_train, y_train)\nprint(\"Decision Tree Model Fitted\")",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "allmodels",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "allmodels = [\n    logregmodel,\n    dt_model,\n    ada_model,\n    logregmodel_lemmatized,\n    dt_model_lemmatized,\n    ada_model_lemmatized,\n]  # list of all models\n#! Results Display\nfor every_model in allmodels:",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "self.X",
        "kind": 5,
        "importPath": "scripts.suppl",
        "description": "scripts.suppl",
        "peekOfCode": "self.X = (df[\"selftext\"],)\n        self.y = df[\"is_autism\"]  #\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            self.X, self.y, random_state=self.random_state\n        )\n    def model_operation(self, df):\n        # note: be sure that the selftext col has no null values before passing it to the model function.\n        # Instantiate the CountVectorizer\n        vectorizer = CountVectorizer()\n        df[\"selftext\"] = df[\"selftext\"].fillna(",
        "detail": "scripts.suppl",
        "documentation": {}
    },
    {
        "label": "stop",
        "kind": 5,
        "importPath": "scripts.word_lists",
        "description": "scripts.word_lists",
        "peekOfCode": "stop = stopwords.words(\"english\")\n# Custom Word Lists\nlistofknown_medications = [\n    'clonidine', 'quetiapine', 'risperidone', 'vyvanse', 'adderall', 'dexedrine', 'wellbutrin', 'focalinxr', 'modafanil', 'fluvoxamine', 'serzone', 'fluvoxamine', 'prozac', 'lexapro', 'paxil', 'celexa', 'effexor', 'zoloft', 'cymbalta', 'luvox', 'pristiq', 'remeron', 'venlafaxine', 'sarafem', 'anafranil', 'nortriptyline', 'tofranil', 'xanax', 'klonopin', 'ativan', 'valium', 'buspirone', 'oxazepam', 'aripiprazole', 'dextroamphetamine', 'ssrisnri', 'clonazepam', 'lorazepam', 'temazepam', 'alprazolam', 'chlordiazepoxide', 'flurazepam', 'oxazepam', 'triazolam', 'divalproexsodium', 'dronabinol', 'nabilone', 'duloxetine',\"Atorvastatin\", \"Levothyroxine\", \"Metformin\", \"Lisinopril\", \"Amlodipine\", \"Metoprolol\", \"Albuterol\", \"Omeprazole\", \"Losartan\", \"Gabapentin\", \"Hydrochlorothiazide\", \"Sertraline\", \"Simvastatin\", \"Montelukast\", \"Escitalopram\", \"Acetaminophen; Hydrocodone\", \"Rosuvastatin\", \"Bupropion\", \"Furosemide\", \"Pantoprazole\", \"Trazodone\", \"Dextroamphetamine; Dextroamphetamine Saccharate; Amphetamine; Amphetamine Aspartate\", \"Fluticasone\", \"Tamsulosin\", \"Fluoxetine\", \"Carvedilol\", \"Duloxetine\", \"Meloxicam\", \"Clopidogrel\", \"Prednisone\", \"Citalopram\", \"Insulin Glargine\", \"Potassium Chloride\", \"Pravastatin\", \"Tramadol\", \"Aspirin\", \"Alprazolam\", \"Ibuprofen\", \"Cyclobenzaprine\", \"Amoxicillin\", \"Methylphenidate\", \"Allopurinol\", \"Venlafaxine\", \"Clonazepam\", \"Ethinyl Estradiol; Norethindrone\", \"Ergocalciferol\", \"Zolpidem\", \"Apixaban\", \"Glipizide\", \"Hydrochlorothiazide; Lisinopril\", \"Spironolactone\", \"Cetirizine\", \"Atenolol\", \"Oxycodone\", \"Buspirone\", \"Fluticasone; Salmeterol\", \"Topiramate\", \"Warfarin\", \"Estradiol\", \"Cholecalciferol\", \"Budesonide; Formoterol\", \"Lamotrigine\", \"Ethinyl Estradiol; Norgestimate\", \"Quetiapine\", \"Lorazepam\", \"Famotidine\", \"Folic Acid\", \"Azithromycin\", \"Acetaminophen; Oxycodone\", \"Hydroxyzine\", \"Insulin Lispro\", \"Diclofenac\", \"Loratadine\", \"Sitagliptin\", \"Clonidine\", \"Diltiazem\", \"Latanoprost\", \"Pregabalin\", \"Doxycycline\", \"Insulin Aspart\", \"Amitriptyline\", \"Paroxetine\", \"Ondansetron\", \"Tizanidine\", \"Lisdexamfetamine\", \"Rivaroxaban\", \"Glimepiride\", \"Propranolol\", \"Aripiprazole\", \"Finasteride\", \"Naproxen\", \"Levetiracetam\", \"Hydrochlorothiazide; Losartan\", \"Alendronate\", \"Fenofibrate\", \"Dulaglutide\", \"Oxybutynin\", \"Celecoxib\", \"Lovastatin\", \"Ezetimibe\", \"Cephalexin\", \"Empagliflozin\", \"Hydralazine\", \"Mirtazapine\", \"Cyanocobalamin\", \"Triamcinolone\", \"Amoxicillin; Clavulanate\", \"Baclofen\", \"Valproate\", \"Tiotropium\", \"Sumatriptan\", \"Donepezil\", \"Methotrexate\", \"Isosorbide\", \"Fluticasone; Vilanterol\", \"Ferrous Sulfate\", \"Thyroid\", \"Acetaminophen\", \"Valacyclovir\", \"Desogestrel; Ethinyl Estradiol\", \"Sulfamethoxazole; Trimethoprim\", \"Esomeprazole\", \"Valsartan\", \"Insulin Detemir\", \"Clindamycin\", \"Hydroxychloroquine\", \"Methocarbamol\", \"Diazepam\", \"Semaglutide\", \"Dexmethylphenidate\", \"Hydrochlorothiazide; Triamterene\", \"Ciprofloxacin\", \"Chlorthalidone\", \"Rizatriptan\", \"Nifedipine\", \"Insulin Degludec\", \"Norethindrone\", \"Risperidone\", \"Olmesartan\", \"Morphine\", \"Benazepril\", \"Meclizine\", \"Timolol\", \"Oxcarbazepine\", \"Drospirenone; Ethinyl Estradiol\", \"Liraglutide\", \"Dicyclomine\", \"Irbesartan\", \"Hydrocortisone\", \"Albuterol; Ipratropium\", \"Verapamil\", \"Memantine\", \"Prednisolone\", \"Metformin; Sitagliptin\", \"Nortriptyline\", \"Ropinirole\", \"Benzonatate\", \"Progesterone\", \"Ethinyl Estradiol; Levonorgestrel\", \"Mirabegron\", \"Methylprednisolone\", \"Acyclovir\", \"Docusate\", \"Olanzapine\", \"Nitroglycerin\", \"Bimatoprost\", \"Nitrofurantoin\", \"Pioglitazone\", \"Amlodipine; Benazepril\", \"Ketoconazole\", \"Clobetasol\", \"Testosterone\", \"Azelastine\", \"Fluconazole\", \"Brimonidine\", \"Desvenlafaxine\", \"Ranitidine\", \"Oseltamivir\", \"Levocetirizine\", \"Anastrozole\", \"Phentermine\", \"Sucralfate\", \"Sildenafil\", \"Mesalamine\", \"Carbamazepine\", \"Buprenorphine\", \"Acetaminophen; Codeine\", \"Flecainide\", \"Gemfibrozil\", \"Prazosin\", \"Lansoprazole\", \"Diphenhydramine\", \"Pramipexole\", \"Ethinyl Estradiol; Etonogestrel\", \"Dorzolamide; Timolol\", \"Ramipril\", \"Lithium\", \"Amiodarone\", \"Omega-3-acid Ethyl Esters\", \"Glyburide\", \"Acetaminophen; Butalbital; Caffeine\", \"Magnesium Salts\", \"Mupirocin\", \"Calcium\", \"Adalimumab\", \"Methimazole\", \"Budesonide\", \"Promethazine\", \"Doxazosin\", \"Labetalol\", \"Terazosin\", \"Cyclosporine\", \"Torsemide\", \"Medroxyprogesterone\", \"Calcium; Vitamin D\", \"Dorzolamide\", \"Dapagliflozin\", \"Liothyronine\", \"Sacubitril; Valsartan\", \"Beclomethasone\", \"Insulin Isophane\", \"Metronidazole\", \"Temazepam\", \"Fluticasone; Umeclidinium; Vilanterol\", \"Erythromycin\", \"Polyethylene Glycol 3350\", \"Nystatin\", \"Cefdinir\", \"Benztropine\", \"Tretinoin\", \"Mometasone\", \"Eszopiclone\", \"Betamethasone\", \"Erenumab\", \"Hydrochlorothiazide; Valsartan\", \"Minocycline\", \"Digoxin\", \"Empagliflozin; Metformin\", \"Nebivolol\", \"Levofloxacin\", \"Colchicine\", \"Ofloxacin\", \"Vortioxetine\", \"Linaclotide\", \"Umeclidinium\", \"Insulin Human; Insulin Isophane Human\", \"Ticagrelor\", \"Telmisartan\", \"Ketorolac\", \"Hydromorphone\", \"Epinephrine\", \"Doxepin\", \"Quinapril\", \"Umeclidinium; Vilanterol\", \"Fexofenadine\", \"Brimonidine; Timolol\", \"Letrozole\", \"Ranolazine\", \"Lurasidone\", \"Phenytoin\", \"Tadalafil\", \"Pancrelipase Amylase; Pancrelipase Lipase; Pancrelipase Protease\", \"Dexlansoprazole\", \"Isotretinoin\", \"Sodium Fluoride\", \"Solifenacin\", \"Bisoprolol\", \"Olopatadine\", \"Primidone\", \"Bumetanide\", \"Tolterodine\", \"Dexamethasone\", \"Chlorhexidine\", \"Sodium Salts\", \"Varenicline\", \"Zonisamide\", \"Calcitriol\", \"Emtricitabine; Tenofovir Disoproxil\", \"Terbinafine\", \"Fluocinonide\", \"Hydrochlorothiazide; Olmesartan\", \"Ziprasidone\", \"Estrogens, Conjugated\", \"Sulfasalazine\", \"Icosapent Ethyl\", \"Dexamethasone; Moxifloxacin\", \"Atomoxetine\", \"Formoterol; Mometasone\", \"Ketotifen\", \"Bisoprolol; Hydrochlorothiazide\", \"Sennosides\", \"Raloxifene\", \"Linagliptin\", \"Canagliflozin\", \"Alogliptin\", \"Sotalol\", \"Potassium Citrate\", \"Melatonin\", \"Isosorbide Dinitrate\", \"Guanfacine\"]\nlistofknown_medications = [x.lower() for x in listofknown_medications] # make all the words lowercase\nconditions = ['autism','ocd'] # list of conditions to search for\nslang_dict = {\n    \" tho \": \" though \",\n    \"thru\": \" through \",\n    \"thx\": \" thanks \",",
        "detail": "scripts.word_lists",
        "documentation": {}
    },
    {
        "label": "listofknown_medications",
        "kind": 5,
        "importPath": "scripts.word_lists",
        "description": "scripts.word_lists",
        "peekOfCode": "listofknown_medications = [\n    'clonidine', 'quetiapine', 'risperidone', 'vyvanse', 'adderall', 'dexedrine', 'wellbutrin', 'focalinxr', 'modafanil', 'fluvoxamine', 'serzone', 'fluvoxamine', 'prozac', 'lexapro', 'paxil', 'celexa', 'effexor', 'zoloft', 'cymbalta', 'luvox', 'pristiq', 'remeron', 'venlafaxine', 'sarafem', 'anafranil', 'nortriptyline', 'tofranil', 'xanax', 'klonopin', 'ativan', 'valium', 'buspirone', 'oxazepam', 'aripiprazole', 'dextroamphetamine', 'ssrisnri', 'clonazepam', 'lorazepam', 'temazepam', 'alprazolam', 'chlordiazepoxide', 'flurazepam', 'oxazepam', 'triazolam', 'divalproexsodium', 'dronabinol', 'nabilone', 'duloxetine',\"Atorvastatin\", \"Levothyroxine\", \"Metformin\", \"Lisinopril\", \"Amlodipine\", \"Metoprolol\", \"Albuterol\", \"Omeprazole\", \"Losartan\", \"Gabapentin\", \"Hydrochlorothiazide\", \"Sertraline\", \"Simvastatin\", \"Montelukast\", \"Escitalopram\", \"Acetaminophen; Hydrocodone\", \"Rosuvastatin\", \"Bupropion\", \"Furosemide\", \"Pantoprazole\", \"Trazodone\", \"Dextroamphetamine; Dextroamphetamine Saccharate; Amphetamine; Amphetamine Aspartate\", \"Fluticasone\", \"Tamsulosin\", \"Fluoxetine\", \"Carvedilol\", \"Duloxetine\", \"Meloxicam\", \"Clopidogrel\", \"Prednisone\", \"Citalopram\", \"Insulin Glargine\", \"Potassium Chloride\", \"Pravastatin\", \"Tramadol\", \"Aspirin\", \"Alprazolam\", \"Ibuprofen\", \"Cyclobenzaprine\", \"Amoxicillin\", \"Methylphenidate\", \"Allopurinol\", \"Venlafaxine\", \"Clonazepam\", \"Ethinyl Estradiol; Norethindrone\", \"Ergocalciferol\", \"Zolpidem\", \"Apixaban\", \"Glipizide\", \"Hydrochlorothiazide; Lisinopril\", \"Spironolactone\", \"Cetirizine\", \"Atenolol\", \"Oxycodone\", \"Buspirone\", \"Fluticasone; Salmeterol\", \"Topiramate\", \"Warfarin\", \"Estradiol\", \"Cholecalciferol\", \"Budesonide; Formoterol\", \"Lamotrigine\", \"Ethinyl Estradiol; Norgestimate\", \"Quetiapine\", \"Lorazepam\", \"Famotidine\", \"Folic Acid\", \"Azithromycin\", \"Acetaminophen; Oxycodone\", \"Hydroxyzine\", \"Insulin Lispro\", \"Diclofenac\", \"Loratadine\", \"Sitagliptin\", \"Clonidine\", \"Diltiazem\", \"Latanoprost\", \"Pregabalin\", \"Doxycycline\", \"Insulin Aspart\", \"Amitriptyline\", \"Paroxetine\", \"Ondansetron\", \"Tizanidine\", \"Lisdexamfetamine\", \"Rivaroxaban\", \"Glimepiride\", \"Propranolol\", \"Aripiprazole\", \"Finasteride\", \"Naproxen\", \"Levetiracetam\", \"Hydrochlorothiazide; Losartan\", \"Alendronate\", \"Fenofibrate\", \"Dulaglutide\", \"Oxybutynin\", \"Celecoxib\", \"Lovastatin\", \"Ezetimibe\", \"Cephalexin\", \"Empagliflozin\", \"Hydralazine\", \"Mirtazapine\", \"Cyanocobalamin\", \"Triamcinolone\", \"Amoxicillin; Clavulanate\", \"Baclofen\", \"Valproate\", \"Tiotropium\", \"Sumatriptan\", \"Donepezil\", \"Methotrexate\", \"Isosorbide\", \"Fluticasone; Vilanterol\", \"Ferrous Sulfate\", \"Thyroid\", \"Acetaminophen\", \"Valacyclovir\", \"Desogestrel; Ethinyl Estradiol\", \"Sulfamethoxazole; Trimethoprim\", \"Esomeprazole\", \"Valsartan\", \"Insulin Detemir\", \"Clindamycin\", \"Hydroxychloroquine\", \"Methocarbamol\", \"Diazepam\", \"Semaglutide\", \"Dexmethylphenidate\", \"Hydrochlorothiazide; Triamterene\", \"Ciprofloxacin\", \"Chlorthalidone\", \"Rizatriptan\", \"Nifedipine\", \"Insulin Degludec\", \"Norethindrone\", \"Risperidone\", \"Olmesartan\", \"Morphine\", \"Benazepril\", \"Meclizine\", \"Timolol\", \"Oxcarbazepine\", \"Drospirenone; Ethinyl Estradiol\", \"Liraglutide\", \"Dicyclomine\", \"Irbesartan\", \"Hydrocortisone\", \"Albuterol; Ipratropium\", \"Verapamil\", \"Memantine\", \"Prednisolone\", \"Metformin; Sitagliptin\", \"Nortriptyline\", \"Ropinirole\", \"Benzonatate\", \"Progesterone\", \"Ethinyl Estradiol; Levonorgestrel\", \"Mirabegron\", \"Methylprednisolone\", \"Acyclovir\", \"Docusate\", \"Olanzapine\", \"Nitroglycerin\", \"Bimatoprost\", \"Nitrofurantoin\", \"Pioglitazone\", \"Amlodipine; Benazepril\", \"Ketoconazole\", \"Clobetasol\", \"Testosterone\", \"Azelastine\", \"Fluconazole\", \"Brimonidine\", \"Desvenlafaxine\", \"Ranitidine\", \"Oseltamivir\", \"Levocetirizine\", \"Anastrozole\", \"Phentermine\", \"Sucralfate\", \"Sildenafil\", \"Mesalamine\", \"Carbamazepine\", \"Buprenorphine\", \"Acetaminophen; Codeine\", \"Flecainide\", \"Gemfibrozil\", \"Prazosin\", \"Lansoprazole\", \"Diphenhydramine\", \"Pramipexole\", \"Ethinyl Estradiol; Etonogestrel\", \"Dorzolamide; Timolol\", \"Ramipril\", \"Lithium\", \"Amiodarone\", \"Omega-3-acid Ethyl Esters\", \"Glyburide\", \"Acetaminophen; Butalbital; Caffeine\", \"Magnesium Salts\", \"Mupirocin\", \"Calcium\", \"Adalimumab\", \"Methimazole\", \"Budesonide\", \"Promethazine\", \"Doxazosin\", \"Labetalol\", \"Terazosin\", \"Cyclosporine\", \"Torsemide\", \"Medroxyprogesterone\", \"Calcium; Vitamin D\", \"Dorzolamide\", \"Dapagliflozin\", \"Liothyronine\", \"Sacubitril; Valsartan\", \"Beclomethasone\", \"Insulin Isophane\", \"Metronidazole\", \"Temazepam\", \"Fluticasone; Umeclidinium; Vilanterol\", \"Erythromycin\", \"Polyethylene Glycol 3350\", \"Nystatin\", \"Cefdinir\", \"Benztropine\", \"Tretinoin\", \"Mometasone\", \"Eszopiclone\", \"Betamethasone\", \"Erenumab\", \"Hydrochlorothiazide; Valsartan\", \"Minocycline\", \"Digoxin\", \"Empagliflozin; Metformin\", \"Nebivolol\", \"Levofloxacin\", \"Colchicine\", \"Ofloxacin\", \"Vortioxetine\", \"Linaclotide\", \"Umeclidinium\", \"Insulin Human; Insulin Isophane Human\", \"Ticagrelor\", \"Telmisartan\", \"Ketorolac\", \"Hydromorphone\", \"Epinephrine\", \"Doxepin\", \"Quinapril\", \"Umeclidinium; Vilanterol\", \"Fexofenadine\", \"Brimonidine; Timolol\", \"Letrozole\", \"Ranolazine\", \"Lurasidone\", \"Phenytoin\", \"Tadalafil\", \"Pancrelipase Amylase; Pancrelipase Lipase; Pancrelipase Protease\", \"Dexlansoprazole\", \"Isotretinoin\", \"Sodium Fluoride\", \"Solifenacin\", \"Bisoprolol\", \"Olopatadine\", \"Primidone\", \"Bumetanide\", \"Tolterodine\", \"Dexamethasone\", \"Chlorhexidine\", \"Sodium Salts\", \"Varenicline\", \"Zonisamide\", \"Calcitriol\", \"Emtricitabine; Tenofovir Disoproxil\", \"Terbinafine\", \"Fluocinonide\", \"Hydrochlorothiazide; Olmesartan\", \"Ziprasidone\", \"Estrogens, Conjugated\", \"Sulfasalazine\", \"Icosapent Ethyl\", \"Dexamethasone; Moxifloxacin\", \"Atomoxetine\", \"Formoterol; Mometasone\", \"Ketotifen\", \"Bisoprolol; Hydrochlorothiazide\", \"Sennosides\", \"Raloxifene\", \"Linagliptin\", \"Canagliflozin\", \"Alogliptin\", \"Sotalol\", \"Potassium Citrate\", \"Melatonin\", \"Isosorbide Dinitrate\", \"Guanfacine\"]\nlistofknown_medications = [x.lower() for x in listofknown_medications] # make all the words lowercase\nconditions = ['autism','ocd'] # list of conditions to search for\nslang_dict = {\n    \" tho \": \" though \",\n    \"thru\": \" through \",\n    \"thx\": \" thanks \",\n    \" u \": \" you \",\n    \" ur \": \" your \",",
        "detail": "scripts.word_lists",
        "documentation": {}
    },
    {
        "label": "listofknown_medications",
        "kind": 5,
        "importPath": "scripts.word_lists",
        "description": "scripts.word_lists",
        "peekOfCode": "listofknown_medications = [x.lower() for x in listofknown_medications] # make all the words lowercase\nconditions = ['autism','ocd'] # list of conditions to search for\nslang_dict = {\n    \" tho \": \" though \",\n    \"thru\": \" through \",\n    \"thx\": \" thanks \",\n    \" u \": \" you \",\n    \" ur \": \" your \",\n    \" yr \": \" your \",\n    \" yrs \": \" years \",",
        "detail": "scripts.word_lists",
        "documentation": {}
    },
    {
        "label": "conditions",
        "kind": 5,
        "importPath": "scripts.word_lists",
        "description": "scripts.word_lists",
        "peekOfCode": "conditions = ['autism','ocd'] # list of conditions to search for\nslang_dict = {\n    \" tho \": \" though \",\n    \"thru\": \" through \",\n    \"thx\": \" thanks \",\n    \" u \": \" you \",\n    \" ur \": \" your \",\n    \" yr \": \" your \",\n    \" yrs \": \" years \",\n    \" b \": \" be \",",
        "detail": "scripts.word_lists",
        "documentation": {}
    },
    {
        "label": "slang_dict",
        "kind": 5,
        "importPath": "scripts.word_lists",
        "description": "scripts.word_lists",
        "peekOfCode": "slang_dict = {\n    \" tho \": \" though \",\n    \"thru\": \" through \",\n    \"thx\": \" thanks \",\n    \" u \": \" you \",\n    \" ur \": \" your \",\n    \" yr \": \" your \",\n    \" yrs \": \" years \",\n    \" b \": \" be \",\n    \" r \": \" are \",",
        "detail": "scripts.word_lists",
        "documentation": {}
    },
    {
        "label": "biasing_terms",
        "kind": 5,
        "importPath": "scripts.word_lists",
        "description": "scripts.word_lists",
        "peekOfCode": "biasing_terms = [\"ocd\", \"autism\"]\n# sources\n# https://clincalc.com/DrugStats/Top300Drugs.aspx - list of top 300 drugs",
        "detail": "scripts.word_lists",
        "documentation": {}
    }
]