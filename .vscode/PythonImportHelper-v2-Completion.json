[
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "biasing_terms",
        "importPath": "word_lists",
        "description": "word_lists",
        "isExtraImport": true,
        "detail": "word_lists",
        "documentation": {}
    },
    {
        "label": "stop",
        "importPath": "word_lists",
        "description": "word_lists",
        "isExtraImport": true,
        "detail": "word_lists",
        "documentation": {}
    },
    {
        "label": "listofknown_medications",
        "importPath": "word_lists",
        "description": "word_lists",
        "isExtraImport": true,
        "detail": "word_lists",
        "documentation": {}
    },
    {
        "label": "stop",
        "importPath": "word_lists",
        "description": "word_lists",
        "isExtraImport": true,
        "detail": "word_lists",
        "documentation": {}
    },
    {
        "label": "biasing_terms",
        "importPath": "word_lists",
        "description": "word_lists",
        "isExtraImport": true,
        "detail": "word_lists",
        "documentation": {}
    },
    {
        "label": "listofknown_medications",
        "importPath": "word_lists",
        "description": "word_lists",
        "isExtraImport": true,
        "detail": "word_lists",
        "documentation": {}
    },
    {
        "label": "conditions",
        "importPath": "word_lists",
        "description": "word_lists",
        "isExtraImport": true,
        "detail": "word_lists",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "CountVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "CountVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "CountVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "DecisionTreeClassifier",
        "importPath": "sklearn.tree",
        "description": "sklearn.tree",
        "isExtraImport": true,
        "detail": "sklearn.tree",
        "documentation": {}
    },
    {
        "label": "DecisionTreeClassifier",
        "importPath": "sklearn.tree",
        "description": "sklearn.tree",
        "isExtraImport": true,
        "detail": "sklearn.tree",
        "documentation": {}
    },
    {
        "label": "DecisionTreeClassifier",
        "importPath": "sklearn.tree",
        "description": "sklearn.tree",
        "isExtraImport": true,
        "detail": "sklearn.tree",
        "documentation": {}
    },
    {
        "label": "DecisionTreeClassifier",
        "importPath": "sklearn.tree",
        "description": "sklearn.tree",
        "isExtraImport": true,
        "detail": "sklearn.tree",
        "documentation": {}
    },
    {
        "label": "export_graphviz",
        "importPath": "sklearn.tree",
        "description": "sklearn.tree",
        "isExtraImport": true,
        "detail": "sklearn.tree",
        "documentation": {}
    },
    {
        "label": "plot_tree",
        "importPath": "sklearn.tree",
        "description": "sklearn.tree",
        "isExtraImport": true,
        "detail": "sklearn.tree",
        "documentation": {}
    },
    {
        "label": "export_text",
        "importPath": "sklearn.tree",
        "description": "sklearn.tree",
        "isExtraImport": true,
        "detail": "sklearn.tree",
        "documentation": {}
    },
    {
        "label": "RandomForestClassifier",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "AdaBoostClassifier",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "GradientBoostingClassifier",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "AdaBoostClassifier",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "AdaBoostClassifier",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "GridSearchCV",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "GridSearchCV",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "cross_val_score",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "cross_val_score",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "LogisticRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "LogisticRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "LogisticRegression",
        "importPath": "sklearn.linear_model",
        "description": "sklearn.linear_model",
        "isExtraImport": true,
        "detail": "sklearn.linear_model",
        "documentation": {}
    },
    {
        "label": "plot_roc_curve",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "adjusted_rand_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "silhouette_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "classification_report",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "roc_curve",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "roc_auc_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_recall_curve",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "average_precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "sklearn.pipeline",
        "description": "sklearn.pipeline",
        "isExtraImport": true,
        "detail": "sklearn.pipeline",
        "documentation": {}
    },
    {
        "label": "XGBClassifier",
        "importPath": "xgboost",
        "description": "xgboost",
        "isExtraImport": true,
        "detail": "xgboost",
        "documentation": {}
    },
    {
        "label": "stop",
        "importPath": "modeling",
        "description": "modeling",
        "isExtraImport": true,
        "detail": "modeling",
        "documentation": {}
    },
    {
        "label": "run_modeling",
        "importPath": "modeling",
        "description": "modeling",
        "isExtraImport": true,
        "detail": "modeling",
        "documentation": {}
    },
    {
        "label": "nbformat",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nbformat",
        "description": "nbformat",
        "detail": "nbformat",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "linkage",
        "importPath": "scipy.cluster.hierarchy",
        "description": "scipy.cluster.hierarchy",
        "isExtraImport": true,
        "detail": "scipy.cluster.hierarchy",
        "documentation": {}
    },
    {
        "label": "linkage",
        "importPath": "scipy.cluster.hierarchy",
        "description": "scipy.cluster.hierarchy",
        "isExtraImport": true,
        "detail": "scipy.cluster.hierarchy",
        "documentation": {}
    },
    {
        "label": "pyplot",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "pyplot",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "feature_engineer",
        "importPath": "feature_engineering",
        "description": "feature_engineering",
        "isExtraImport": true,
        "detail": "feature_engineering",
        "documentation": {}
    },
    {
        "label": "remove_ocd_meds",
        "importPath": "feature_engineering",
        "description": "feature_engineering",
        "isExtraImport": true,
        "detail": "feature_engineering",
        "documentation": {}
    },
    {
        "label": "run_feature_engineering",
        "importPath": "feature_engineering",
        "description": "feature_engineering",
        "isExtraImport": true,
        "detail": "feature_engineering",
        "documentation": {}
    },
    {
        "label": "PorterStemmer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "pandas_profiling",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas_profiling",
        "description": "pandas_profiling",
        "detail": "pandas_profiling",
        "documentation": {}
    },
    {
        "label": "ProfileReport",
        "importPath": "pandas_profiling",
        "description": "pandas_profiling",
        "isExtraImport": true,
        "detail": "pandas_profiling",
        "documentation": {}
    },
    {
        "label": "KMeans",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "AgglomerativeClustering",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "DBSCAN",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "AffinityPropagation",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "MeanShift",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "SpectralClustering",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "OPTICS",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "Birch",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "AgglomerativeClustering",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "MiniBatchKMeans",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "LatentDirichletAllocation",
        "importPath": "sklearn.decomposition",
        "description": "sklearn.decomposition",
        "isExtraImport": true,
        "detail": "sklearn.decomposition",
        "documentation": {}
    },
    {
        "label": "NMF",
        "importPath": "sklearn.decomposition",
        "description": "sklearn.decomposition",
        "isExtraImport": true,
        "detail": "sklearn.decomposition",
        "documentation": {}
    },
    {
        "label": "floor",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "tree",
        "importPath": "sklearn",
        "description": "sklearn",
        "isExtraImport": true,
        "detail": "sklearn",
        "documentation": {}
    },
    {
        "label": "alive_bar",
        "importPath": "alive_progress",
        "description": "alive_progress",
        "isExtraImport": true,
        "detail": "alive_progress",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "run_preprocess_data",
        "importPath": "preprocessing",
        "description": "preprocessing",
        "isExtraImport": true,
        "detail": "preprocessing",
        "documentation": {}
    },
    {
        "label": "process_dataframe",
        "kind": 2,
        "importPath": "scripts.011_preprocessing",
        "description": "scripts.011_preprocessing",
        "peekOfCode": "def process_dataframe(df):\n    # Quick Eliminations\n    if \"selftext\" in df.columns:\n        df = df[\n            [\"title\", \"selftext\", \"subreddit\", \"author\", \"created_utc\"]\n        ]  # Eliminate the Unused Columns\n    else:  # selftext has already been renamed to selftext\n        df = df[[\"title\", \"selftext\", \"subreddit\", \"author\", \"created_utc\"]]\n    # Rename 'selftext' to 'selftext'\n    df = df.rename(columns={\"selftext\": \"selftext\"})",
        "detail": "scripts.011_preprocessing",
        "documentation": {}
    },
    {
        "label": "remove_overly_biasing_terms",
        "kind": 2,
        "importPath": "scripts.011_preprocessing",
        "description": "scripts.011_preprocessing",
        "peekOfCode": "def remove_overly_biasing_terms(df):\n    # adapted from source: https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n    # remove the overly biasing terms from the selftext in the dataframe\n    df[\"selftext\"] = df[\"selftext\"].apply(\n        lambda x: \" \".join(x for x in x.split() if x not in biasing_terms)\n    )\n    return df\ndef merge_text(df):\n    \"\"\"\n    Merges text in the title and selftext columns into the selftext column. This is done to increase the amount of text in the selftext column and eliminate the issues that could come up with title-heavy posts.",
        "detail": "scripts.011_preprocessing",
        "documentation": {}
    },
    {
        "label": "merge_text",
        "kind": 2,
        "importPath": "scripts.011_preprocessing",
        "description": "scripts.011_preprocessing",
        "peekOfCode": "def merge_text(df):\n    \"\"\"\n    Merges text in the title and selftext columns into the selftext column. This is done to increase the amount of text in the selftext column and eliminate the issues that could come up with title-heavy posts.\n    Args:\n        df (dataframe): The dataframe to be processed.\n    Raises:\n        Exception: If the dataframe does not have a title and selftext column.\n    Returns:\n        df: The processed dataframe.\n    \"\"\"",
        "detail": "scripts.011_preprocessing",
        "documentation": {}
    },
    {
        "label": "preprocessing_function",
        "kind": 2,
        "importPath": "scripts.011_preprocessing",
        "description": "scripts.011_preprocessing",
        "peekOfCode": "def preprocessing_function(df):\n    \"\"\"\n    Run the process_dataframe and merge_text functions on the dataframe.\n    Args:\n        df (dataframe): The dataframe to be processed.\n    Returns:\n        dataframe: The processed dataframe.\n    \"\"\"\n    df = merge_text(df)  # introduce new features\n    df = process_dataframe(df)  # process the dataframe",
        "detail": "scripts.011_preprocessing",
        "documentation": {}
    },
    {
        "label": "data_exploration",
        "kind": 2,
        "importPath": "scripts.011_preprocessing",
        "description": "scripts.011_preprocessing",
        "peekOfCode": "def data_exploration(df):\n    \"\"\"\n    Perform data exploration on the dataframe.\n    Args:\n        df (dataframe): The dataframe to be explored.\n    \"\"\"\n    # We want to explore the data to see if there is anything we can do to improve the model.\n    # We want to see if there is any correlation between the length of the title and the length of the selftext\n    # and the subreddit that the post is from.\n    sns.scatterplot(x=\"title_length\", y=\"selftext_length\", hue=\"is_autism\", data=df)",
        "detail": "scripts.011_preprocessing",
        "documentation": {}
    },
    {
        "label": "run_preprocess_data",
        "kind": 2,
        "importPath": "scripts.011_preprocessing",
        "description": "scripts.011_preprocessing",
        "peekOfCode": "def run_preprocess_data(df):\n    # ^ File I/O\n    # If the df_cleaned.csv file has not been generated yet, then we want to generate it with the preprocessing_function.\n    # If the df_cleaned.csv file has been generated, then we want to read it in and use it for the model.\n    if not os.path.isfile(\"data/df_cleaned.csv\"):\n        logging.info(\n            \"The df_cleaned.csv file has not been generated yet, so we are generating it now.\"\n        )\n        # Create df by concatenating the dataframes from the two subreddits\n        # & Reading in both Reddit threads as csv files.",
        "detail": "scripts.011_preprocessing",
        "documentation": {}
    },
    {
        "label": "model_iterator",
        "kind": 2,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "def model_iterator(model, pipe_params, model_names, lemmatized_bool):\n    \"\"\"\n    Run a model with a set of parameters\n    Args:\n        model (str): alias and name of the model\n        pipe_params (dict): _description_\n        lemmatized_bool (bool): whether the model is lemmatized or not\n    Returns:\n        model: _description_\n    \"\"\"",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "model_names",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "model_names = {\n    \"logreg\": LogisticRegression(),\n    \"dt\": DecisionTreeClassifier(),\n    \"adaboost\": AdaBoostClassifier(),\n    \"rf\": RandomForestClassifier()\n}\n### Count Vectorizer\n# Instantiate the CountVectorizer\nvectorizer = CountVectorizer()\nprint(df[\"selftext\"].isnull().sum())",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "vectorizer",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "vectorizer = CountVectorizer()\nprint(df[\"selftext\"].isnull().sum())\ncorpus = df[\"selftext\"]\n# source: https://stackoverflow.com/a/39308809/12801757 for below\ncvec = vectorizer.fit(corpus)  # fit and transform the data to the self-text column\n##! section one. Nonlemmatized fields\nX = df[\"selftext\"]  # post\ny = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "corpus",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "corpus = df[\"selftext\"]\n# source: https://stackoverflow.com/a/39308809/12801757 for below\ncvec = vectorizer.fit(corpus)  # fit and transform the data to the self-text column\n##! section one. Nonlemmatized fields\nX = df[\"selftext\"]  # post\ny = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\ndef model_iterator(model, pipe_params, model_names, lemmatized_bool):",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "cvec",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "cvec = vectorizer.fit(corpus)  # fit and transform the data to the self-text column\n##! section one. Nonlemmatized fields\nX = df[\"selftext\"]  # post\ny = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\ndef model_iterator(model, pipe_params, model_names, lemmatized_bool):\n    \"\"\"\n    Run a model with a set of parameters",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "X = df[\"selftext\"]  # post\ny = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\ndef model_iterator(model, pipe_params, model_names, lemmatized_bool):\n    \"\"\"\n    Run a model with a set of parameters\n    Args:\n        model (str): alias and name of the model",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "y = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\ndef model_iterator(model, pipe_params, model_names, lemmatized_bool):\n    \"\"\"\n    Run a model with a set of parameters\n    Args:\n        model (str): alias and name of the model\n        pipe_params (dict): _description_",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "lemmatizedBoolean",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "lemmatizedBoolean = False\n# 1. Logistic Regression\n# * Original Params\n# pipe_params_sent_len = {\n#     'cvec__max_features': [1000, 2000, 3000],\n#     'cvec__min_df': [2, 3],\n#     'cvec__max_df': [.9, .95],\n#     'cvec__ngram_range': [(1,1), (1,2)],\n#     'logreg__penalty': ['l1','l2'],\n#     'logreg__C': [1, 2, 3]",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "pipe_params",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "pipe_params = {\n    \"cvec__max_features\": [3000],\n    \"cvec__min_df\": [2],\n    \"cvec__max_df\": [0.9],\n    \"cvec__ngram_range\": [(1, 1)],\n    \"logreg__penalty\": [\"l2\"],\n    \"logreg__C\": [1, 2, 3],\n}\nlogregmodel = model_iterator(\"logreg\", pipe_params, model_names, lemmatizedBoolean)\n# 2. Decision Tree",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "logregmodel",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "logregmodel = model_iterator(\"logreg\", pipe_params, model_names, lemmatizedBoolean)\n# 2. Decision Tree\npipe_params_sent_len = {\n    \"cvec__max_features\": [1000, 2000, 3000],\n    \"cvec__min_df\": [2, 3],\n    \"cvec__max_df\": [0.9, 0.95],\n    \"cvec__ngram_range\": [(1, 1), (1, 2)],\n    \"dt__max_depth\": [None, 2, 3, 4],\n    \"dt__min_samples_split\": [2, 3, 4],\n    \"dt__min_samples_leaf\": [1, 2, 3],",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "pipe_params_sent_len",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "pipe_params_sent_len = {\n    \"cvec__max_features\": [1000, 2000, 3000],\n    \"cvec__min_df\": [2, 3],\n    \"cvec__max_df\": [0.9, 0.95],\n    \"cvec__ngram_range\": [(1, 1), (1, 2)],\n    \"dt__max_depth\": [None, 2, 3, 4],\n    \"dt__min_samples_split\": [2, 3, 4],\n    \"dt__min_samples_leaf\": [1, 2, 3],\n}\n# Running the model",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "dt_model",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "dt_model = model_iterator(\"dt\", pipe_params_sent_len, model_names, lemmatizedBoolean)\n# 3. AdaBoost Model\npipe_params_sent_len = {\n    \"cvec__max_features\": [1000, 2000, 3000],\n    \"cvec__min_df\": [2, 3],\n    \"cvec__max_df\": [0.9, 0.95],\n    \"cvec__ngram_range\": [(1, 1), (1, 2)],\n    \"ada__n_estimators\": [50, 100, 150],\n    \"ada__learning_rate\": [0.1, 0.5, 1],\n}",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "pipe_params_sent_len",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "pipe_params_sent_len = {\n    \"cvec__max_features\": [1000, 2000, 3000],\n    \"cvec__min_df\": [2, 3],\n    \"cvec__max_df\": [0.9, 0.95],\n    \"cvec__ngram_range\": [(1, 1), (1, 2)],\n    \"ada__n_estimators\": [50, 100, 150],\n    \"ada__learning_rate\": [0.1, 0.5, 1],\n}\n# Running the model\nada_model = model_iterator(\"ada\", pipe_params_sent_len, model_names, lemmatizedBoolean)",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "ada_model",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "ada_model = model_iterator(\"ada\", pipe_params_sent_len, model_names, lemmatizedBoolean)\n#! section two. Lemmatized fields\nlemmatizedBoolean = True\nX = df[\"selftext_lemmatized\"]  # post\ny = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n# 1. Logistic Regression\n# params",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "lemmatizedBoolean",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "lemmatizedBoolean = True\nX = df[\"selftext_lemmatized\"]  # post\ny = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n# 1. Logistic Regression\n# params\nipe_params = {\n    \"cvec__max_features\": [3000],",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "X = df[\"selftext_lemmatized\"]  # post\ny = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n# 1. Logistic Regression\n# params\nipe_params = {\n    \"cvec__max_features\": [3000],\n    \"cvec__min_df\": [2],",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "y = df[\"is_autism\"]  # predicting if the post is on the autism subreddit\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n# 1. Logistic Regression\n# params\nipe_params = {\n    \"cvec__max_features\": [3000],\n    \"cvec__min_df\": [2],\n    \"cvec__max_df\": [0.9],",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "ipe_params",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "ipe_params = {\n    \"cvec__max_features\": [3000],\n    \"cvec__min_df\": [2],\n    \"cvec__max_df\": [0.9],\n    \"cvec__ngram_range\": [(1, 1)],\n    \"logreg__penalty\": [\"l2\"],\n    \"logreg__C\": [1, 2, 3],\n}\nlogregmodel_lemmatized = model_iterator(\n    \"logreg\", pipe_params, model_names, lemmatizedBoolean",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "logregmodel_lemmatized",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "logregmodel_lemmatized = model_iterator(\n    \"logreg\", pipe_params, model_names, lemmatizedBoolean\n)\n# 2. Decision Tree\npipe_params_sent_len = {\n    \"cvec__max_features\": [1000, 2000, 3000],\n    \"cvec__min_df\": [2, 3],\n    \"cvec__max_df\": [0.9, 0.95],\n    \"cvec__ngram_range\": [(1, 1), (1, 2)],\n    \"dt__max_depth\": [None, 2, 3, 4],",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "pipe_params_sent_len",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "pipe_params_sent_len = {\n    \"cvec__max_features\": [1000, 2000, 3000],\n    \"cvec__min_df\": [2, 3],\n    \"cvec__max_df\": [0.9, 0.95],\n    \"cvec__ngram_range\": [(1, 1), (1, 2)],\n    \"dt__max_depth\": [None, 2, 3, 4],\n    \"dt__min_samples_split\": [2, 3, 4],\n    \"dt__min_samples_leaf\": [1, 2, 3],\n}\n# Running the model",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "dt_model_lemmatized",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "dt_model_lemmatized = model_iterator(\n    \"dt\", pipe_params_sent_len, model_names, lemmatizedBoolean\n)\n# 3. AdaBoost Model\npipe_params_sent_len = {\n    \"cvec__max_features\": [1000, 2000, 3000],\n    \"cvec__min_df\": [2, 3],\n    \"cvec__max_df\": [0.9, 0.95],\n    \"cvec__ngram_range\": [(1, 1), (1, 2)],\n    \"ada__n_estimators\": [50, 100, 150],",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "pipe_params_sent_len",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "pipe_params_sent_len = {\n    \"cvec__max_features\": [1000, 2000, 3000],\n    \"cvec__min_df\": [2, 3],\n    \"cvec__max_df\": [0.9, 0.95],\n    \"cvec__ngram_range\": [(1, 1), (1, 2)],\n    \"ada__n_estimators\": [50, 100, 150],\n    \"ada__learning_rate\": [0.1, 0.5, 1],\n}\n# Running the model\nada_model_lemmatized = model_iterator(",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "ada_model_lemmatized",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "ada_model_lemmatized = model_iterator(\n    \"ada\", pipe_params_sent_len, model_names, lemmatizedBoolean\n)\n#! Fitting Models (Takes Time)\n# fit all the models on the training data\nprint(f\"Fitting Models on unlemmatized data\")\nlogregmodel.fit(X_train, y_train)\nprint(\"Logistic Regression Model Fitted\")\ndt_model.fit(X_train, y_train)\nprint(\"Decision Tree Model Fitted\")",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "allmodels",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "allmodels = [\n    logregmodel,\n    dt_model,\n    ada_model,\n    logregmodel_lemmatized,\n    dt_model_lemmatized,\n    ada_model_lemmatized,\n]  # list of all models\n#! Results Display\nfor every_model in allmodels:",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "self.X",
        "kind": 5,
        "importPath": "scripts.012_suppl",
        "description": "scripts.012_suppl",
        "peekOfCode": "self.X = (df[\"selftext\"],)\n        self.y = df[\"is_autism\"]  #\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            self.X, self.y, random_state=self.random_state\n        )\n    def model_operation(self, df):\n        # note: be sure that the selftext col has no null values before passing it to the model function.\n        # Instantiate the CountVectorizer\n        vectorizer = CountVectorizer()\n        df[\"selftext\"] = df[\"selftext\"].fillna(",
        "detail": "scripts.012_suppl",
        "documentation": {}
    },
    {
        "label": "my_models",
        "kind": 6,
        "importPath": "scripts.014_modeling",
        "description": "scripts.014_modeling",
        "peekOfCode": "class my_models:\n    def __init__(self, df):\n        # params grid for logistic regression.\n        self.params_logreg = {\n            \"cvec__max_features\": [3000, 4000, 5000],\n            \"cvec__min_df\": [2, 3, 4],\n            \"cvec__max_df\": [0.9, 1, 0.1],\n            \"cvec__ngram_range\": [(1, 1)],\n            \"logreg__penalty\": [\"l1\", \"l2\"],\n            \"logreg__C\": [1, 2, 3],",
        "detail": "scripts.014_modeling",
        "documentation": {}
    },
    {
        "label": "run_modeling",
        "kind": 2,
        "importPath": "scripts.014_modeling",
        "description": "scripts.014_modeling",
        "peekOfCode": "def run_modeling():\n    \"\"\"\n    Problem Statement:\n    A wealthy donor with a track record of philanthropic contributions to both Autism and OCD research organizations contacted our organization, asking for a model that they can utilize to identify post characteristics on Reddit.\n    The purposes of this study (towards those ends) are to:\n    1) Use Pushshift API to scrape Reddit posts from the Autism and OCD subreddits.\n    2) To build a predictive model that can accurately predict whether a post is from the Autism or OCD subreddit\n    To accomplish these goals, we hypothesize that count vectorization, and Logistic Regression, Adaboost, or Decision Trees can be used to build a model that accurately can predict whether a post is from the Autism or OCD subreddit. Success in this study would mean that our model has a misclassification rate of less than 10 percent and an accuracy score of greater than 90 percent on the test data set.\n    \"\"\"\n    # Control Flow:",
        "detail": "scripts.014_modeling",
        "documentation": {}
    },
    {
        "label": "parse_table_of_contents",
        "kind": 2,
        "importPath": "scripts.018_report_generator_from_readme",
        "description": "scripts.018_report_generator_from_readme",
        "peekOfCode": "def parse_table_of_contents(readme_text):\n    \"\"\"\n    parse_table_of_contents takes a string of text and returns a list of tuples\n    Parameters\n    :param readme_text: a string of text\n    :type readme_text: str\n    :return: a list of tuples\n    :rtype: list\n    \"\"\"\n    # parse the table of contents from the readme.md file",
        "detail": "scripts.018_report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "startup",
        "kind": 2,
        "importPath": "scripts.018_report_generator_from_readme",
        "description": "scripts.018_report_generator_from_readme",
        "peekOfCode": "def startup():\n    # read the readme.md file\n    with open(path, \"r\") as f:\n        readme_text = f.read()\n    # parse the table of contents\n    table_of_contents = parse_table_of_contents(readme_text)\n    return table_of_contents, readme_text\nimport datetime as dt\ndef process_flow_controller():\n    # from start to finish, examine the readme and end with a populated, fully functional report jupyter notebook that can be tweaked and then presented to the end-user as the final report.",
        "detail": "scripts.018_report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "process_flow_controller",
        "kind": 2,
        "importPath": "scripts.018_report_generator_from_readme",
        "description": "scripts.018_report_generator_from_readme",
        "peekOfCode": "def process_flow_controller():\n    # from start to finish, examine the readme and end with a populated, fully functional report jupyter notebook that can be tweaked and then presented to the end-user as the final report.\n    global project_name\n    (\n        table_of_contents,\n        readme_text,\n    ) = startup()  # read the readme.md file and parse the table of contents\n    # using the table of contents, create the sections of the report notebook\n    generate_report_notebook(project_name, table_of_contents, readme_text)\n    return",
        "detail": "scripts.018_report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "create_data_section",
        "kind": 2,
        "importPath": "scripts.018_report_generator_from_readme",
        "description": "scripts.018_report_generator_from_readme",
        "peekOfCode": "def create_data_section(report_notebook):\n    global readme_text\n    # generate the data section of the report notebook\n    # the data section is a markdown cell with the text from the readme.md file for the Data Section.\n    # This section is about explaining what kinds of data are in the dataset and how the data was collected.\n    # find the line that starts with \"Data\" and\n    return\n# A markdown cell for the Data Cleaning Section (with the text from the readme.md file for the Data Cleaning Section).\n# This section is about explaining how the data was cleaned.\n# what kinds of dirtiness was found in the data and how it was cleaned?",
        "detail": "scripts.018_report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "get_text",
        "kind": 2,
        "importPath": "scripts.018_report_generator_from_readme",
        "description": "scripts.018_report_generator_from_readme",
        "peekOfCode": "def get_text(section_name, markdown_text):\n    \"\"\"\n    get_text takes a section name and a string of markdown text and returns a string of text\n    Parameters\n    :param section_name: a string of text\n    :type section_name: str\n    :param markdown_text: a string of text\n    :type markdown_text: str\n    :return: a string of text\n    :rtype: str",
        "detail": "scripts.018_report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "programmatic_pandas",
        "kind": 2,
        "importPath": "scripts.018_report_generator_from_readme",
        "description": "scripts.018_report_generator_from_readme",
        "peekOfCode": "def programmatic_pandas(readme_text, table_of_contents):\n    # clear the destination df to make it clean and new (no artifacts from previous runs).\n    great_panda_df = pd.DataFrame(\n        columns=[\"section_name\", \"section_text\", \"section_level\"]\n    )\n    # The table of contents contains tuples that have this format: ('Steps for Data Cleaning in this Study', 2) for example. The first item in the tuple is the section name and the second item is the level of the section. The level is used to denote how many \"#\" symbols to put in front of the section name.\n    # if a section is level 2, then it belongs under the section that preceeded it with a level of 1. etc. So we need to keep track of the previous section name and level.\n    # create a variable to keep track of the previous section name\n    # I want markdown cells for all cells in the table of contents that are level 1 preceeded with a level one header in a markdown cell.\n    # if there are level 2 sections, then I want a markdown cell for the level 2 section preceeded with a level 2 header in a markdown cell.",
        "detail": "scripts.018_report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "generate_report_notebook",
        "kind": 2,
        "importPath": "scripts.018_report_generator_from_readme",
        "description": "scripts.018_report_generator_from_readme",
        "peekOfCode": "def generate_report_notebook(project_name,table_of_contents, readme_text):\n    # global readme_text\n    global author, date, license_type\n    # generate a jupyter notebook based on the table of contents in the readme.md file.\n    # the pattern it uses is:\n    # A header markdown cell with the project name\n    # For each section in the table of contents:\n    #   A markdown cell with the section name\n    #   A markdown cell with the text from the readme.md file for that section\n    #   add a code cell below this markdown cell for the user to add any code they want to the section.",
        "detail": "scripts.018_report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "scripts.018_report_generator_from_readme",
        "description": "scripts.018_report_generator_from_readme",
        "peekOfCode": "path = \"readme.md\"  # same directory as this script\n# read a readme.md file and make a report jupyter notebook that has the appropriate sections (from the table of contents in the readme.md file)\ndef parse_table_of_contents(readme_text):\n    \"\"\"\n    parse_table_of_contents takes a string of text and returns a list of tuples\n    Parameters\n    :param readme_text: a string of text\n    :type readme_text: str\n    :return: a list of tuples\n    :rtype: list",
        "detail": "scripts.018_report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "project_name",
        "kind": 5,
        "importPath": "scripts.018_report_generator_from_readme",
        "description": "scripts.018_report_generator_from_readme",
        "peekOfCode": "project_name = 'Reddit NLP Project'\nauthor = 'Graham Waters'\ndate = dt.datetime.now().strftime(\"%Y-%m-%d\")\nlicense_text = \"MIT License\"\ndef generate_report_notebook(project_name,table_of_contents, readme_text):\n    # global readme_text\n    global author, date, license_type\n    # generate a jupyter notebook based on the table of contents in the readme.md file.\n    # the pattern it uses is:\n    # A header markdown cell with the project name",
        "detail": "scripts.018_report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "author",
        "kind": 5,
        "importPath": "scripts.018_report_generator_from_readme",
        "description": "scripts.018_report_generator_from_readme",
        "peekOfCode": "author = 'Graham Waters'\ndate = dt.datetime.now().strftime(\"%Y-%m-%d\")\nlicense_text = \"MIT License\"\ndef generate_report_notebook(project_name,table_of_contents, readme_text):\n    # global readme_text\n    global author, date, license_type\n    # generate a jupyter notebook based on the table of contents in the readme.md file.\n    # the pattern it uses is:\n    # A header markdown cell with the project name\n    # For each section in the table of contents:",
        "detail": "scripts.018_report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "date",
        "kind": 5,
        "importPath": "scripts.018_report_generator_from_readme",
        "description": "scripts.018_report_generator_from_readme",
        "peekOfCode": "date = dt.datetime.now().strftime(\"%Y-%m-%d\")\nlicense_text = \"MIT License\"\ndef generate_report_notebook(project_name,table_of_contents, readme_text):\n    # global readme_text\n    global author, date, license_type\n    # generate a jupyter notebook based on the table of contents in the readme.md file.\n    # the pattern it uses is:\n    # A header markdown cell with the project name\n    # For each section in the table of contents:\n    #   A markdown cell with the section name",
        "detail": "scripts.018_report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "license_text",
        "kind": 5,
        "importPath": "scripts.018_report_generator_from_readme",
        "description": "scripts.018_report_generator_from_readme",
        "peekOfCode": "license_text = \"MIT License\"\ndef generate_report_notebook(project_name,table_of_contents, readme_text):\n    # global readme_text\n    global author, date, license_type\n    # generate a jupyter notebook based on the table of contents in the readme.md file.\n    # the pattern it uses is:\n    # A header markdown cell with the project name\n    # For each section in the table of contents:\n    #   A markdown cell with the section name\n    #   A markdown cell with the text from the readme.md file for that section",
        "detail": "scripts.018_report_generator_from_readme",
        "documentation": {}
    },
    {
        "label": "parse_table_of_contents",
        "kind": 2,
        "importPath": "scripts.019_report_generator_from_readme copy",
        "description": "scripts.019_report_generator_from_readme copy",
        "peekOfCode": "def parse_table_of_contents(readme_text):\n    \"\"\"\n    parse_table_of_contents takes a string of text and returns a list of tuples\n    Parameters\n    :param readme_text: a string of text\n    :type readme_text: str\n    :return: a list of tuples\n    :rtype: list\n    \"\"\"\n    # parse the table of contents from the readme.md file",
        "detail": "scripts.019_report_generator_from_readme copy",
        "documentation": {}
    },
    {
        "label": "startup",
        "kind": 2,
        "importPath": "scripts.019_report_generator_from_readme copy",
        "description": "scripts.019_report_generator_from_readme copy",
        "peekOfCode": "def startup():\n    # read the readme.md file\n    with open(path, \"r\") as f:\n        readme_text = f.read()\n    # parse the table of contents\n    table_of_contents = parse_table_of_contents(readme_text)\n    return table_of_contents, readme_text\ndef process_flow_controller():\n    # from start to finish, examine the readme and end with a populated, fully functional report jupyter notebook that can be tweaked and then presented to the end-user as the final report.\n    (",
        "detail": "scripts.019_report_generator_from_readme copy",
        "documentation": {}
    },
    {
        "label": "process_flow_controller",
        "kind": 2,
        "importPath": "scripts.019_report_generator_from_readme copy",
        "description": "scripts.019_report_generator_from_readme copy",
        "peekOfCode": "def process_flow_controller():\n    # from start to finish, examine the readme and end with a populated, fully functional report jupyter notebook that can be tweaked and then presented to the end-user as the final report.\n    (\n        table_of_contents,\n        readme_text,\n    ) = startup()  # read the readme.md file and parse the table of contents\n    # using the table of contents, create the sections of the report notebook\n    generate_report_notebook(table_of_contents, readme_text)\n    return\n# What are the expected sections in a data science report notebook?",
        "detail": "scripts.019_report_generator_from_readme copy",
        "documentation": {}
    },
    {
        "label": "create_data_section",
        "kind": 2,
        "importPath": "scripts.019_report_generator_from_readme copy",
        "description": "scripts.019_report_generator_from_readme copy",
        "peekOfCode": "def create_data_section(report_notebook):\n    global readme_text\n    # generate the data section of the report notebook\n    # the data section is a markdown cell with the text from the readme.md file for the Data Section.\n    # This section is about explaining what kinds of data are in the dataset and how the data was collected.\n    # find the line that starts with \"Data\" and\n    return\n# A markdown cell for the Data Cleaning Section (with the text from the readme.md file for the Data Cleaning Section).\n# This section is about explaining how the data was cleaned.\n# what kinds of dirtiness was found in the data and how it was cleaned?",
        "detail": "scripts.019_report_generator_from_readme copy",
        "documentation": {}
    },
    {
        "label": "get_text",
        "kind": 2,
        "importPath": "scripts.019_report_generator_from_readme copy",
        "description": "scripts.019_report_generator_from_readme copy",
        "peekOfCode": "def get_text(section_name, markdown_text):\n    \"\"\"\n    get_text takes a section name and a string of markdown text and returns a string of text\n    Parameters\n    :param section_name: a string of text\n    :type section_name: str\n    :param markdown_text: a string of text\n    :type markdown_text: str\n    :return: a string of text\n    :rtype: str",
        "detail": "scripts.019_report_generator_from_readme copy",
        "documentation": {}
    },
    {
        "label": "programmatic_pandas",
        "kind": 2,
        "importPath": "scripts.019_report_generator_from_readme copy",
        "description": "scripts.019_report_generator_from_readme copy",
        "peekOfCode": "def programmatic_pandas(readme_text, table_of_contents):\n    # clear the destination df to make it clean and new (no artifacts from previous runs).\n    great_panda_df = pd.DataFrame(\n        columns=[\"section_name\", \"section_text\", \"section_level\"]\n    )\n    # The table of contents contains tuples that have this format: ('Steps for Data Cleaning in this Study', 2) for example. The first item in the tuple is the section name and the second item is the level of the section. The level is used to denote how many \"#\" symbols to put in front of the section name.\n    # if a section is level 2, then it belongs under the section that preceeded it with a level of 1. etc. So we need to keep track of the previous section name and level.\n    # create a variable to keep track of the previous section name\n    # I want markdown cells for all cells in the table of contents that are level 1 preceeded with a level one header in a markdown cell.\n    # if there are level 2 sections, then I want a markdown cell for the level 2 section preceeded with a level 2 header in a markdown cell.",
        "detail": "scripts.019_report_generator_from_readme copy",
        "documentation": {}
    },
    {
        "label": "create_notebook",
        "kind": 2,
        "importPath": "scripts.019_report_generator_from_readme copy",
        "description": "scripts.019_report_generator_from_readme copy",
        "peekOfCode": "def create_notebook(markdown_cells):\n    # create a notebook with the markdown cells\n    # create a notebook object\n    nb = nbf.v4.new_notebook()\n    # loop through the markdown cells\n    for markdown_cell in markdown_cells:\n        # create a markdown cell\n        markdown_cell = nbf.v4.new_markdown_cell(markdown_cell)\n        # add the markdown cell to the notebook\n        nb[\"cells\"].append(markdown_cell)",
        "detail": "scripts.019_report_generator_from_readme copy",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.019_report_generator_from_readme copy",
        "description": "scripts.019_report_generator_from_readme copy",
        "peekOfCode": "def main():\n    # read the readme text\n    readme_text = read_readme()\n    # get the table of contents\n    table_of_contents = get_table_of_contents(readme_text)\n    # get the markdown cells\n    markdown_cells = programmatic_pandas(readme_text, table_of_contents)\n    # create the notebook\n    create_notebook(markdown_cells)\ndef generate_report_notebook(table_of_contents, readme_text):",
        "detail": "scripts.019_report_generator_from_readme copy",
        "documentation": {}
    },
    {
        "label": "generate_report_notebook",
        "kind": 2,
        "importPath": "scripts.019_report_generator_from_readme copy",
        "description": "scripts.019_report_generator_from_readme copy",
        "peekOfCode": "def generate_report_notebook(table_of_contents, readme_text):\n    # global readme_text\n    # generate a jupyter notebook based on the table of contents in the readme.md file.\n    # the pattern it uses is:\n    # A header markdown cell with the project name\n    # For each section in the table of contents:\n    #   A markdown cell with the section name\n    #   A markdown cell with the text from the readme.md file for that section\n    #   add a code cell below this markdown cell for the user to add any code they want to the section.\n    # create a jupyter notebook object",
        "detail": "scripts.019_report_generator_from_readme copy",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "scripts.019_report_generator_from_readme copy",
        "description": "scripts.019_report_generator_from_readme copy",
        "peekOfCode": "path = \"readme.md\"  # same directory as this script\n# read a readme.md file and make a report jupyter notebook that has the appropriate sections (from the table of contents in the readme.md file)\ndef parse_table_of_contents(readme_text):\n    \"\"\"\n    parse_table_of_contents takes a string of text and returns a list of tuples\n    Parameters\n    :param readme_text: a string of text\n    :type readme_text: str\n    :return: a list of tuples\n    :rtype: list",
        "detail": "scripts.019_report_generator_from_readme copy",
        "documentation": {}
    },
    {
        "label": "stop",
        "kind": 5,
        "importPath": "scripts.01_word_lists",
        "description": "scripts.01_word_lists",
        "peekOfCode": "stop = stopwords.words(\"english\")\n# Custom Word Lists\nlistofknown_medications = [\n    \"clonidine\",\n    \"quetiapine\",\n    \"risperidone\",\n    \"vyvanse\",\n    \"adderall\",\n    \"dexedrine\",\n    \"wellbutrin\",",
        "detail": "scripts.01_word_lists",
        "documentation": {}
    },
    {
        "label": "listofknown_medications",
        "kind": 5,
        "importPath": "scripts.01_word_lists",
        "description": "scripts.01_word_lists",
        "peekOfCode": "listofknown_medications = [\n    \"clonidine\",\n    \"quetiapine\",\n    \"risperidone\",\n    \"vyvanse\",\n    \"adderall\",\n    \"dexedrine\",\n    \"wellbutrin\",\n    \"focalinxr\",\n    \"modafanil\",",
        "detail": "scripts.01_word_lists",
        "documentation": {}
    },
    {
        "label": "listofknown_medications",
        "kind": 5,
        "importPath": "scripts.01_word_lists",
        "description": "scripts.01_word_lists",
        "peekOfCode": "listofknown_medications = [\n    x.lower() for x in listofknown_medications\n]  # make all the words lowercase\nconditions = [\"autism\", \"ocd\"]  # list of conditions to search for\nslang_dict = {\n    \" tho \": \" though \",\n    \"thru\": \" through \",\n    \"thx\": \" thanks \",\n    \" u \": \" you \",\n    \" ur \": \" your \",",
        "detail": "scripts.01_word_lists",
        "documentation": {}
    },
    {
        "label": "conditions",
        "kind": 5,
        "importPath": "scripts.01_word_lists",
        "description": "scripts.01_word_lists",
        "peekOfCode": "conditions = [\"autism\", \"ocd\"]  # list of conditions to search for\nslang_dict = {\n    \" tho \": \" though \",\n    \"thru\": \" through \",\n    \"thx\": \" thanks \",\n    \" u \": \" you \",\n    \" ur \": \" your \",\n    \" yr \": \" your \",\n    \" yrs \": \" years \",\n    \" b \": \" be \",",
        "detail": "scripts.01_word_lists",
        "documentation": {}
    },
    {
        "label": "slang_dict",
        "kind": 5,
        "importPath": "scripts.01_word_lists",
        "description": "scripts.01_word_lists",
        "peekOfCode": "slang_dict = {\n    \" tho \": \" though \",\n    \"thru\": \" through \",\n    \"thx\": \" thanks \",\n    \" u \": \" you \",\n    \" ur \": \" your \",\n    \" yr \": \" your \",\n    \" yrs \": \" years \",\n    \" b \": \" be \",\n    \" r \": \" are \",",
        "detail": "scripts.01_word_lists",
        "documentation": {}
    },
    {
        "label": "biasing_terms",
        "kind": 5,
        "importPath": "scripts.01_word_lists",
        "description": "scripts.01_word_lists",
        "peekOfCode": "biasing_terms = [\"ocd\", \"autism\"]\n# sources\n# https://clincalc.com/DrugStats/Top300Drugs.aspx - list of top 300 drugs",
        "detail": "scripts.01_word_lists",
        "documentation": {}
    },
    {
        "label": "remove_ocd_meds",
        "kind": 2,
        "importPath": "scripts.02_feature_engineering",
        "description": "scripts.02_feature_engineering",
        "peekOfCode": "def remove_ocd_meds(text, listofknown_medications):\n    global meds\n    if len(meds) > 0:\n        log_string = f\"The latest medication mentioned is: {meds[-1]}\"\n        logging.info(log_string)\n    wordsintext = text.split(\" \")\n    for word in wordsintext:\n        if word in listofknown_medications:\n            meds.append(word)\n            text = text.replace(word, \" \")",
        "detail": "scripts.02_feature_engineering",
        "documentation": {}
    },
    {
        "label": "feature_engineer",
        "kind": 2,
        "importPath": "scripts.02_feature_engineering",
        "description": "scripts.02_feature_engineering",
        "peekOfCode": "def feature_engineer(df, listofknown_medications):\n    \"\"\"\n    summary: This function takes in a dataframe and a list of known medications and returns a dataframe with new features.\n    List of Features that will be created:\n    1. Number of words in the post (word_count)\n    2. Number of unique words in the post (unique_words)\n    2. Length of the post (number of characters)\n    4. unique words in the post (unique_word_count)\n    5. If the selftext of the post contains a known medication (medication_mentioned) (list of the meds mentioned)\n    Args:",
        "detail": "scripts.02_feature_engineering",
        "documentation": {}
    },
    {
        "label": "binarize_target_feature",
        "kind": 2,
        "importPath": "scripts.02_feature_engineering",
        "description": "scripts.02_feature_engineering",
        "peekOfCode": "def binarize_target_feature(df):\n    df[\"is_autism\"] = df[\"subreddit\"].apply(lambda x: 1 if x == \"autism\" else 0)\n    # ? Drop the 'subreddit' column\n    # note: not sure if the subreddit column is needed anymore but I will keep it for now.\n    return df\ndef run_feature_engineering(df):\n    # & Importing the Data and Engineering Features\n    df = pd.read_csv(\"./data/reddit_threads.csv\")\n    logging.info(f\"Beginning feature engineering...\")\n    # Run the functions in this script to create new features for the model.",
        "detail": "scripts.02_feature_engineering",
        "documentation": {}
    },
    {
        "label": "run_feature_engineering",
        "kind": 2,
        "importPath": "scripts.02_feature_engineering",
        "description": "scripts.02_feature_engineering",
        "peekOfCode": "def run_feature_engineering(df):\n    # & Importing the Data and Engineering Features\n    df = pd.read_csv(\"./data/reddit_threads.csv\")\n    logging.info(f\"Beginning feature engineering...\")\n    # Run the functions in this script to create new features for the model.\n    # & binarize the target feature\n    df = binarize_target_feature(df)  # binarize the target feature (i.e. 'subreddit')\n    # & create new features\n    df = feature_engineer(df, listofknown_medications)\n    # save the dataframe with the new features to a csv file 'df_after_feature_engineering.csv' in the data folder.",
        "detail": "scripts.02_feature_engineering",
        "documentation": {}
    },
    {
        "label": "OCD_Posts_With_Meds",
        "kind": 5,
        "importPath": "scripts.02_feature_engineering",
        "description": "scripts.02_feature_engineering",
        "peekOfCode": "OCD_Posts_With_Meds = (\n    0  # initialized count of posts mentioning meds from the OCD subreddit to zero.\n)\nAutism_Posts_With_Meds = (\n    0  # initialized count of posts mentioning meds from the Autism subreddit to zero.\n)\nTotal_Posts_With_Meds = 0  # the total number of posts that mention medications.\nmedications_mentioned = (\n    []\n)  # the list of all medications that are mentioned in the posts.",
        "detail": "scripts.02_feature_engineering",
        "documentation": {}
    },
    {
        "label": "Autism_Posts_With_Meds",
        "kind": 5,
        "importPath": "scripts.02_feature_engineering",
        "description": "scripts.02_feature_engineering",
        "peekOfCode": "Autism_Posts_With_Meds = (\n    0  # initialized count of posts mentioning meds from the Autism subreddit to zero.\n)\nTotal_Posts_With_Meds = 0  # the total number of posts that mention medications.\nmedications_mentioned = (\n    []\n)  # the list of all medications that are mentioned in the posts.\n# import the lists of medications and conditions from word_lists.py\nfrom word_lists import listofknown_medications, conditions\nimport pandas as pd",
        "detail": "scripts.02_feature_engineering",
        "documentation": {}
    },
    {
        "label": "Total_Posts_With_Meds",
        "kind": 5,
        "importPath": "scripts.02_feature_engineering",
        "description": "scripts.02_feature_engineering",
        "peekOfCode": "Total_Posts_With_Meds = 0  # the total number of posts that mention medications.\nmedications_mentioned = (\n    []\n)  # the list of all medications that are mentioned in the posts.\n# import the lists of medications and conditions from word_lists.py\nfrom word_lists import listofknown_medications, conditions\nimport pandas as pd\nimport numpy as np\n# Logging Setup\nimport logging",
        "detail": "scripts.02_feature_engineering",
        "documentation": {}
    },
    {
        "label": "medications_mentioned",
        "kind": 5,
        "importPath": "scripts.02_feature_engineering",
        "description": "scripts.02_feature_engineering",
        "peekOfCode": "medications_mentioned = (\n    []\n)  # the list of all medications that are mentioned in the posts.\n# import the lists of medications and conditions from word_lists.py\nfrom word_lists import listofknown_medications, conditions\nimport pandas as pd\nimport numpy as np\n# Logging Setup\nimport logging\n# set the logfile to be 'logs/feature_engineering.log'",
        "detail": "scripts.02_feature_engineering",
        "documentation": {}
    },
    {
        "label": "get_keywords",
        "kind": 2,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "def get_keywords(post):\n    \"\"\"Get the keywords from a post\"\"\"\n    # Get the keywords from the post\n    keywords = set()\n    for word in re.split(\"\\W+\", post.text):\n        if word in keywords:\n            continue\n        else:\n            keywords.add(word)\n    return keywords",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "remove_punctuation",
        "kind": 2,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "def remove_punctuation(text):\n    \"\"\"Remove punctuation from a string\"\"\"\n    return ''.join(ch for ch in text if ch not in stop_words)\n# Lower Case\ndef lowercase(text):\n    \"\"\"Lower case a string\"\"\"\n    return text.lower()\ndf_ocd = pd.read_csv('./data/ocd_thread.csv')\ndf_autism = pd.read_csv('./data/autism_thread.csv')\nlr = LogisticRegression(C=1e6)",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "lowercase",
        "kind": 2,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "def lowercase(text):\n    \"\"\"Lower case a string\"\"\"\n    return text.lower()\ndf_ocd = pd.read_csv('./data/ocd_thread.csv')\ndf_autism = pd.read_csv('./data/autism_thread.csv')\nlr = LogisticRegression(C=1e6)\nlr.fit(df_ocd.drop(columns=('self_text', 'author'), axis=1), df_ocd.target)\naccuracy = accuracy_score(lr.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\nprint(f'Accuracy: {accuracy}')\nadaboost = AdaBoostClassifier(n_estimators=100, random_state=0)",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "url",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "url = 'https://www.reddit.com/r/Autism/'\nheaders = dict()\nheaders.update(dict(Accept='application/json', Authorization='Bearer <token>'))\nresponse = requests.get(url, headers=headers)\nsoup = BeautifulSoup(response.content, \"html.parser\")\nposts = soup.findAll('div', class_=\"listing-item-container\")\ndef get_keywords(post):\n    \"\"\"Get the keywords from a post\"\"\"\n    # Get the keywords from the post\n    keywords = set()",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "headers",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "headers = dict()\nheaders.update(dict(Accept='application/json', Authorization='Bearer <token>'))\nresponse = requests.get(url, headers=headers)\nsoup = BeautifulSoup(response.content, \"html.parser\")\nposts = soup.findAll('div', class_=\"listing-item-container\")\ndef get_keywords(post):\n    \"\"\"Get the keywords from a post\"\"\"\n    # Get the keywords from the post\n    keywords = set()\n    for word in re.split(\"\\W+\", post.text):",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "response = requests.get(url, headers=headers)\nsoup = BeautifulSoup(response.content, \"html.parser\")\nposts = soup.findAll('div', class_=\"listing-item-container\")\ndef get_keywords(post):\n    \"\"\"Get the keywords from a post\"\"\"\n    # Get the keywords from the post\n    keywords = set()\n    for word in re.split(\"\\W+\", post.text):\n        if word in keywords:\n            continue",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "soup",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "soup = BeautifulSoup(response.content, \"html.parser\")\nposts = soup.findAll('div', class_=\"listing-item-container\")\ndef get_keywords(post):\n    \"\"\"Get the keywords from a post\"\"\"\n    # Get the keywords from the post\n    keywords = set()\n    for word in re.split(\"\\W+\", post.text):\n        if word in keywords:\n            continue\n        else:",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "posts",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "posts = soup.findAll('div', class_=\"listing-item-container\")\ndef get_keywords(post):\n    \"\"\"Get the keywords from a post\"\"\"\n    # Get the keywords from the post\n    keywords = set()\n    for word in re.split(\"\\W+\", post.text):\n        if word in keywords:\n            continue\n        else:\n            keywords.add(word)",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "stop_words = set(stopwords.words(\"english\"))\n# Remove Punctuation\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from a string\"\"\"\n    return ''.join(ch for ch in text if ch not in stop_words)\n# Lower Case\ndef lowercase(text):\n    \"\"\"Lower case a string\"\"\"\n    return text.lower()\ndf_ocd = pd.read_csv('./data/ocd_thread.csv')",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "df_ocd",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "df_ocd = pd.read_csv('./data/ocd_thread.csv')\ndf_autism = pd.read_csv('./data/autism_thread.csv')\nlr = LogisticRegression(C=1e6)\nlr.fit(df_ocd.drop(columns=('self_text', 'author'), axis=1), df_ocd.target)\naccuracy = accuracy_score(lr.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\nprint(f'Accuracy: {accuracy}')\nadaboost = AdaBoostClassifier(n_estimators=100, random_state=0)\nadaboost.fit(df_ocd.drop(columns=('self_text', 'author'), axis=1), df_ocd.target)\naccuracy = accuracy_score(adaboost.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\nprint(f'Accuracy: {accuracy}')",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "df_autism",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "df_autism = pd.read_csv('./data/autism_thread.csv')\nlr = LogisticRegression(C=1e6)\nlr.fit(df_ocd.drop(columns=('self_text', 'author'), axis=1), df_ocd.target)\naccuracy = accuracy_score(lr.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\nprint(f'Accuracy: {accuracy}')\nadaboost = AdaBoostClassifier(n_estimators=100, random_state=0)\nadaboost.fit(df_ocd.drop(columns=('self_text', 'author'), axis=1), df_ocd.target)\naccuracy = accuracy_score(adaboost.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\nprint(f'Accuracy: {accuracy}')\ndt = DecisionTreeClassifier(random_state=0)",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "lr = LogisticRegression(C=1e6)\nlr.fit(df_ocd.drop(columns=('self_text', 'author'), axis=1), df_ocd.target)\naccuracy = accuracy_score(lr.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\nprint(f'Accuracy: {accuracy}')\nadaboost = AdaBoostClassifier(n_estimators=100, random_state=0)\nadaboost.fit(df_ocd.drop(columns=('self_text', 'author'), axis=1), df_ocd.target)\naccuracy = accuracy_score(adaboost.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\nprint(f'Accuracy: {accuracy}')\ndt = DecisionTreeClassifier(random_state=0)\ndt.fit(df_ocd.drop(columns=('self_text', 'author'), axis=1), df_ocd.target)",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "accuracy = accuracy_score(lr.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\nprint(f'Accuracy: {accuracy}')\nadaboost = AdaBoostClassifier(n_estimators=100, random_state=0)\nadaboost.fit(df_ocd.drop(columns=('self_text', 'author'), axis=1), df_ocd.target)\naccuracy = accuracy_score(adaboost.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\nprint(f'Accuracy: {accuracy}')\ndt = DecisionTreeClassifier(random_state=0)\ndt.fit(df_ocd.drop(columns=('self_text', 'author'), axis=1), df_ocd.target)\naccuracy = accuracy_score(dt.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\nprint(f'Accuracy: {accuracy}')",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "adaboost",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "adaboost = AdaBoostClassifier(n_estimators=100, random_state=0)\nadaboost.fit(df_ocd.drop(columns=('self_text', 'author'), axis=1), df_ocd.target)\naccuracy = accuracy_score(adaboost.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\nprint(f'Accuracy: {accuracy}')\ndt = DecisionTreeClassifier(random_state=0)\ndt.fit(df_ocd.drop(columns=('self_text', 'author'), axis=1), df_ocd.target)\naccuracy = accuracy_score(dt.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\nprint(f'Accuracy: {accuracy}')\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df_ocd.drop(columns=('self_text', 'author'), axis=1).astype(str))",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "accuracy = accuracy_score(adaboost.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\nprint(f'Accuracy: {accuracy}')\ndt = DecisionTreeClassifier(random_state=0)\ndt.fit(df_ocd.drop(columns=('self_text', 'author'), axis=1), df_ocd.target)\naccuracy = accuracy_score(dt.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\nprint(f'Accuracy: {accuracy}')\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df_ocd.drop(columns=('self_text', 'author'), axis=1).astype(str))\ntf_vocab = Counter().most_common(len(stop_words)+5)\nfor i in range(10):",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "dt",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "dt = DecisionTreeClassifier(random_state=0)\ndt.fit(df_ocd.drop(columns=('self_text', 'author'), axis=1), df_ocd.target)\naccuracy = accuracy_score(dt.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\nprint(f'Accuracy: {accuracy}')\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df_ocd.drop(columns=('self_text', 'author'), axis=1).astype(str))\ntf_vocab = Counter().most_common(len(stop_words)+5)\nfor i in range(10):\n    print(i, len(set(tokens)))\n    for j, k in enumerate(tf_vocab):",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "accuracy = accuracy_score(dt.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\nprint(f'Accuracy: {accuracy}')\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df_ocd.drop(columns=('self_text', 'author'), axis=1).astype(str))\ntf_vocab = Counter().most_common(len(stop_words)+5)\nfor i in range(10):\n    print(i, len(set(tokens)))\n    for j, k in enumerate(tf_vocab):\n        if k >= 5:\n            break",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "vectorizer",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "vectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df_ocd.drop(columns=('self_text', 'author'), axis=1).astype(str))\ntf_vocab = Counter().most_common(len(stop_words)+5)\nfor i in range(10):\n    print(i, len(set(tokens)))\n    for j, k in enumerate(tf_vocab):\n        if k >= 5:\n            break\n        X_k = np.array(np.where(X == k))\n        X_j = np.zeros((0, 0))",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "X",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "X = vectorizer.fit_transform(df_ocd.drop(columns=('self_text', 'author'), axis=1).astype(str))\ntf_vocab = Counter().most_common(len(stop_words)+5)\nfor i in range(10):\n    print(i, len(set(tokens)))\n    for j, k in enumerate(tf_vocab):\n        if k >= 5:\n            break\n        X_k = np.array(np.where(X == k))\n        X_j = np.zeros((0, 0))\n        for x in X_k:",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "tf_vocab",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "tf_vocab = Counter().most_common(len(stop_words)+5)\nfor i in range(10):\n    print(i, len(set(tokens)))\n    for j, k in enumerate(tf_vocab):\n        if k >= 5:\n            break\n        X_k = np.array(np.where(X == k))\n        X_j = np.zeros((0, 0))\n        for x in X_k:\n            X_j += 1 * x",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "ax",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "ax = plt.subplot(111)\nx = df_ocd.target.values\ny = lr.predict(x)\nbar_width = .35\ncolor_map = sns.light_palette(\"Greens\", 10)\ncolors = color_map.as_hex()\nlabels = list(range(len(df_ocd.target.index)))\nrects = ax.bar(labels, y, width=bar_width, label='Predicted Probability', edgecolor=None, align=\"center\")\nax.legend(loc=\"best\")\nax.set_yticks(np.arange(0, 1.05, .25))",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "x = df_ocd.target.values\ny = lr.predict(x)\nbar_width = .35\ncolor_map = sns.light_palette(\"Greens\", 10)\ncolors = color_map.as_hex()\nlabels = list(range(len(df_ocd.target.index)))\nrects = ax.bar(labels, y, width=bar_width, label='Predicted Probability', edgecolor=None, align=\"center\")\nax.legend(loc=\"best\")\nax.set_yticks(np.arange(0, 1.05, .25))\nax.set_xticklabels(list(df_ocd.target.index))",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "y = lr.predict(x)\nbar_width = .35\ncolor_map = sns.light_palette(\"Greens\", 10)\ncolors = color_map.as_hex()\nlabels = list(range(len(df_ocd.target.index)))\nrects = ax.bar(labels, y, width=bar_width, label='Predicted Probability', edgecolor=None, align=\"center\")\nax.legend(loc=\"best\")\nax.set_yticks(np.arange(0, 1.05, .25))\nax.set_xticklabels(list(df_ocd.target.index))\nax.invert_yaxis()",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "bar_width",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "bar_width = .35\ncolor_map = sns.light_palette(\"Greens\", 10)\ncolors = color_map.as_hex()\nlabels = list(range(len(df_ocd.target.index)))\nrects = ax.bar(labels, y, width=bar_width, label='Predicted Probability', edgecolor=None, align=\"center\")\nax.legend(loc=\"best\")\nax.set_yticks(np.arange(0, 1.05, .25))\nax.set_xticklabels(list(df_ocd.target.index))\nax.invert_yaxis()\nax.set_title(\"Logistic Regression Predictions\")",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "color_map",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "color_map = sns.light_palette(\"Greens\", 10)\ncolors = color_map.as_hex()\nlabels = list(range(len(df_ocd.target.index)))\nrects = ax.bar(labels, y, width=bar_width, label='Predicted Probability', edgecolor=None, align=\"center\")\nax.legend(loc=\"best\")\nax.set_yticks(np.arange(0, 1.05, .25))\nax.set_xticklabels(list(df_ocd.target.index))\nax.invert_yaxis()\nax.set_title(\"Logistic Regression Predictions\")\nfig = plt.gcf()",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "colors",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "colors = color_map.as_hex()\nlabels = list(range(len(df_ocd.target.index)))\nrects = ax.bar(labels, y, width=bar_width, label='Predicted Probability', edgecolor=None, align=\"center\")\nax.legend(loc=\"best\")\nax.set_yticks(np.arange(0, 1.05, .25))\nax.set_xticklabels(list(df_ocd.target.index))\nax.invert_yaxis()\nax.set_title(\"Logistic Regression Predictions\")\nfig = plt.gcf()\nfig.tight_layout()",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "labels",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "labels = list(range(len(df_ocd.target.index)))\nrects = ax.bar(labels, y, width=bar_width, label='Predicted Probability', edgecolor=None, align=\"center\")\nax.legend(loc=\"best\")\nax.set_yticks(np.arange(0, 1.05, .25))\nax.set_xticklabels(list(df_ocd.target.index))\nax.invert_yaxis()\nax.set_title(\"Logistic Regression Predictions\")\nfig = plt.gcf()\nfig.tight_layout()\nconfusion_matrix = pd.crosstab(df_ocd.target, df_ocd.prediction)",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "rects",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "rects = ax.bar(labels, y, width=bar_width, label='Predicted Probability', edgecolor=None, align=\"center\")\nax.legend(loc=\"best\")\nax.set_yticks(np.arange(0, 1.05, .25))\nax.set_xticklabels(list(df_ocd.target.index))\nax.invert_yaxis()\nax.set_title(\"Logistic Regression Predictions\")\nfig = plt.gcf()\nfig.tight_layout()\nconfusion_matrix = pd.crosstab(df_ocd.target, df_ocd.prediction)\ncm = confusion_matrix(df_ocd.target, df_ocd.prediction)",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "fig",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "fig = plt.gcf()\nfig.tight_layout()\nconfusion_matrix = pd.crosstab(df_ocd.target, df_ocd.prediction)\ncm = confusion_matrix(df_ocd.target, df_ocd.prediction)\nnum_classes = cm.sum(1).max()+1\nclass_names = list(range(num_classes))\nrow_positions = np.argsort(-df_ocd.target)\ncol_indices = np.argpartition(df_ocd.target, -df_ocd.target.size-1)\nfor row_number, col_name in zip(row_positions, class_names):\n    fig = plt.figure()",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "confusion_matrix = pd.crosstab(df_ocd.target, df_ocd.prediction)\ncm = confusion_matrix(df_ocd.target, df_ocd.prediction)\nnum_classes = cm.sum(1).max()+1\nclass_names = list(range(num_classes))\nrow_positions = np.argsort(-df_ocd.target)\ncol_indices = np.argpartition(df_ocd.target, -df_ocd.target.size-1)\nfor row_number, col_name in zip(row_positions, class_names):\n    fig = plt.figure()\n    ax = plt.subplot(2, num_classes, row_number)\n    cnt = conf_matrix.iloc.get_value(row_position=row_number, column_label=col_name)",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "cm",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "cm = confusion_matrix(df_ocd.target, df_ocd.prediction)\nnum_classes = cm.sum(1).max()+1\nclass_names = list(range(num_classes))\nrow_positions = np.argsort(-df_ocd.target)\ncol_indices = np.argpartition(df_ocd.target, -df_ocd.target.size-1)\nfor row_number, col_name in zip(row_positions, class_names):\n    fig = plt.figure()\n    ax = plt.subplot(2, num_classes, row_number)\n    cnt = conf_matrix.iloc.get_value(row_position=row_number, column_label=col_name)\n    ax.barh(class_names, cnt)",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "num_classes = cm.sum(1).max()+1\nclass_names = list(range(num_classes))\nrow_positions = np.argsort(-df_ocd.target)\ncol_indices = np.argpartition(df_ocd.target, -df_ocd.target.size-1)\nfor row_number, col_name in zip(row_positions, class_names):\n    fig = plt.figure()\n    ax = plt.subplot(2, num_classes, row_number)\n    cnt = conf_matrix.iloc.get_value(row_position=row_number, column_label=col_name)\n    ax.barh(class_names, cnt)\n    ax.set_ylabel(col_name)",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "class_names",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "class_names = list(range(num_classes))\nrow_positions = np.argsort(-df_ocd.target)\ncol_indices = np.argpartition(df_ocd.target, -df_ocd.target.size-1)\nfor row_number, col_name in zip(row_positions, class_names):\n    fig = plt.figure()\n    ax = plt.subplot(2, num_classes, row_number)\n    cnt = conf_matrix.iloc.get_value(row_position=row_number, column_label=col_name)\n    ax.barh(class_names, cnt)\n    ax.set_ylabel(col_name)\n    ax.set_xlim(0, num_classes)",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "row_positions",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "row_positions = np.argsort(-df_ocd.target)\ncol_indices = np.argpartition(df_ocd.target, -df_ocd.target.size-1)\nfor row_number, col_name in zip(row_positions, class_names):\n    fig = plt.figure()\n    ax = plt.subplot(2, num_classes, row_number)\n    cnt = conf_matrix.iloc.get_value(row_position=row_number, column_label=col_name)\n    ax.barh(class_names, cnt)\n    ax.set_ylabel(col_name)\n    ax.set_xlim(0, num_classes)\n    ax.set_xticks(np.arange(0, num_classes, 1))",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "col_indices",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "col_indices = np.argpartition(df_ocd.target, -df_ocd.target.size-1)\nfor row_number, col_name in zip(row_positions, class_names):\n    fig = plt.figure()\n    ax = plt.subplot(2, num_classes, row_number)\n    cnt = conf_matrix.iloc.get_value(row_position=row_number, column_label=col_name)\n    ax.barh(class_names, cnt)\n    ax.set_ylabel(col_name)\n    ax.set_xlim(0, num_classes)\n    ax.set_xticks(np.arange(0, num_classes, 1))\n    ax.grid()",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "linkage_obj",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "linkage_obj = linkage(distance_func=euclidean)\ndendro = linkage_obj.apply(X)\nplt.figure()\nplt.show()",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "dendro",
        "kind": 5,
        "importPath": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "description": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "peekOfCode": "dendro = linkage_obj.apply(X)\nplt.figure()\nplt.show()",
        "detail": "scripts.06_ocdvsautismaredditthreadnlpanalysis_python_code",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "scripts.08_data_exploration",
        "description": "scripts.08_data_exploration",
        "peekOfCode": "df = pd.read_csv(\"data/reddit_threads.csv\")  # The combined data\n# # Explore the data\n# 1. Top Bigrams in the post selftext for r/Autism\n# 2. Top Bigrams in the post selftext for r/OCD\n# 3. Top Trigrams in the post selftext for r/Autism\n# 4. Top Trigrams in the post selftext for r/OCD\n# 5. Top Words in the post selftext for r/Autism\n# 6. Top Words in the post selftext for r/OCD\n# 7. Top Users posting on r/Autism in the data\n# 8. Top Users posting on r/OCD in the data",
        "detail": "scripts.08_data_exploration",
        "documentation": {}
    },
    {
        "label": "censor_words",
        "kind": 2,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "def censor_words(text):\n    # Remove all words that begin with 'aut' from the sentence and return the result\n    # regex pattern\n    pattern = r'aut(.*?)[^a-zA-Z]' # aut followed by any number of characters then ending in any character that is not a letter\n    # replace those pattern matches with '' (nothing)\n    text =  re.sub(pattern, '', text) # replace the pattern matches with '' (nothing)\n    # pattern 2 - remove all words that begin with 'ocd' from the sentence and return the result\n    pattern = r'ocd(.*?)[^a-zA-Z]' # ocd followed by any number of characters then ending in any character that is not a letter\n    # replace those pattern matches with '' (nothing)\n    text =  re.sub(pattern, '', text) # replace the pattern matches with '' (nothing)",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "get_keywords",
        "kind": 2,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "def get_keywords(post):\n    \"\"\"Get the keywords from a post\"\"\"\n    # Get the keywords from the post\n    keywords = set()\n    for word in re.split(\"\\W+\", post.text):\n        if word in keywords:\n            continue\n        else:\n            keywords.add(word)\n    return keywords",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "remove_punctuation",
        "kind": 2,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "def remove_punctuation(text):\n    \"\"\"Remove punctuation from a string\"\"\"\n    return ''.join(ch for ch in text if ch not in stop_words)\n# Lower Case\ndef lowercase(text):\n    \"\"\"Lower case a string\"\"\"\n    return text.lower()\n# %% [markdown]\n# # Importing the Data\n# %%",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "lowercase",
        "kind": 2,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "def lowercase(text):\n    \"\"\"Lower case a string\"\"\"\n    return text.lower()\n# %% [markdown]\n# # Importing the Data\n# %%\n# opening the scraped data saved in csv files and creating a dataframe for each\ndf_ocd = pd.read_csv('../data/ocd_thread.csv')\ndf_autism = pd.read_csv('../data/autism_thread.csv')\n# creating a target column for each dataframe",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "num_distinct_words",
        "kind": 2,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "def num_distinct_words(text,df):\n    # for this text, find the words that do not appear in any other text in the dataframe column 'selftext'\n    # split the text into a list of words\n    if type(text) == str:\n        text = text.split(' ')\n        # find the number of words that are not in any other text in the dataframe\n    else:\n        # the text is a list of words\n        words = text\n        pass",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "random_color",
        "kind": 2,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "def random_color():\n    return '#%06x' % random.randint(0, 0xFFFFFF) # this will generate a random hex color code\ndf['author_color'] = df['author'].map({author: random_color() for author in df['author'].unique()}) # add a column to the df that shows the color for each author\n# sample\ndf.head()\n# %%\n# plot the sentiment values for the top 100 authors (by number of posts) on their own subplots as scatterplots with the x-axis being the created_utc, y-axis being the selftext_length, and color representing the sentiment value column.\n# add a space between the plots to make them easier to read and to make the plot more aesthetically pleasing\n# for this code block ignore the IndexError\n# use coolwarm for the color map to make the colors more distinct",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "stop_words = set(stopwords.words(\"english\"))\n# Remove Punctuation\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from a string\"\"\"\n    return ''.join(ch for ch in text if ch not in stop_words)\n# Lower Case\ndef lowercase(text):\n    \"\"\"Lower case a string\"\"\"\n    return text.lower()\n# %% [markdown]",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd = pd.read_csv('../data/ocd_thread.csv')\ndf_autism = pd.read_csv('../data/autism_thread.csv')\n# creating a target column for each dataframe\ndf_ocd['target'] = 1\ndf_autism['target'] = 0\n# remove posts before 1546777579 and after 1596525852\nprint(f'number of rows in df before removing posts before 1546777579 and after 1596525852: {df.shape[0]}')\npreshape = df_ocd.shape[0]\ndf_ocd = df_ocd[(df_ocd['created_utc'] > 1546777579) & (df_ocd['created_utc'] < 1596525852)]\n# drop rows where `is_video` or `media_only` columns are True",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism = pd.read_csv('../data/autism_thread.csv')\n# creating a target column for each dataframe\ndf_ocd['target'] = 1\ndf_autism['target'] = 0\n# remove posts before 1546777579 and after 1596525852\nprint(f'number of rows in df before removing posts before 1546777579 and after 1596525852: {df.shape[0]}')\npreshape = df_ocd.shape[0]\ndf_ocd = df_ocd[(df_ocd['created_utc'] > 1546777579) & (df_ocd['created_utc'] < 1596525852)]\n# drop rows where `is_video` or `media_only` columns are True\ndf_ocd = df_ocd[(df_ocd['is_video'] == False) & (df_ocd['media_only'] == False)]",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd['target']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd['target'] = 1\ndf_autism['target'] = 0\n# remove posts before 1546777579 and after 1596525852\nprint(f'number of rows in df before removing posts before 1546777579 and after 1596525852: {df.shape[0]}')\npreshape = df_ocd.shape[0]\ndf_ocd = df_ocd[(df_ocd['created_utc'] > 1546777579) & (df_ocd['created_utc'] < 1596525852)]\n# drop rows where `is_video` or `media_only` columns are True\ndf_ocd = df_ocd[(df_ocd['is_video'] == False) & (df_ocd['media_only'] == False)]\ndf_autism = df_autism[(df_autism['is_video'] == False) & (df_autism['media_only'] == False)]\nprint(f'Dimensions after removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism['target']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism['target'] = 0\n# remove posts before 1546777579 and after 1596525852\nprint(f'number of rows in df before removing posts before 1546777579 and after 1596525852: {df.shape[0]}')\npreshape = df_ocd.shape[0]\ndf_ocd = df_ocd[(df_ocd['created_utc'] > 1546777579) & (df_ocd['created_utc'] < 1596525852)]\n# drop rows where `is_video` or `media_only` columns are True\ndf_ocd = df_ocd[(df_ocd['is_video'] == False) & (df_ocd['media_only'] == False)]\ndf_autism = df_autism[(df_autism['is_video'] == False) & (df_autism['media_only'] == False)]\nprint(f'Dimensions after removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n# drop columns with more than 50% missing values from the dataframes",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "preshape",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "preshape = df_ocd.shape[0]\ndf_ocd = df_ocd[(df_ocd['created_utc'] > 1546777579) & (df_ocd['created_utc'] < 1596525852)]\n# drop rows where `is_video` or `media_only` columns are True\ndf_ocd = df_ocd[(df_ocd['is_video'] == False) & (df_ocd['media_only'] == False)]\ndf_autism = df_autism[(df_autism['is_video'] == False) & (df_autism['media_only'] == False)]\nprint(f'Dimensions after removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n# drop columns with more than 50% missing values from the dataframes\nprint(f'Dimensions before dropping columns with more than 50% missing values: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\ndf_ocd = df_ocd.dropna(thresh=0.5*len(df_ocd), axis=1)\ndf_autism = df_autism.dropna(thresh=0.5*len(df_autism), axis=1)",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd = df_ocd[(df_ocd['created_utc'] > 1546777579) & (df_ocd['created_utc'] < 1596525852)]\n# drop rows where `is_video` or `media_only` columns are True\ndf_ocd = df_ocd[(df_ocd['is_video'] == False) & (df_ocd['media_only'] == False)]\ndf_autism = df_autism[(df_autism['is_video'] == False) & (df_autism['media_only'] == False)]\nprint(f'Dimensions after removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n# drop columns with more than 50% missing values from the dataframes\nprint(f'Dimensions before dropping columns with more than 50% missing values: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\ndf_ocd = df_ocd.dropna(thresh=0.5*len(df_ocd), axis=1)\ndf_autism = df_autism.dropna(thresh=0.5*len(df_autism), axis=1)\nprint(f'Dimensions after dropping columns with more than 50% missing values: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd = df_ocd[(df_ocd['is_video'] == False) & (df_ocd['media_only'] == False)]\ndf_autism = df_autism[(df_autism['is_video'] == False) & (df_autism['media_only'] == False)]\nprint(f'Dimensions after removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n# drop columns with more than 50% missing values from the dataframes\nprint(f'Dimensions before dropping columns with more than 50% missing values: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\ndf_ocd = df_ocd.dropna(thresh=0.5*len(df_ocd), axis=1)\ndf_autism = df_autism.dropna(thresh=0.5*len(df_autism), axis=1)\nprint(f'Dimensions after dropping columns with more than 50% missing values: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\nprint(f'columns in df_ocd: {df_ocd.columns}')\n#* Only keep the columns in these two dataframes that are in both dataframes and are in the lists below",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism = df_autism[(df_autism['is_video'] == False) & (df_autism['media_only'] == False)]\nprint(f'Dimensions after removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n# drop columns with more than 50% missing values from the dataframes\nprint(f'Dimensions before dropping columns with more than 50% missing values: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\ndf_ocd = df_ocd.dropna(thresh=0.5*len(df_ocd), axis=1)\ndf_autism = df_autism.dropna(thresh=0.5*len(df_autism), axis=1)\nprint(f'Dimensions after dropping columns with more than 50% missing values: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\nprint(f'columns in df_ocd: {df_ocd.columns}')\n#* Only keep the columns in these two dataframes that are in both dataframes and are in the lists below\nautism_columns_to_keep = ['author', 'author_flair_richtext', 'author_flair_type','created_utc', 'id', 'is_video', 'selftext', 'title', 'is_original_content','media_only', 'author_fullname','target']",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd = df_ocd.dropna(thresh=0.5*len(df_ocd), axis=1)\ndf_autism = df_autism.dropna(thresh=0.5*len(df_autism), axis=1)\nprint(f'Dimensions after dropping columns with more than 50% missing values: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\nprint(f'columns in df_ocd: {df_ocd.columns}')\n#* Only keep the columns in these two dataframes that are in both dataframes and are in the lists below\nautism_columns_to_keep = ['author', 'author_flair_richtext', 'author_flair_type','created_utc', 'id', 'is_video', 'selftext', 'title', 'is_original_content','media_only', 'author_fullname','target']\nocd_columns_to_keep = ['author', 'author_flair_richtext', 'author_flair_type','created_utc', 'id', 'is_video', 'selftext', 'title', 'is_original_content','media_only', 'author_fullname','target']\n# drop columns that are not in the lists above\nprint(f'Dimensions before dropping columns that are not in the lists above: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\ndf_ocd = df_ocd[ocd_columns_to_keep]",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism = df_autism.dropna(thresh=0.5*len(df_autism), axis=1)\nprint(f'Dimensions after dropping columns with more than 50% missing values: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\nprint(f'columns in df_ocd: {df_ocd.columns}')\n#* Only keep the columns in these two dataframes that are in both dataframes and are in the lists below\nautism_columns_to_keep = ['author', 'author_flair_richtext', 'author_flair_type','created_utc', 'id', 'is_video', 'selftext', 'title', 'is_original_content','media_only', 'author_fullname','target']\nocd_columns_to_keep = ['author', 'author_flair_richtext', 'author_flair_type','created_utc', 'id', 'is_video', 'selftext', 'title', 'is_original_content','media_only', 'author_fullname','target']\n# drop columns that are not in the lists above\nprint(f'Dimensions before dropping columns that are not in the lists above: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\ndf_ocd = df_ocd[ocd_columns_to_keep]\ndf_autism = df_autism[autism_columns_to_keep]",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "autism_columns_to_keep",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "autism_columns_to_keep = ['author', 'author_flair_richtext', 'author_flair_type','created_utc', 'id', 'is_video', 'selftext', 'title', 'is_original_content','media_only', 'author_fullname','target']\nocd_columns_to_keep = ['author', 'author_flair_richtext', 'author_flair_type','created_utc', 'id', 'is_video', 'selftext', 'title', 'is_original_content','media_only', 'author_fullname','target']\n# drop columns that are not in the lists above\nprint(f'Dimensions before dropping columns that are not in the lists above: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\ndf_ocd = df_ocd[ocd_columns_to_keep]\ndf_autism = df_autism[autism_columns_to_keep]\nprint(f'Dimensions after dropping columns that are not in the lists above: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n# Now remove any posts from these dataframes where the `is_video` or `media_only` columns are True\nprint(f'Dimensions before removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\ndf_ocd = df_ocd[(df_ocd['is_video'] == False) & (df_ocd['media_only'] == False)]",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "ocd_columns_to_keep",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "ocd_columns_to_keep = ['author', 'author_flair_richtext', 'author_flair_type','created_utc', 'id', 'is_video', 'selftext', 'title', 'is_original_content','media_only', 'author_fullname','target']\n# drop columns that are not in the lists above\nprint(f'Dimensions before dropping columns that are not in the lists above: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\ndf_ocd = df_ocd[ocd_columns_to_keep]\ndf_autism = df_autism[autism_columns_to_keep]\nprint(f'Dimensions after dropping columns that are not in the lists above: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n# Now remove any posts from these dataframes where the `is_video` or `media_only` columns are True\nprint(f'Dimensions before removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\ndf_ocd = df_ocd[(df_ocd['is_video'] == False) & (df_ocd['media_only'] == False)]\ndf_autism = df_autism[(df_autism['is_video'] == False) & (df_autism['media_only'] == False)]",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd = df_ocd[ocd_columns_to_keep]\ndf_autism = df_autism[autism_columns_to_keep]\nprint(f'Dimensions after dropping columns that are not in the lists above: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n# Now remove any posts from these dataframes where the `is_video` or `media_only` columns are True\nprint(f'Dimensions before removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\ndf_ocd = df_ocd[(df_ocd['is_video'] == False) & (df_ocd['media_only'] == False)]\ndf_autism = df_autism[(df_autism['is_video'] == False) & (df_autism['media_only'] == False)]\nprint(f'Dimensions after removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n# and now we can drop the `is_video` and `media_only` columns\ndf_ocd = df_ocd.drop(columns=['is_video', 'media_only'])",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism = df_autism[autism_columns_to_keep]\nprint(f'Dimensions after dropping columns that are not in the lists above: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n# Now remove any posts from these dataframes where the `is_video` or `media_only` columns are True\nprint(f'Dimensions before removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\ndf_ocd = df_ocd[(df_ocd['is_video'] == False) & (df_ocd['media_only'] == False)]\ndf_autism = df_autism[(df_autism['is_video'] == False) & (df_autism['media_only'] == False)]\nprint(f'Dimensions after removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n# and now we can drop the `is_video` and `media_only` columns\ndf_ocd = df_ocd.drop(columns=['is_video', 'media_only'])\ndf_autism = df_autism.drop(columns=['is_video', 'media_only'])",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd = df_ocd[(df_ocd['is_video'] == False) & (df_ocd['media_only'] == False)]\ndf_autism = df_autism[(df_autism['is_video'] == False) & (df_autism['media_only'] == False)]\nprint(f'Dimensions after removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n# and now we can drop the `is_video` and `media_only` columns\ndf_ocd = df_ocd.drop(columns=['is_video', 'media_only'])\ndf_autism = df_autism.drop(columns=['is_video', 'media_only'])\nprint(f'Dropped the `is_video` and `media_only` columns')\n# some posts are in the title column and some are in the selftext column so we need to combine these columns into one column if they are long enough.\n# find the median length of the title and selftext columns combined for each dataframe\nmed_len_title_selftext_ocd = df_ocd.title.str.len().add(df_ocd.selftext.str.len()).median()",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism = df_autism[(df_autism['is_video'] == False) & (df_autism['media_only'] == False)]\nprint(f'Dimensions after removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n# and now we can drop the `is_video` and `media_only` columns\ndf_ocd = df_ocd.drop(columns=['is_video', 'media_only'])\ndf_autism = df_autism.drop(columns=['is_video', 'media_only'])\nprint(f'Dropped the `is_video` and `media_only` columns')\n# some posts are in the title column and some are in the selftext column so we need to combine these columns into one column if they are long enough.\n# find the median length of the title and selftext columns combined for each dataframe\nmed_len_title_selftext_ocd = df_ocd.title.str.len().add(df_ocd.selftext.str.len()).median()\nmed_len_title_selftext_autism = df_autism.title.str.len().add(df_autism.selftext.str.len()).median()",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd = df_ocd.drop(columns=['is_video', 'media_only'])\ndf_autism = df_autism.drop(columns=['is_video', 'media_only'])\nprint(f'Dropped the `is_video` and `media_only` columns')\n# some posts are in the title column and some are in the selftext column so we need to combine these columns into one column if they are long enough.\n# find the median length of the title and selftext columns combined for each dataframe\nmed_len_title_selftext_ocd = df_ocd.title.str.len().add(df_ocd.selftext.str.len()).median()\nmed_len_title_selftext_autism = df_autism.title.str.len().add(df_autism.selftext.str.len()).median()\nprint(f'Median length of title and selftext columns combined for OCD: {med_len_title_selftext_ocd}')\nprint(f'Median length of title and selftext columns combined for Autism: {med_len_title_selftext_autism}')\n# how many posts have a title and selftext combined that are longer than the median length of the title and selftext columns combined for each dataframe?",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism = df_autism.drop(columns=['is_video', 'media_only'])\nprint(f'Dropped the `is_video` and `media_only` columns')\n# some posts are in the title column and some are in the selftext column so we need to combine these columns into one column if they are long enough.\n# find the median length of the title and selftext columns combined for each dataframe\nmed_len_title_selftext_ocd = df_ocd.title.str.len().add(df_ocd.selftext.str.len()).median()\nmed_len_title_selftext_autism = df_autism.title.str.len().add(df_autism.selftext.str.len()).median()\nprint(f'Median length of title and selftext columns combined for OCD: {med_len_title_selftext_ocd}')\nprint(f'Median length of title and selftext columns combined for Autism: {med_len_title_selftext_autism}')\n# how many posts have a title and selftext combined that are longer than the median length of the title and selftext columns combined for each dataframe?\nprint(f'Acceptable number of OCD posts: {len(df_ocd[df_ocd.title.str.len().add(df_ocd.selftext.str.len()) > med_len_title_selftext_ocd])}')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "med_len_title_selftext_ocd",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "med_len_title_selftext_ocd = df_ocd.title.str.len().add(df_ocd.selftext.str.len()).median()\nmed_len_title_selftext_autism = df_autism.title.str.len().add(df_autism.selftext.str.len()).median()\nprint(f'Median length of title and selftext columns combined for OCD: {med_len_title_selftext_ocd}')\nprint(f'Median length of title and selftext columns combined for Autism: {med_len_title_selftext_autism}')\n# how many posts have a title and selftext combined that are longer than the median length of the title and selftext columns combined for each dataframe?\nprint(f'Acceptable number of OCD posts: {len(df_ocd[df_ocd.title.str.len().add(df_ocd.selftext.str.len()) > med_len_title_selftext_ocd])}')\nprint(f'Acceptable number of Autism posts: {len(df_autism[df_autism.title.str.len().add(df_autism.selftext.str.len()) > med_len_title_selftext_autism])}')\n# remove posts where the title and selftext combined are shorter than the median length of the title and selftext columns combined for each dataframe\nprint(f'Dimensions before: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\ndf_ocd = df_ocd[df_ocd.title.str.len().add(df_ocd.selftext.str.len()) > med_len_title_selftext_ocd]",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "med_len_title_selftext_autism",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "med_len_title_selftext_autism = df_autism.title.str.len().add(df_autism.selftext.str.len()).median()\nprint(f'Median length of title and selftext columns combined for OCD: {med_len_title_selftext_ocd}')\nprint(f'Median length of title and selftext columns combined for Autism: {med_len_title_selftext_autism}')\n# how many posts have a title and selftext combined that are longer than the median length of the title and selftext columns combined for each dataframe?\nprint(f'Acceptable number of OCD posts: {len(df_ocd[df_ocd.title.str.len().add(df_ocd.selftext.str.len()) > med_len_title_selftext_ocd])}')\nprint(f'Acceptable number of Autism posts: {len(df_autism[df_autism.title.str.len().add(df_autism.selftext.str.len()) > med_len_title_selftext_autism])}')\n# remove posts where the title and selftext combined are shorter than the median length of the title and selftext columns combined for each dataframe\nprint(f'Dimensions before: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\ndf_ocd = df_ocd[df_ocd.title.str.len().add(df_ocd.selftext.str.len()) > med_len_title_selftext_ocd]\ndf_autism = df_autism[df_autism.title.str.len().add(df_autism.selftext.str.len()) > med_len_title_selftext_autism]",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd = df_ocd[df_ocd.title.str.len().add(df_ocd.selftext.str.len()) > med_len_title_selftext_ocd]\ndf_autism = df_autism[df_autism.title.str.len().add(df_autism.selftext.str.len()) > med_len_title_selftext_autism]\nprint(f'Dimensions before: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n# drop author_flair_richtext\ndf_ocd = df_ocd.drop(columns=['author_flair_richtext'])\ndf_autism = df_autism.drop(columns=['author_flair_richtext'])\n# how many authors are in each dataframe?\nprint(f'Number of authors in df_ocd: {len(df_ocd.author.unique())}')\nprint(f'Number of authors in df_autism: {len(df_autism.author.unique())}')\n# how many posts are there for the top 100 authors in each dataframe?",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism = df_autism[df_autism.title.str.len().add(df_autism.selftext.str.len()) > med_len_title_selftext_autism]\nprint(f'Dimensions before: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n# drop author_flair_richtext\ndf_ocd = df_ocd.drop(columns=['author_flair_richtext'])\ndf_autism = df_autism.drop(columns=['author_flair_richtext'])\n# how many authors are in each dataframe?\nprint(f'Number of authors in df_ocd: {len(df_ocd.author.unique())}')\nprint(f'Number of authors in df_autism: {len(df_autism.author.unique())}')\n# how many posts are there for the top 100 authors in each dataframe?\ntop_authors_ocd = df_ocd.author.value_counts().head(100)",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd = df_ocd.drop(columns=['author_flair_richtext'])\ndf_autism = df_autism.drop(columns=['author_flair_richtext'])\n# how many authors are in each dataframe?\nprint(f'Number of authors in df_ocd: {len(df_ocd.author.unique())}')\nprint(f'Number of authors in df_autism: {len(df_autism.author.unique())}')\n# how many posts are there for the top 100 authors in each dataframe?\ntop_authors_ocd = df_ocd.author.value_counts().head(100)\ntop_authors_byfullname_ocd = df_ocd.author_fullname.value_counts().head(100)\ntop_authors_autism = df_autism.author.value_counts().head(100)\ntop_authors_byfullname_autism = df_autism.author_fullname.value_counts().head(100)",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism = df_autism.drop(columns=['author_flair_richtext'])\n# how many authors are in each dataframe?\nprint(f'Number of authors in df_ocd: {len(df_ocd.author.unique())}')\nprint(f'Number of authors in df_autism: {len(df_autism.author.unique())}')\n# how many posts are there for the top 100 authors in each dataframe?\ntop_authors_ocd = df_ocd.author.value_counts().head(100)\ntop_authors_byfullname_ocd = df_ocd.author_fullname.value_counts().head(100)\ntop_authors_autism = df_autism.author.value_counts().head(100)\ntop_authors_byfullname_autism = df_autism.author_fullname.value_counts().head(100)\n# %%",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "top_authors_ocd",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "top_authors_ocd = df_ocd.author.value_counts().head(100)\ntop_authors_byfullname_ocd = df_ocd.author_fullname.value_counts().head(100)\ntop_authors_autism = df_autism.author.value_counts().head(100)\ntop_authors_byfullname_autism = df_autism.author_fullname.value_counts().head(100)\n# %%\ntop_authors_ocd.head(2)\n# %%\ntop_authors_autism.head(2)\n# %%\n# are there any authors that are in both dataframes?",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "top_authors_byfullname_ocd",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "top_authors_byfullname_ocd = df_ocd.author_fullname.value_counts().head(100)\ntop_authors_autism = df_autism.author.value_counts().head(100)\ntop_authors_byfullname_autism = df_autism.author_fullname.value_counts().head(100)\n# %%\ntop_authors_ocd.head(2)\n# %%\ntop_authors_autism.head(2)\n# %%\n# are there any authors that are in both dataframes?\nprint(f'Number of authors that are in both dataframes: {len(set(top_authors_ocd.index).intersection(set(top_authors_autism.index)))}')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "top_authors_autism",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "top_authors_autism = df_autism.author.value_counts().head(100)\ntop_authors_byfullname_autism = df_autism.author_fullname.value_counts().head(100)\n# %%\ntop_authors_ocd.head(2)\n# %%\ntop_authors_autism.head(2)\n# %%\n# are there any authors that are in both dataframes?\nprint(f'Number of authors that are in both dataframes: {len(set(top_authors_ocd.index).intersection(set(top_authors_autism.index)))}')\nlist_of_cross_posters = list(set(top_authors_ocd.index).intersection(set(top_authors_autism.index)))",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "top_authors_byfullname_autism",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "top_authors_byfullname_autism = df_autism.author_fullname.value_counts().head(100)\n# %%\ntop_authors_ocd.head(2)\n# %%\ntop_authors_autism.head(2)\n# %%\n# are there any authors that are in both dataframes?\nprint(f'Number of authors that are in both dataframes: {len(set(top_authors_ocd.index).intersection(set(top_authors_autism.index)))}')\nlist_of_cross_posters = list(set(top_authors_ocd.index).intersection(set(top_authors_autism.index)))\nprint(f'List of authors that are in both dataframes: {list_of_cross_posters}')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "list_of_cross_posters",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "list_of_cross_posters = list(set(top_authors_ocd.index).intersection(set(top_authors_autism.index)))\nprint(f'List of authors that are in both dataframes: {list_of_cross_posters}')\n# %%\n# drop author_flair_type and author_fullname columns from both dataframes\ndf_ocd = df_ocd.drop(columns=['author_flair_type', 'author_fullname'])\ndf_autism = df_autism.drop(columns=['author_flair_type', 'author_fullname'])\n# combine the title and self text columns into one column with the format `title - selftext`\ndf_ocd['title_selftext'] = df_ocd.title + ' - ' + df_ocd.selftext\ndf_autism['title_selftext'] = df_autism.title + ' - ' + df_autism.selftext\n# drop the title and selftext columns",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd = df_ocd.drop(columns=['author_flair_type', 'author_fullname'])\ndf_autism = df_autism.drop(columns=['author_flair_type', 'author_fullname'])\n# combine the title and self text columns into one column with the format `title - selftext`\ndf_ocd['title_selftext'] = df_ocd.title + ' - ' + df_ocd.selftext\ndf_autism['title_selftext'] = df_autism.title + ' - ' + df_autism.selftext\n# drop the title and selftext columns\ndf_ocd = df_ocd.drop(columns=['title', 'selftext'])\ndf_autism = df_autism.drop(columns=['title', 'selftext'])\n# rename the `title_selftext` column to `selftext`\ndf_ocd = df_ocd.rename(columns={'title_selftext': 'selftext'})",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism = df_autism.drop(columns=['author_flair_type', 'author_fullname'])\n# combine the title and self text columns into one column with the format `title - selftext`\ndf_ocd['title_selftext'] = df_ocd.title + ' - ' + df_ocd.selftext\ndf_autism['title_selftext'] = df_autism.title + ' - ' + df_autism.selftext\n# drop the title and selftext columns\ndf_ocd = df_ocd.drop(columns=['title', 'selftext'])\ndf_autism = df_autism.drop(columns=['title', 'selftext'])\n# rename the `title_selftext` column to `selftext`\ndf_ocd = df_ocd.rename(columns={'title_selftext': 'selftext'})\ndf_autism = df_autism.rename(columns={'title_selftext': 'selftext'})",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd['title_selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd['title_selftext'] = df_ocd.title + ' - ' + df_ocd.selftext\ndf_autism['title_selftext'] = df_autism.title + ' - ' + df_autism.selftext\n# drop the title and selftext columns\ndf_ocd = df_ocd.drop(columns=['title', 'selftext'])\ndf_autism = df_autism.drop(columns=['title', 'selftext'])\n# rename the `title_selftext` column to `selftext`\ndf_ocd = df_ocd.rename(columns={'title_selftext': 'selftext'})\ndf_autism = df_autism.rename(columns={'title_selftext': 'selftext'})\n# randomly sample one post from each dataframe and print it\nprint(f'Random OCD post: {df_ocd.sample(1).selftext.values[0]}')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism['title_selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism['title_selftext'] = df_autism.title + ' - ' + df_autism.selftext\n# drop the title and selftext columns\ndf_ocd = df_ocd.drop(columns=['title', 'selftext'])\ndf_autism = df_autism.drop(columns=['title', 'selftext'])\n# rename the `title_selftext` column to `selftext`\ndf_ocd = df_ocd.rename(columns={'title_selftext': 'selftext'})\ndf_autism = df_autism.rename(columns={'title_selftext': 'selftext'})\n# randomly sample one post from each dataframe and print it\nprint(f'Random OCD post: {df_ocd.sample(1).selftext.values[0]}')\nprint('='*100)",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd = df_ocd.drop(columns=['title', 'selftext'])\ndf_autism = df_autism.drop(columns=['title', 'selftext'])\n# rename the `title_selftext` column to `selftext`\ndf_ocd = df_ocd.rename(columns={'title_selftext': 'selftext'})\ndf_autism = df_autism.rename(columns={'title_selftext': 'selftext'})\n# randomly sample one post from each dataframe and print it\nprint(f'Random OCD post: {df_ocd.sample(1).selftext.values[0]}')\nprint('='*100)\nprint(f'Random Autism post: {df_autism.sample(1).selftext.values[0]}')\ncancel_words = ['ocd','aut*','autism','obsess*','compuls*','disorder','executive dysfunction','adhd','diagnosis','ive been taking','spectrum','intrusive thoughts','germaphobes','depression']",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism = df_autism.drop(columns=['title', 'selftext'])\n# rename the `title_selftext` column to `selftext`\ndf_ocd = df_ocd.rename(columns={'title_selftext': 'selftext'})\ndf_autism = df_autism.rename(columns={'title_selftext': 'selftext'})\n# randomly sample one post from each dataframe and print it\nprint(f'Random OCD post: {df_ocd.sample(1).selftext.values[0]}')\nprint('='*100)\nprint(f'Random Autism post: {df_autism.sample(1).selftext.values[0]}')\ncancel_words = ['ocd','aut*','autism','obsess*','compuls*','disorder','executive dysfunction','adhd','diagnosis','ive been taking','spectrum','intrusive thoughts','germaphobes','depression']\n# apply the censor_words function to the selftext column of each dataframe",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd = df_ocd.rename(columns={'title_selftext': 'selftext'})\ndf_autism = df_autism.rename(columns={'title_selftext': 'selftext'})\n# randomly sample one post from each dataframe and print it\nprint(f'Random OCD post: {df_ocd.sample(1).selftext.values[0]}')\nprint('='*100)\nprint(f'Random Autism post: {df_autism.sample(1).selftext.values[0]}')\ncancel_words = ['ocd','aut*','autism','obsess*','compuls*','disorder','executive dysfunction','adhd','diagnosis','ive been taking','spectrum','intrusive thoughts','germaphobes','depression']\n# apply the censor_words function to the selftext column of each dataframe\ndf_ocd['selftext'] = df_ocd['selftext'].apply(censor_words)\n# remove punctuation",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism = df_autism.rename(columns={'title_selftext': 'selftext'})\n# randomly sample one post from each dataframe and print it\nprint(f'Random OCD post: {df_ocd.sample(1).selftext.values[0]}')\nprint('='*100)\nprint(f'Random Autism post: {df_autism.sample(1).selftext.values[0]}')\ncancel_words = ['ocd','aut*','autism','obsess*','compuls*','disorder','executive dysfunction','adhd','diagnosis','ive been taking','spectrum','intrusive thoughts','germaphobes','depression']\n# apply the censor_words function to the selftext column of each dataframe\ndf_ocd['selftext'] = df_ocd['selftext'].apply(censor_words)\n# remove punctuation\ndf_ocd['selftext'] = df_ocd['selftext'].str.replace('[^\\w\\s]','')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "cancel_words",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "cancel_words = ['ocd','aut*','autism','obsess*','compuls*','disorder','executive dysfunction','adhd','diagnosis','ive been taking','spectrum','intrusive thoughts','germaphobes','depression']\n# apply the censor_words function to the selftext column of each dataframe\ndf_ocd['selftext'] = df_ocd['selftext'].apply(censor_words)\n# remove punctuation\ndf_ocd['selftext'] = df_ocd['selftext'].str.replace('[^\\w\\s]','')\n# remove numbers\ndf_ocd['selftext'] = df_ocd['selftext'].str.replace('\\d+', '')\n# remove whitespace\ndf_ocd['selftext'] = df_ocd['selftext'].str.replace('\\s+', ' ')\n# do the same for the autism dataframe",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd['selftext'] = df_ocd['selftext'].apply(censor_words)\n# remove punctuation\ndf_ocd['selftext'] = df_ocd['selftext'].str.replace('[^\\w\\s]','')\n# remove numbers\ndf_ocd['selftext'] = df_ocd['selftext'].str.replace('\\d+', '')\n# remove whitespace\ndf_ocd['selftext'] = df_ocd['selftext'].str.replace('\\s+', ' ')\n# do the same for the autism dataframe\ndf_autism['selftext'] = df_autism['selftext'].apply(censor_words)\n# remove punctuation",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd['selftext'] = df_ocd['selftext'].str.replace('[^\\w\\s]','')\n# remove numbers\ndf_ocd['selftext'] = df_ocd['selftext'].str.replace('\\d+', '')\n# remove whitespace\ndf_ocd['selftext'] = df_ocd['selftext'].str.replace('\\s+', ' ')\n# do the same for the autism dataframe\ndf_autism['selftext'] = df_autism['selftext'].apply(censor_words)\n# remove punctuation\ndf_autism['selftext'] = df_autism['selftext'].str.replace('[^\\w\\s]','')\n# remove numbers",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd['selftext'] = df_ocd['selftext'].str.replace('\\d+', '')\n# remove whitespace\ndf_ocd['selftext'] = df_ocd['selftext'].str.replace('\\s+', ' ')\n# do the same for the autism dataframe\ndf_autism['selftext'] = df_autism['selftext'].apply(censor_words)\n# remove punctuation\ndf_autism['selftext'] = df_autism['selftext'].str.replace('[^\\w\\s]','')\n# remove numbers\ndf_autism['selftext'] = df_autism['selftext'].str.replace('\\d+', '')\n# remove whitespace",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd['selftext'] = df_ocd['selftext'].str.replace('\\s+', ' ')\n# do the same for the autism dataframe\ndf_autism['selftext'] = df_autism['selftext'].apply(censor_words)\n# remove punctuation\ndf_autism['selftext'] = df_autism['selftext'].str.replace('[^\\w\\s]','')\n# remove numbers\ndf_autism['selftext'] = df_autism['selftext'].str.replace('\\d+', '')\n# remove whitespace\ndf_autism['selftext'] = df_autism['selftext'].str.replace('\\s+', ' ')\n# remove words from posts that are in the cancel_words list. There are regex patterns in the cancel_words list so we need to use the `regex=True` parameter",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism['selftext'] = df_autism['selftext'].apply(censor_words)\n# remove punctuation\ndf_autism['selftext'] = df_autism['selftext'].str.replace('[^\\w\\s]','')\n# remove numbers\ndf_autism['selftext'] = df_autism['selftext'].str.replace('\\d+', '')\n# remove whitespace\ndf_autism['selftext'] = df_autism['selftext'].str.replace('\\s+', ' ')\n# remove words from posts that are in the cancel_words list. There are regex patterns in the cancel_words list so we need to use the `regex=True` parameter\n# then remove double spaces\ndf_ocd['selftext'] = df_ocd['selftext'].str.replace('  ', ' ')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism['selftext'] = df_autism['selftext'].str.replace('[^\\w\\s]','')\n# remove numbers\ndf_autism['selftext'] = df_autism['selftext'].str.replace('\\d+', '')\n# remove whitespace\ndf_autism['selftext'] = df_autism['selftext'].str.replace('\\s+', ' ')\n# remove words from posts that are in the cancel_words list. There are regex patterns in the cancel_words list so we need to use the `regex=True` parameter\n# then remove double spaces\ndf_ocd['selftext'] = df_ocd['selftext'].str.replace('  ', ' ')\ndf_autism['selftext'] = df_autism['selftext'].str.replace('  ', ' ')\n# make a new dataframe called df_reddit that combines the two dataframes",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism['selftext'] = df_autism['selftext'].str.replace('\\d+', '')\n# remove whitespace\ndf_autism['selftext'] = df_autism['selftext'].str.replace('\\s+', ' ')\n# remove words from posts that are in the cancel_words list. There are regex patterns in the cancel_words list so we need to use the `regex=True` parameter\n# then remove double spaces\ndf_ocd['selftext'] = df_ocd['selftext'].str.replace('  ', ' ')\ndf_autism['selftext'] = df_autism['selftext'].str.replace('  ', ' ')\n# make a new dataframe called df_reddit that combines the two dataframes\n# %% [markdown]\n# ## Equilibrium",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism['selftext'] = df_autism['selftext'].str.replace('\\s+', ' ')\n# remove words from posts that are in the cancel_words list. There are regex patterns in the cancel_words list so we need to use the `regex=True` parameter\n# then remove double spaces\ndf_ocd['selftext'] = df_ocd['selftext'].str.replace('  ', ' ')\ndf_autism['selftext'] = df_autism['selftext'].str.replace('  ', ' ')\n# make a new dataframe called df_reddit that combines the two dataframes\n# %% [markdown]\n# ## Equilibrium\n# %%\ndf_reddit = pd.DataFrame(columns=df_ocd.columns)",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_ocd['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_ocd['selftext'] = df_ocd['selftext'].str.replace('  ', ' ')\ndf_autism['selftext'] = df_autism['selftext'].str.replace('  ', ' ')\n# make a new dataframe called df_reddit that combines the two dataframes\n# %% [markdown]\n# ## Equilibrium\n# %%\ndf_reddit = pd.DataFrame(columns=df_ocd.columns)\n# what is the length of the shorter dataframe?\nif len(df_ocd) < len(df_autism): # if the OCD dataframe is shorter\n    shorter_df = df_ocd # set the shorter dataframe to the OCD dataframe",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_autism['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_autism['selftext'] = df_autism['selftext'].str.replace('  ', ' ')\n# make a new dataframe called df_reddit that combines the two dataframes\n# %% [markdown]\n# ## Equilibrium\n# %%\ndf_reddit = pd.DataFrame(columns=df_ocd.columns)\n# what is the length of the shorter dataframe?\nif len(df_ocd) < len(df_autism): # if the OCD dataframe is shorter\n    shorter_df = df_ocd # set the shorter dataframe to the OCD dataframe\n    longer_df = df_autism # set the longer dataframe to the Autism dataframe",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit = pd.DataFrame(columns=df_ocd.columns)\n# what is the length of the shorter dataframe?\nif len(df_ocd) < len(df_autism): # if the OCD dataframe is shorter\n    shorter_df = df_ocd # set the shorter dataframe to the OCD dataframe\n    longer_df = df_autism # set the longer dataframe to the Autism dataframe\nelse: # if the Autism dataframe is shorter\n    shorter_df = df_autism\n    longer_df = df_ocd\n# add the shorter dataframe to the new dataframe using concat\ndf_reddit = pd.concat([df_reddit, shorter_df], axis=0)",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit = pd.concat([df_reddit, shorter_df], axis=0)\n# shorten the longer dataframe to the length of the shorter dataframe\nlonger_df = longer_df.head(len(shorter_df))\n# add the shortened longer dataframe to the new dataframe using concat\ndf_reddit = pd.concat([df_reddit, longer_df], axis=0)\n# reset the index\ndf_reddit = df_reddit.reset_index(drop=True)\n# shuffle the dataframe\ndf_reddit = df_reddit.sample(frac=1).reset_index(drop=True)\n# check the dimensions of the new dataframe",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "longer_df",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "longer_df = longer_df.head(len(shorter_df))\n# add the shortened longer dataframe to the new dataframe using concat\ndf_reddit = pd.concat([df_reddit, longer_df], axis=0)\n# reset the index\ndf_reddit = df_reddit.reset_index(drop=True)\n# shuffle the dataframe\ndf_reddit = df_reddit.sample(frac=1).reset_index(drop=True)\n# check the dimensions of the new dataframe\nprint(f'Dimensions of the new dataframe: {df_reddit.shape}')\ndf_reddit.head(5)",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit = pd.concat([df_reddit, longer_df], axis=0)\n# reset the index\ndf_reddit = df_reddit.reset_index(drop=True)\n# shuffle the dataframe\ndf_reddit = df_reddit.sample(frac=1).reset_index(drop=True)\n# check the dimensions of the new dataframe\nprint(f'Dimensions of the new dataframe: {df_reddit.shape}')\ndf_reddit.head(5)\n# %%\n# double check that the number of posts for each subreddit is the same",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit = df_reddit.reset_index(drop=True)\n# shuffle the dataframe\ndf_reddit = df_reddit.sample(frac=1).reset_index(drop=True)\n# check the dimensions of the new dataframe\nprint(f'Dimensions of the new dataframe: {df_reddit.shape}')\ndf_reddit.head(5)\n# %%\n# double check that the number of posts for each subreddit is the same\nprint(f'Number of posts for OCD: {len(df_reddit[df_reddit.target == 1])}')\nprint(f'Number of posts for Autism: {len(df_reddit[df_reddit.target == 0])}')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit = df_reddit.sample(frac=1).reset_index(drop=True)\n# check the dimensions of the new dataframe\nprint(f'Dimensions of the new dataframe: {df_reddit.shape}')\ndf_reddit.head(5)\n# %%\n# double check that the number of posts for each subreddit is the same\nprint(f'Number of posts for OCD: {len(df_reddit[df_reddit.target == 1])}')\nprint(f'Number of posts for Autism: {len(df_reddit[df_reddit.target == 0])}')\n# %%\ndf_ocd.head(2)",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "drug_info",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "drug_info = pd.read_csv('../data/drug_info.csv')\ndrug_info['Medication Name'] = drug_info['Medication Name'].str.lower()\n# create a list of the medications\nmedications = drug_info['Medication Name'].tolist()\nprint(f'Number of medications: {len(medications)}')\n# how many posts contain a medication?\nprint(f'Number of posts that contain a medication: {len(df_reddit[df_reddit.selftext.str.contains(\"|\".join(medications), regex=True)])}')\n# %%\nmedications[0]\n# %%",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "medications",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "medications = drug_info['Medication Name'].tolist()\nprint(f'Number of medications: {len(medications)}')\n# how many posts contain a medication?\nprint(f'Number of posts that contain a medication: {len(df_reddit[df_reddit.selftext.str.contains(\"|\".join(medications), regex=True)])}')\n# %%\nmedications[0]\n# %%\nmedications = [med for med in medications if len(med) > 5]\n# create a list of rows and the medications mentioned in each row\nimport os",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "medications",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "medications = [med for med in medications if len(med) > 5]\n# create a list of rows and the medications mentioned in each row\nimport os\nmedications_mentioned = []\nif os.path.exists('../data/cleaned_reddit.csv'):\n    pass\nelse:\n    # with alive_bar (len(df_reddit)) as bar:\n    for index, row in df_reddit.iterrows(): # iterate through each row in the dataframe\n        # use regex to find all of the medications in the selftext column",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "medications_mentioned",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "medications_mentioned = []\nif os.path.exists('../data/cleaned_reddit.csv'):\n    pass\nelse:\n    # with alive_bar (len(df_reddit)) as bar:\n    for index, row in df_reddit.iterrows(): # iterate through each row in the dataframe\n        # use regex to find all of the medications in the selftext column\n        meds = re.findall(r'\\b(?:{})\\b'.format('|'.join(medications)), row['selftext'])\n        if len(meds) > 0: # if there are medications mentioned in the post\n            # replace the medications with ' ' (empty string)",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit['selftext'] = df_reddit['selftext'].str.replace('[^\\w\\s]','')\n# remove numbers\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace('\\d+', '')\n# remove double spaces\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace('  ', ' ')\n# remove single characters\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n# remove newlines\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\n', ' ')\n# remove urls",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit['selftext'] = df_reddit['selftext'].str.replace('\\d+', '')\n# remove double spaces\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace('  ', ' ')\n# remove single characters\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n# remove newlines\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\n', ' ')\n# remove urls\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'http\\S+', '')\n# remove html tags",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit['selftext'] = df_reddit['selftext'].str.replace('  ', ' ')\n# remove single characters\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n# remove newlines\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\n', ' ')\n# remove urls\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'http\\S+', '')\n# remove html tags\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'<.*?>', '')\n# remove extra spaces",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n# remove newlines\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\n', ' ')\n# remove urls\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'http\\S+', '')\n# remove html tags\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'<.*?>', '')\n# remove extra spaces\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+', ' ')\n# remove extra spaces at the beginning of the string",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\n', ' ')\n# remove urls\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'http\\S+', '')\n# remove html tags\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'<.*?>', '')\n# remove extra spaces\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+', ' ')\n# remove extra spaces at the beginning of the string\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'^\\s+', '')\n# remove extra spaces at the end of the string",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'http\\S+', '')\n# remove html tags\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'<.*?>', '')\n# remove extra spaces\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+', ' ')\n# remove extra spaces at the beginning of the string\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'^\\s+', '')\n# remove extra spaces at the end of the string\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+$', '')\n# save progress to a csv file",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'<.*?>', '')\n# remove extra spaces\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+', ' ')\n# remove extra spaces at the beginning of the string\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'^\\s+', '')\n# remove extra spaces at the end of the string\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+$', '')\n# save progress to a csv file\ndf_reddit.to_csv('../data/cleaned_reddit.csv', index=False)\n# read the file into a dataframe",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+', ' ')\n# remove extra spaces at the beginning of the string\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'^\\s+', '')\n# remove extra spaces at the end of the string\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+$', '')\n# save progress to a csv file\ndf_reddit.to_csv('../data/cleaned_reddit.csv', index=False)\n# read the file into a dataframe\ndf_reddit = pd.read_csv('../data/cleaned_reddit.csv')\n# remove any rows that have a null value in the selftext column",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'^\\s+', '')\n# remove extra spaces at the end of the string\ndf_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+$', '')\n# save progress to a csv file\ndf_reddit.to_csv('../data/cleaned_reddit.csv', index=False)\n# read the file into a dataframe\ndf_reddit = pd.read_csv('../data/cleaned_reddit.csv')\n# remove any rows that have a null value in the selftext column\ndf_reddit = df_reddit.dropna(subset=['selftext'])\n# reset the index",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit['selftext']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+$', '')\n# save progress to a csv file\ndf_reddit.to_csv('../data/cleaned_reddit.csv', index=False)\n# read the file into a dataframe\ndf_reddit = pd.read_csv('../data/cleaned_reddit.csv')\n# remove any rows that have a null value in the selftext column\ndf_reddit = df_reddit.dropna(subset=['selftext'])\n# reset the index\ndf_reddit = df_reddit.reset_index(drop=True)\n# check the dimensions of the dataframe",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit = pd.read_csv('../data/cleaned_reddit.csv')\n# remove any rows that have a null value in the selftext column\ndf_reddit = df_reddit.dropna(subset=['selftext'])\n# reset the index\ndf_reddit = df_reddit.reset_index(drop=True)\n# check the dimensions of the dataframe\nprint(f'Dimensions of the dataframe: {df_reddit.shape}')\ndef num_distinct_words(text,df):\n    # for this text, find the words that do not appear in any other text in the dataframe column 'selftext'\n    # split the text into a list of words",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit = df_reddit.dropna(subset=['selftext'])\n# reset the index\ndf_reddit = df_reddit.reset_index(drop=True)\n# check the dimensions of the dataframe\nprint(f'Dimensions of the dataframe: {df_reddit.shape}')\ndef num_distinct_words(text,df):\n    # for this text, find the words that do not appear in any other text in the dataframe column 'selftext'\n    # split the text into a list of words\n    if type(text) == str:\n        text = text.split(' ')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df_reddit",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df_reddit = df_reddit.reset_index(drop=True)\n# check the dimensions of the dataframe\nprint(f'Dimensions of the dataframe: {df_reddit.shape}')\ndef num_distinct_words(text,df):\n    # for this text, find the words that do not appear in any other text in the dataframe column 'selftext'\n    # split the text into a list of words\n    if type(text) == str:\n        text = text.split(' ')\n        # find the number of words that are not in any other text in the dataframe\n    else:",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df = df_reddit.copy() # make a copy of the dataframe\n#* flag\nblock_active = False # do not run this section of code unless the flag is set to True.\n# if ../data/distinct_words_and_extras.csv does not exist, then create it with the following code\nif block_active:\n    if os.path.exists('../data/distinct_words_and_extras.csv'):\n        # load the file as df\n        df = pd.read_csv('../data/distinct_words_and_extras.csv')\n        print(f'Loaded the file ../data/distinct_words_and_extras.csv')\n        print(f'   the included features are {df.columns}')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "block_active",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "block_active = False # do not run this section of code unless the flag is set to True.\n# if ../data/distinct_words_and_extras.csv does not exist, then create it with the following code\nif block_active:\n    if os.path.exists('../data/distinct_words_and_extras.csv'):\n        # load the file as df\n        df = pd.read_csv('../data/distinct_words_and_extras.csv')\n        print(f'Loaded the file ../data/distinct_words_and_extras.csv')\n        print(f'   the included features are {df.columns}')\n    else:\n        # add selftext_length as a column",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df = pd.read_csv('../data/cleaned_reddit.csv')\ntry:\n    #~ Dropping Constant Value columns from the data ~#\n    df.drop(columns=['is_original_content'], inplace=True)\n    #~ Dropping duplicated selftext rows from the data ~#\n    print(f'Before dropping duplicates, the shape of the data is: {df.shape}')\n    preshape = df.shape[0]\n    df.drop_duplicates(subset=['selftext'], inplace=True)\n    print(f'After dropping duplicates, the shape of the data is: {df.shape}')\n    print(f'The number of rows dropped is: {preshape - df.shape[0]}')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df['selftext_length']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df['selftext_length'] = df['selftext'].str.len()\n# Create a new column that is the number of words in the selftext\ndf['selftext_word_count'] = df['selftext'].str.split().str.len()\n# A column for each letter of the alphabet that is the number of times that letter appears in the selftext\nfor letter in 'abcdefghijklmnopqrstuvwxyz':\n    df[f'{letter}'] = df['selftext'].str.count(letter)\n# save the data to a csv\ndf.to_csv('../data/cleaned_reddit.csv', index=False)\ndf.head()\n# %%",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df['selftext_word_count']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df['selftext_word_count'] = df['selftext'].str.split().str.len()\n# A column for each letter of the alphabet that is the number of times that letter appears in the selftext\nfor letter in 'abcdefghijklmnopqrstuvwxyz':\n    df[f'{letter}'] = df['selftext'].str.count(letter)\n# save the data to a csv\ndf.to_csv('../data/cleaned_reddit.csv', index=False)\ndf.head()\n# %%\n# make a figure plotting letters against number of occurances in selftext for each selftext length bin. To avoid the ValueError \"ValueError: num must be 1 <= num <= 16, not 17\" the number of bins is set to 25 instead of 26 (the number of letters in the alphabet).\n# add a space between the plots to make them easier to read and to make the plot more aesthetically pleasing",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df = pd.read_csv('../data/cleaned_reddit_withsentiment.csv')\ndf = df[(df['created_utc'] >= 1547500000) & (df['created_utc'] <= 1598000000)] # filter the dataframe to only include posts that were posted within the UTC range listed above. (1.5475 to 1.598)\ndf.head()\n# %%\n# add 'ocd_selftext_length' and 'autism_selftext_length' columns to the df\ndf['selftext_length'] = df['selftext'].apply(lambda x: len(x))\ndf['ocd_selftext_length'] = df[df['target']==1]['selftext_length']\ndf['autism_selftext_length'] = df[df['target']==0]['selftext_length']\ndf['posts_by_author'] = df['author'].map(df['author'].value_counts()) # add a column to the df that shows the number of posts by each author\n# %%",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df = df[(df['created_utc'] >= 1547500000) & (df['created_utc'] <= 1598000000)] # filter the dataframe to only include posts that were posted within the UTC range listed above. (1.5475 to 1.598)\ndf.head()\n# %%\n# add 'ocd_selftext_length' and 'autism_selftext_length' columns to the df\ndf['selftext_length'] = df['selftext'].apply(lambda x: len(x))\ndf['ocd_selftext_length'] = df[df['target']==1]['selftext_length']\ndf['autism_selftext_length'] = df[df['target']==0]['selftext_length']\ndf['posts_by_author'] = df['author'].map(df['author'].value_counts()) # add a column to the df that shows the number of posts by each author\n# %%\n# plot the sentiment values for the top 100 authors (by number of posts) on their own subplots as scatterplots with the x-axis being the created_utc, y-axis being the selftext_length, and color representing the sentiment value column.",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df['selftext_length']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df['selftext_length'] = df['selftext'].apply(lambda x: len(x))\ndf['ocd_selftext_length'] = df[df['target']==1]['selftext_length']\ndf['autism_selftext_length'] = df[df['target']==0]['selftext_length']\ndf['posts_by_author'] = df['author'].map(df['author'].value_counts()) # add a column to the df that shows the number of posts by each author\n# %%\n# plot the sentiment values for the top 100 authors (by number of posts) on their own subplots as scatterplots with the x-axis being the created_utc, y-axis being the selftext_length, and color representing the sentiment value column.\n# add a space between the plots to make them easier to read and to make the plot more aesthetically pleasing\n# for this code block ignore the IndexError\n# use coolwarm for the color map to make the colors more distinct\n# add an annotation to each plot that shows the target value for that author (0 or 1) but for 1 show the text 'OCD' and for 0 show the text 'Autism' (this is the target column in the df) and make the text larger and bold.",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df['ocd_selftext_length']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df['ocd_selftext_length'] = df[df['target']==1]['selftext_length']\ndf['autism_selftext_length'] = df[df['target']==0]['selftext_length']\ndf['posts_by_author'] = df['author'].map(df['author'].value_counts()) # add a column to the df that shows the number of posts by each author\n# %%\n# plot the sentiment values for the top 100 authors (by number of posts) on their own subplots as scatterplots with the x-axis being the created_utc, y-axis being the selftext_length, and color representing the sentiment value column.\n# add a space between the plots to make them easier to read and to make the plot more aesthetically pleasing\n# for this code block ignore the IndexError\n# use coolwarm for the color map to make the colors more distinct\n# add an annotation to each plot that shows the target value for that author (0 or 1) but for 1 show the text 'OCD' and for 0 show the text 'Autism' (this is the target column in the df) and make the text larger and bold.\n# make the background color of each plot correspond to the average sentiment value for that author in the `sentiment` column (this is the average sentiment value for all of the posts by that author) use the following color map: 'coolwarm' (this is the same color map used for the scatterplots)",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df['autism_selftext_length']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df['autism_selftext_length'] = df[df['target']==0]['selftext_length']\ndf['posts_by_author'] = df['author'].map(df['author'].value_counts()) # add a column to the df that shows the number of posts by each author\n# %%\n# plot the sentiment values for the top 100 authors (by number of posts) on their own subplots as scatterplots with the x-axis being the created_utc, y-axis being the selftext_length, and color representing the sentiment value column.\n# add a space between the plots to make them easier to read and to make the plot more aesthetically pleasing\n# for this code block ignore the IndexError\n# use coolwarm for the color map to make the colors more distinct\n# add an annotation to each plot that shows the target value for that author (0 or 1) but for 1 show the text 'OCD' and for 0 show the text 'Autism' (this is the target column in the df) and make the text larger and bold.\n# make the background color of each plot correspond to the average sentiment value for that author in the `sentiment` column (this is the average sentiment value for all of the posts by that author) use the following color map: 'coolwarm' (this is the same color map used for the scatterplots)\n# make the background color of the plot lighter if the average sentiment value is closer to 0 and darker if the average sentiment value is closer to 1",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df['posts_by_author']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df['posts_by_author'] = df['author'].map(df['author'].value_counts()) # add a column to the df that shows the number of posts by each author\n# %%\n# plot the sentiment values for the top 100 authors (by number of posts) on their own subplots as scatterplots with the x-axis being the created_utc, y-axis being the selftext_length, and color representing the sentiment value column.\n# add a space between the plots to make them easier to read and to make the plot more aesthetically pleasing\n# for this code block ignore the IndexError\n# use coolwarm for the color map to make the colors more distinct\n# add an annotation to each plot that shows the target value for that author (0 or 1) but for 1 show the text 'OCD' and for 0 show the text 'Autism' (this is the target column in the df) and make the text larger and bold.\n# make the background color of each plot correspond to the average sentiment value for that author in the `sentiment` column (this is the average sentiment value for all of the posts by that author) use the following color map: 'coolwarm' (this is the same color map used for the scatterplots)\n# make the background color of the plot lighter if the average sentiment value is closer to 0 and darker if the average sentiment value is closer to 1\nfig, axes = plt.subplots(10, 10, figsize=(20,20), sharey=True, sharex=True)",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df['posts_by_author']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df['posts_by_author'] = df['author'].map(df['author'].value_counts()) # add a column to the df that shows the number of posts by each author\n# give each author a random unique hex color code i.e. '#aabbcc' for the first author, '#ffe111' for the second author, etc. (this will be used for the color of the points on the scatterplot)\n# use the following color map: 'coolwarm'\nimport random # this is used to generate random hex color codes\ndef random_color():\n    return '#%06x' % random.randint(0, 0xFFFFFF) # this will generate a random hex color code\ndf['author_color'] = df['author'].map({author: random_color() for author in df['author'].unique()}) # add a column to the df that shows the color for each author\n# sample\ndf.head()\n# %%",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df['author_color']",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df['author_color'] = df['author'].map({author: random_color() for author in df['author'].unique()}) # add a column to the df that shows the color for each author\n# sample\ndf.head()\n# %%\n# plot the sentiment values for the top 100 authors (by number of posts) on their own subplots as scatterplots with the x-axis being the created_utc, y-axis being the selftext_length, and color representing the sentiment value column.\n# add a space between the plots to make them easier to read and to make the plot more aesthetically pleasing\n# for this code block ignore the IndexError\n# use coolwarm for the color map to make the colors more distinct\n# add an annotation to each plot that shows the target value for that author (0 or 1) but for 1 show the text 'OCD' and for 0 show the text 'Autism' (this is the target column in the df) and make the text larger and bold.\nfig, axes = plt.subplots(10, 10, figsize=(20,20), sharey=True, sharex=True)",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "IQR",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "IQR = df['selftext_length'].quantile(0.75) - df['selftext_length'].quantile(0.25) # calculate the interquartile range\nlower_bound = df['selftext_length'].quantile(0.25) - (1.5 * IQR) # calculate the lower bound\nupper_bound = df['selftext_length'].quantile(0.75) + (1.5 * IQR) # calculate the upper bound\nprint(f'lower bound: {lower_bound} characters in selftext')\nprint(f'upper bound: {upper_bound} characters in selftext')\n# remove outliers from the df\nprint(f'number of rows in df before removing outliers: {df.shape[0]}')\npreshape = df.shape[0]\ndf = df[(df['selftext_length'] > lower_bound) & (df['selftext_length'] < upper_bound)]\nprint(f'number of rows in df after removing outliers: {df.shape[0]}')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "lower_bound",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "lower_bound = df['selftext_length'].quantile(0.25) - (1.5 * IQR) # calculate the lower bound\nupper_bound = df['selftext_length'].quantile(0.75) + (1.5 * IQR) # calculate the upper bound\nprint(f'lower bound: {lower_bound} characters in selftext')\nprint(f'upper bound: {upper_bound} characters in selftext')\n# remove outliers from the df\nprint(f'number of rows in df before removing outliers: {df.shape[0]}')\npreshape = df.shape[0]\ndf = df[(df['selftext_length'] > lower_bound) & (df['selftext_length'] < upper_bound)]\nprint(f'number of rows in df after removing outliers: {df.shape[0]}')\nprint(f'number of rows removed: {preshape - df.shape[0]}')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "upper_bound",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "upper_bound = df['selftext_length'].quantile(0.75) + (1.5 * IQR) # calculate the upper bound\nprint(f'lower bound: {lower_bound} characters in selftext')\nprint(f'upper bound: {upper_bound} characters in selftext')\n# remove outliers from the df\nprint(f'number of rows in df before removing outliers: {df.shape[0]}')\npreshape = df.shape[0]\ndf = df[(df['selftext_length'] > lower_bound) & (df['selftext_length'] < upper_bound)]\nprint(f'number of rows in df after removing outliers: {df.shape[0]}')\nprint(f'number of rows removed: {preshape - df.shape[0]}')\noutliers_removed = preshape - df.shape[0]",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "preshape",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "preshape = df.shape[0]\ndf = df[(df['selftext_length'] > lower_bound) & (df['selftext_length'] < upper_bound)]\nprint(f'number of rows in df after removing outliers: {df.shape[0]}')\nprint(f'number of rows removed: {preshape - df.shape[0]}')\noutliers_removed = preshape - df.shape[0]\n# %%\n# repeat the plot above (using seaborn) but add a density plot to the plot that shows the distribution of selftext length for the OCD and Autism subreddits (together) - orange filled area (use matplotlib) - this will be the background of the plot (i.e. the density plot will be the background of the plot) - make the density plot transparent by 0.5 (i.e. make it 50% transparent) so that the points are more visible - make the density plot have a black outline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n# set the area of the plot to focus on the data (i.e. remove the whitespace around the data)",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df = df[(df['selftext_length'] > lower_bound) & (df['selftext_length'] < upper_bound)]\nprint(f'number of rows in df after removing outliers: {df.shape[0]}')\nprint(f'number of rows removed: {preshape - df.shape[0]}')\noutliers_removed = preshape - df.shape[0]\n# %%\n# repeat the plot above (using seaborn) but add a density plot to the plot that shows the distribution of selftext length for the OCD and Autism subreddits (together) - orange filled area (use matplotlib) - this will be the background of the plot (i.e. the density plot will be the background of the plot) - make the density plot transparent by 0.5 (i.e. make it 50% transparent) so that the points are more visible - make the density plot have a black outline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n# set the area of the plot to focus on the data (i.e. remove the whitespace around the data)\n# set the background color of the plot to white",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "outliers_removed",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "outliers_removed = preshape - df.shape[0]\n# %%\n# repeat the plot above (using seaborn) but add a density plot to the plot that shows the distribution of selftext length for the OCD and Autism subreddits (together) - orange filled area (use matplotlib) - this will be the background of the plot (i.e. the density plot will be the background of the plot) - make the density plot transparent by 0.5 (i.e. make it 50% transparent) so that the points are more visible - make the density plot have a black outline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n# set the area of the plot to focus on the data (i.e. remove the whitespace around the data)\n# set the background color of the plot to white\n# set the style of the plot to whitegrid\n# set the size of the plot to 20 inches by 10 inches\nfig = sns.kdeplot(df['ocd_selftext_length'], shade=True, color='red', alpha=0.5)",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "fig",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "fig = sns.kdeplot(df['ocd_selftext_length'], shade=True, color='red', alpha=0.5)\nfig = sns.kdeplot(df['autism_selftext_length'], shade=True, color='blue', alpha=0.5)\nplt.title('Figure 12. Density Distribution of the Subreddits\\n Outliers Removed', fontsize=15)\n# annotate with outlier removal information in top right corner\nplt.annotate(f'{outliers_removed} outliers removed', xy=(0.95, 0.95), xycoords='axes fraction', horizontalalignment='right', verticalalignment='top', fontsize=10)\nplt.xlabel('Selftext Length', fontsize=15)\nplt.ylabel('Density', fontsize=15)\nplt.grid(False) # or use plt.grid(b=None) to remove the grid from all subplots\n# save the figure\nplt.savefig('../images/figure_12.png')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "fig",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "fig = sns.kdeplot(df['autism_selftext_length'], shade=True, color='blue', alpha=0.5)\nplt.title('Figure 12. Density Distribution of the Subreddits\\n Outliers Removed', fontsize=15)\n# annotate with outlier removal information in top right corner\nplt.annotate(f'{outliers_removed} outliers removed', xy=(0.95, 0.95), xycoords='axes fraction', horizontalalignment='right', verticalalignment='top', fontsize=10)\nplt.xlabel('Selftext Length', fontsize=15)\nplt.ylabel('Density', fontsize=15)\nplt.grid(False) # or use plt.grid(b=None) to remove the grid from all subplots\n# save the figure\nplt.savefig('../images/figure_12.png')\nplt.show();",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "median_val_preoutlier_removal",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "median_val_preoutlier_removal = df['selftext_length'].median()\nmean_val_preoutlier_removal = df['selftext_length'].mean()\n# How many posts have selftext length greater than the median selftext length for the OCD and Autism subreddits (together)?\nlong_posts_count = df[df['selftext_length'] > median_val_preoutlier_removal].shape[0]\nprint(f'number of posts with selftext length greater than the median selftext length for the OCD and Autism subreddits (together): {long_posts_count}')\n# what is the length of the shortest post created before 1540000000 UTC ?\nshortest_post_length = df[df['created_utc'] < 1540000000]['selftext_length'].min()\nprint(f'length of the shortest post created before 1540000000 UTC: {shortest_post_length} characters')\n# let's removed all posts with length less than 570 characters (i.e. the length of the shortest post created before 1540000000 UTC) and then plot the plot again.\n# remove all posts with length less than 570 characters",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "mean_val_preoutlier_removal",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "mean_val_preoutlier_removal = df['selftext_length'].mean()\n# How many posts have selftext length greater than the median selftext length for the OCD and Autism subreddits (together)?\nlong_posts_count = df[df['selftext_length'] > median_val_preoutlier_removal].shape[0]\nprint(f'number of posts with selftext length greater than the median selftext length for the OCD and Autism subreddits (together): {long_posts_count}')\n# what is the length of the shortest post created before 1540000000 UTC ?\nshortest_post_length = df[df['created_utc'] < 1540000000]['selftext_length'].min()\nprint(f'length of the shortest post created before 1540000000 UTC: {shortest_post_length} characters')\n# let's removed all posts with length less than 570 characters (i.e. the length of the shortest post created before 1540000000 UTC) and then plot the plot again.\n# remove all posts with length less than 570 characters\nprint(f'number of rows in df before removing posts with length less than 570 characters: {df.shape[0]}')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "long_posts_count",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "long_posts_count = df[df['selftext_length'] > median_val_preoutlier_removal].shape[0]\nprint(f'number of posts with selftext length greater than the median selftext length for the OCD and Autism subreddits (together): {long_posts_count}')\n# what is the length of the shortest post created before 1540000000 UTC ?\nshortest_post_length = df[df['created_utc'] < 1540000000]['selftext_length'].min()\nprint(f'length of the shortest post created before 1540000000 UTC: {shortest_post_length} characters')\n# let's removed all posts with length less than 570 characters (i.e. the length of the shortest post created before 1540000000 UTC) and then plot the plot again.\n# remove all posts with length less than 570 characters\nprint(f'number of rows in df before removing posts with length less than 570 characters: {df.shape[0]}')\npreshape = df.shape[0]\ndf = df[df['selftext_length'] > 570]",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "shortest_post_length",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "shortest_post_length = df[df['created_utc'] < 1540000000]['selftext_length'].min()\nprint(f'length of the shortest post created before 1540000000 UTC: {shortest_post_length} characters')\n# let's removed all posts with length less than 570 characters (i.e. the length of the shortest post created before 1540000000 UTC) and then plot the plot again.\n# remove all posts with length less than 570 characters\nprint(f'number of rows in df before removing posts with length less than 570 characters: {df.shape[0]}')\npreshape = df.shape[0]\ndf = df[df['selftext_length'] > 570]\nprint(f'number of rows in df after removing posts with length less than 570 characters: {df.shape[0]}')\nprint(f'number of rows removed: {preshape - df.shape[0]}')\nshort_posts_removed = preshape - df.shape[0]",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "preshape",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "preshape = df.shape[0]\ndf = df[df['selftext_length'] > 570]\nprint(f'number of rows in df after removing posts with length less than 570 characters: {df.shape[0]}')\nprint(f'number of rows removed: {preshape - df.shape[0]}')\nshort_posts_removed = preshape - df.shape[0]\n# when was the earliest OCD post created?\nearliest_ocd_post = df[df['target'] == 1]['created_utc'].min() # 1530000000\nprint(f'earliest OCD post created: {earliest_ocd_post}')\n# when was the earliest Autism post created?\nearliest_autism_post = df[df['target'] == 0]['created_utc'].min() # 1530000000",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "df = df[df['selftext_length'] > 570]\nprint(f'number of rows in df after removing posts with length less than 570 characters: {df.shape[0]}')\nprint(f'number of rows removed: {preshape - df.shape[0]}')\nshort_posts_removed = preshape - df.shape[0]\n# when was the earliest OCD post created?\nearliest_ocd_post = df[df['target'] == 1]['created_utc'].min() # 1530000000\nprint(f'earliest OCD post created: {earliest_ocd_post}')\n# when was the earliest Autism post created?\nearliest_autism_post = df[df['target'] == 0]['created_utc'].min() # 1530000000\nprint(f'earliest Autism post created: {earliest_autism_post}')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "short_posts_removed",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "short_posts_removed = preshape - df.shape[0]\n# when was the earliest OCD post created?\nearliest_ocd_post = df[df['target'] == 1]['created_utc'].min() # 1530000000\nprint(f'earliest OCD post created: {earliest_ocd_post}')\n# when was the earliest Autism post created?\nearliest_autism_post = df[df['target'] == 0]['created_utc'].min() # 1530000000\nprint(f'earliest Autism post created: {earliest_autism_post}')\n# when was the latest OCD post created?\nlatest_ocd_post = df[df['target'] == 1]['created_utc'].max() # 1540000000\nprint(f'latest OCD post created: {latest_ocd_post}')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "earliest_ocd_post",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "earliest_ocd_post = df[df['target'] == 1]['created_utc'].min() # 1530000000\nprint(f'earliest OCD post created: {earliest_ocd_post}')\n# when was the earliest Autism post created?\nearliest_autism_post = df[df['target'] == 0]['created_utc'].min() # 1530000000\nprint(f'earliest Autism post created: {earliest_autism_post}')\n# when was the latest OCD post created?\nlatest_ocd_post = df[df['target'] == 1]['created_utc'].max() # 1540000000\nprint(f'latest OCD post created: {latest_ocd_post}')\n# when was the latest Autism post created?\nlatest_autism_post = df[df['target'] == 0]['created_utc'].max() # 1540000000",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "earliest_autism_post",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "earliest_autism_post = df[df['target'] == 0]['created_utc'].min() # 1530000000\nprint(f'earliest Autism post created: {earliest_autism_post}')\n# when was the latest OCD post created?\nlatest_ocd_post = df[df['target'] == 1]['created_utc'].max() # 1540000000\nprint(f'latest OCD post created: {latest_ocd_post}')\n# when was the latest Autism post created?\nlatest_autism_post = df[df['target'] == 0]['created_utc'].max() # 1540000000\nprint(f'latest Autism post created: {latest_autism_post}')\n# # remove posts before 1546777579 and after 1596525852\n# print(f'number of rows in df before removing posts before 1546777579 and after 1596525852: {df.shape[0]}')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "latest_ocd_post",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "latest_ocd_post = df[df['target'] == 1]['created_utc'].max() # 1540000000\nprint(f'latest OCD post created: {latest_ocd_post}')\n# when was the latest Autism post created?\nlatest_autism_post = df[df['target'] == 0]['created_utc'].max() # 1540000000\nprint(f'latest Autism post created: {latest_autism_post}')\n# # remove posts before 1546777579 and after 1596525852\n# print(f'number of rows in df before removing posts before 1546777579 and after 1596525852: {df.shape[0]}')\n# preshape = df.shape[0]\n# df = df[(df['created_utc'] > 1546777579) & (df['created_utc'] < 1596525852)]\n# print(f'number of rows in df after removing posts before 1546777579 and after 1596525852: {df.shape[0]}')",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "latest_autism_post",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "latest_autism_post = df[df['target'] == 0]['created_utc'].max() # 1540000000\nprint(f'latest Autism post created: {latest_autism_post}')\n# # remove posts before 1546777579 and after 1596525852\n# print(f'number of rows in df before removing posts before 1546777579 and after 1596525852: {df.shape[0]}')\n# preshape = df.shape[0]\n# df = df[(df['created_utc'] > 1546777579) & (df['created_utc'] < 1596525852)]\n# print(f'number of rows in df after removing posts before 1546777579 and after 1596525852: {df.shape[0]}')\n# print(f'number of rows removed: {preshape - df.shape[0]}')\n# posts_removed = preshape - df.shape[0]\n# %%",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "figure",
        "kind": 5,
        "importPath": "full",
        "description": "full",
        "peekOfCode": "figure = plt.figure(figsize=(20,5))\nplt.scatter(df['created_utc'], df['selftext_length'], c=df['author_color'], s=df['posts_by_author'], alpha=0.5)\nplt.title('Figure 13. Distribution of post lengths in both subreddits\\n After removing outliers', fontsize=15)\nplt.xlabel('Created UTC', fontsize=15)\nplt.ylabel('Selftext Length', fontsize=15)\nplt.grid(False) # or use plt.grid(b=None) to remove the grid from all subplots\nplt.annotate('size denotes post volume by author', xy=(0.95, 0.01), xycoords='axes fraction', horizontalalignment='right', verticalalignment='bottom', fontsize=10)\nplt.axhline(mean_val_preoutlier_removal, color='black', label='Average post length') # using the previously calculated mean value of the selftext length\nplt.axhline(median_val_preoutlier_removal, color='red', label='Median post length') # using the previously calculated median value of the selftext length\nplt.legend()",
        "detail": "full",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "df = pd.read_csv(\"data/reddit_threads.csv\")  # The combined data\nprint(f\"Completed Step One. The shape of the data is {df.shape}\")\n# & Step Two. Preprocess the data\ndf_preprocessed = run_preprocess_data(df)\nprint(f\"Completed Step Two. The shape of the data is {df_preprocessed.shape}\")\n# & Step Three. Run the feature engineering functions on the data\n# Run the feature engineering script\n# selftext is converted to string now.\ndf = run_feature_engineering(df_preprocessed)\nprint(f\"Completed Step Three. The shape of the data is {df.shape}\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "df_preprocessed",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "df_preprocessed = run_preprocess_data(df)\nprint(f\"Completed Step Two. The shape of the data is {df_preprocessed.shape}\")\n# & Step Three. Run the feature engineering functions on the data\n# Run the feature engineering script\n# selftext is converted to string now.\ndf = run_feature_engineering(df_preprocessed)\nprint(f\"Completed Step Three. The shape of the data is {df.shape}\")\n# & Step Four. Run the modeling script to test the models\n# Run the modeling script\nrun_modeling(df)",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "df = run_feature_engineering(df_preprocessed)\nprint(f\"Completed Step Three. The shape of the data is {df.shape}\")\n# & Step Four. Run the modeling script to test the models\n# Run the modeling script\nrun_modeling(df)\nprint(\n    f\"Completed Step Four. The shape of the data is {df.shape}, and we generated our model results.\"\n)",
        "detail": "main",
        "documentation": {}
    }
]