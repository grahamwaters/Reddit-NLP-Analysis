{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "import re\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# adaboost imports\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "# import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Tree imports\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from alive_progress import alive_bar\n",
    "\n",
    "# import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancel_words = ['ocd','aut*','autism','obsess*','compuls*','disorder','diagnosis']\n",
    "autism_columns_to_keep = ['author', 'author_flair_richtext', 'author_flair_type','created_utc', 'id', 'is_video', 'selftext', 'title', 'is_original_content','media_only', 'author_fullname','target']\n",
    "ocd_columns_to_keep = ['author', 'author_flair_richtext', 'author_flair_type','created_utc', 'id', 'is_video', 'selftext', 'title', 'is_original_content','media_only', 'author_fullname','target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_keywords(post):\n",
    "    \"\"\"Get the keywords from a post\"\"\"\n",
    "    # Get the keywords from the post\n",
    "    keywords = set()\n",
    "    for word in re.split(\"\\W+\", post.text):\n",
    "        if word in keywords:\n",
    "            continue\n",
    "        else:\n",
    "            keywords.add(word)\n",
    "    return keywords\n",
    "\n",
    "\n",
    "# define the stop words list\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Remove Punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from a string\"\"\"\n",
    "    return ''.join(ch for ch in text if ch not in stop_words)\n",
    "\n",
    "# Lower Case\n",
    "def lowercase(text):\n",
    "    \"\"\"Lower case a string\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def censor_words(text):\n",
    "    \"\"\"\n",
    "    censor_words takes in a string and replaces all words that are in the cancel_words list with ''\n",
    "\n",
    "    Parameters\n",
    "\n",
    "    :param text: string to be censored\n",
    "    :type text: str\n",
    "    :return: the censored string\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    text = str(text) # convert to string if not already\n",
    "\n",
    "    text = text.lower()\n",
    "    # Remove all words that begin with 'aut' from the sentence and return the result\n",
    "    # regex pattern\n",
    "    pattern = r'aut(.*?)[^a-zA-Z]' # aut followed by any number of characters then ending in any character that is not a letter\n",
    "    # replace those pattern matches with '' (nothing)\n",
    "    text =  re.sub(pattern, '', text) # replace the pattern matches with '' (nothing)\n",
    "    \n",
    "    # pattern 2 - remove all words that begin with 'ocd' from the sentence and return the result\n",
    "    pattern = r'ocd(.*?)[^a-zA-Z]' # ocd followed by any number of characters then ending in any character that is not a letter\n",
    "    # replace those pattern matches with '' (nothing)\n",
    "    text =  re.sub(pattern, '', text) # replace the pattern matches with '' (nothing)\n",
    "\n",
    "    # pattern 3 - remove all words that begin with 'obsess' from the sentence and return the result\n",
    "    pattern = r'obsess|compuls(.*?)[^a-zA-Z]' # obsess followed by any number of characters then ending in any character that is not a letter\n",
    "    # replace those pattern matches with '' (nothing)\n",
    "    text =  re.sub(pattern, '', text) # replace the pattern matches with '' (nothing)\n",
    "    return text # return the result\n",
    "\n",
    "\n",
    "#* Process the text with these functions\n",
    "\n",
    "def preprocess(df_ocd, df_autism, cancel_words, ocd_columns_to_keep, autism_columns_to_keep):\n",
    "    \"\"\"\n",
    "    preprocess the dataframes by removing the columns that are not needed, removing the rows that have null values, and removing the rows that contain the cancel words\n",
    "\n",
    "    _extended_summary_\n",
    "\n",
    "    :param df_ocd: _description_\n",
    "    :type df_ocd: _type_\n",
    "    :param df_autism: _description_\n",
    "    :type df_autism: _type_\n",
    "    :param cancel_words: _description_\n",
    "    :type cancel_words: _type_\n",
    "    :param ocd_columns_to_keep: _description_\n",
    "    :type ocd_columns_to_keep: _type_\n",
    "    :param autism_columns_to_keep: _description_\n",
    "    :type autism_columns_to_keep: _type_\n",
    "    \"\"\"\n",
    "    # drop columns with more than 50% missing values from the dataframes\n",
    "    print(f'Dimensions before dropping columns with more than 50% missing values: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "    df_ocd = df_ocd.dropna(thresh=0.5*len(df_ocd), axis=1)\n",
    "    df_autism = df_autism.dropna(thresh=0.5*len(df_autism), axis=1)\n",
    "    print(f'Dimensions after dropping columns with more than 50% missing values: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "    print(f'columns in df_ocd: {df_ocd.columns}')\n",
    "\n",
    "    #* Only keep the columns in these two dataframes that are in both dataframes and are in the lists below\n",
    "\n",
    "    # drop columns that are not in the lists above\n",
    "    print(f'Dimensions before dropping columns that are not in the lists above: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "    df_ocd = df_ocd[ocd_columns_to_keep] \n",
    "    df_autism = df_autism[autism_columns_to_keep]\n",
    "    print(f'Dimensions after dropping columns that are not in the lists above: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "\n",
    "    # Now remove any posts from these dataframes where the `is_video` or `media_only` columsn are True\n",
    "    print(f'Dimensions before removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "    df_ocd = df_ocd[(df_ocd['is_video'] == False) & (df_ocd['media_only'] == False)]\n",
    "    df_autism = df_autism[(df_autism['is_video'] == False) & (df_autism['media_only'] == False)]\n",
    "    print(f'Dimensions after removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "\n",
    "    # and now we can drop the `is_video` and `media_only` columns\n",
    "    df_ocd = df_ocd.drop(columns=['is_video', 'media_only'])\n",
    "    df_autism = df_autism.drop(columns=['is_video', 'media_only'])\n",
    "    print(f'Dropped the `is_video` and `media_only` columns')\n",
    "\n",
    "    # some posts are in the title column and some are in the selftext column so we need to combine these columns into one column if they are long enough.\n",
    "    # find the median length of the title and selftext columns combined for each dataframe\n",
    "    med_len_title_selftext_ocd = df_ocd.title.str.len().add(df_ocd.selftext.str.len()).median()\n",
    "    med_len_title_selftext_autism = df_autism.title.str.len().add(df_autism.selftext.str.len()).median()\n",
    "    print(f'Median length of title and selftext columns combined for OCD: {med_len_title_selftext_ocd}')\n",
    "    print(f'Median length of title and selftext columns combined for Autism: {med_len_title_selftext_autism}')\n",
    "\n",
    "    # how many posts have a title and selftext combined that are longer than the median length of the title and selftext columns combined for each dataframe?\n",
    "    print(f'Acceptable number of OCD posts: {len(df_ocd[df_ocd.title.str.len().add(df_ocd.selftext.str.len()) > med_len_title_selftext_ocd])}')\n",
    "    print(f'Acceptable number of Autism posts: {len(df_autism[df_autism.title.str.len().add(df_autism.selftext.str.len()) > med_len_title_selftext_autism])}')\n",
    "\n",
    "    # drop author_flair_richtext\n",
    "    df_ocd = df_ocd.drop(columns=['author_flair_richtext'])\n",
    "    df_autism = df_autism.drop(columns=['author_flair_richtext'])\n",
    "\n",
    "    # how many posts are there for the top 100 authors in each dataframe?\n",
    "    top_authors_ocd = df_ocd.author.value_counts().head(100)\n",
    "    top_authors_byfullname_ocd = df_ocd.author_fullname.value_counts().head(100)\n",
    "    top_authors_autism = df_autism.author.value_counts().head(100)\n",
    "    top_authors_byfullname_autism = df_autism.author_fullname.value_counts().head(100)\n",
    "\n",
    "    # are there any authors that are in both dataframes?\n",
    "    print(f'Number of authors that are in both dataframes: {len(set(top_authors_ocd.index).intersection(set(top_authors_autism.index)))}')\n",
    "    list_of_cross_posters = list(set(top_authors_ocd.index).intersection(set(top_authors_autism.index)))\n",
    "    print(f'List of authors that are in both dataframes: {list_of_cross_posters}')\n",
    "\n",
    "    # drop author_flair_type and author_fullname columns from both dataframes\n",
    "    df_ocd = df_ocd.drop(columns=['author_flair_type', 'author_fullname'])\n",
    "    df_autism = df_autism.drop(columns=['author_flair_type', 'author_fullname'])\n",
    "\n",
    "    # combine the title and self text columns into one column with the format `title - selftext`\n",
    "    df_ocd['title_selftext'] = df_ocd.title + ' - ' + df_ocd.selftext\n",
    "    df_autism['title_selftext'] = df_autism.title + ' - ' + df_autism.selftext\n",
    "\n",
    "    # drop the title and selftext columns\n",
    "    df_ocd = df_ocd.drop(columns=['title', 'selftext'])\n",
    "    df_autism = df_autism.drop(columns=['title', 'selftext'])\n",
    "\n",
    "    # rename the `title_selftext` column to `selftext`\n",
    "    df_ocd = df_ocd.rename(columns={'title_selftext': 'selftext'})\n",
    "    df_autism = df_autism.rename(columns={'title_selftext': 'selftext'})\n",
    "\n",
    "    # apply the censor_words function to the selftext column of each dataframe\n",
    "    df_ocd['selftext'] = df_ocd['selftext'].apply(censor_words)\n",
    "\n",
    "    # remove punctuation\n",
    "    df_ocd['selftext'] = df_ocd['selftext'].str.replace('[^\\w\\s]','')\n",
    "    # remove numbers\n",
    "    df_ocd['selftext'] = df_ocd['selftext'].str.replace('\\d+', '')\n",
    "    # remove whitespace\n",
    "    df_ocd['selftext'] = df_ocd['selftext'].str.replace('\\s+', ' ')\n",
    "\n",
    "    # do the same for the autism dataframe\n",
    "    df_autism['selftext'] = df_autism['selftext'].apply(censor_words)\n",
    "    # remove punctuation\n",
    "    df_autism['selftext'] = df_autism['selftext'].str.replace('[^\\w\\s]','')\n",
    "    # remove numbers\n",
    "    df_autism['selftext'] = df_autism['selftext'].str.replace('\\d+', '')\n",
    "    # remove whitespace\n",
    "    df_autism['selftext'] = df_autism['selftext'].str.replace('\\s+', ' ')\n",
    "\n",
    "    # remove words from posts that are in the cancel_words list. There are regex patterns in the cancel_words list so we need to use the `regex=True` parameter\n",
    "\n",
    "    # then remove double spaces\n",
    "    df_ocd['selftext'] = df_ocd['selftext'].str.replace('  ', ' ')\n",
    "    df_autism['selftext'] = df_autism['selftext'].str.replace('  ', ' ')\n",
    "\n",
    "    # make a new dataframe called df_reddit that combines the two dataframes\n",
    "\n",
    "    df_reddit = pd.DataFrame(columns=df_ocd.columns)\n",
    "    # what is the length of the shorter dataframe?\n",
    "    if len(df_ocd) < len(df_autism): # if the OCD dataframe is shorter\n",
    "        shorter_df = df_ocd # set the shorter dataframe to the OCD dataframe\n",
    "        longer_df = df_autism # set the longer dataframe to the Autism dataframe\n",
    "        df_reddit.append()\n",
    "    else: # if the Autism dataframe is shorter\n",
    "        shorter_df = df_autism\n",
    "        longer_df = df_ocd\n",
    "\n",
    "    # add the shorter dataframe to the new dataframe using concat\n",
    "    df_reddit = pd.concat([df_reddit, shorter_df], axis=0)\n",
    "    # shorten the longer dataframe to the length of the shorter dataframe\n",
    "    longer_df = longer_df.head(len(shorter_df))\n",
    "    # add the shortened longer dataframe to the new dataframe using concat\n",
    "    df_reddit = pd.concat([df_reddit, longer_df], axis=0)\n",
    "\n",
    "    # reset the index\n",
    "    df_reddit = df_reddit.reset_index(drop=True)\n",
    "\n",
    "    # shuffle the dataframe\n",
    "    df_reddit = df_reddit.sample(frac=1).reset_index(drop=True)\n",
    "    # check the dimensions of the new dataframe\n",
    "    print(f'Dimensions of the new dataframe: {df_reddit.shape}')\n",
    "\n",
    "    # double check that the number of posts for each subreddit is the same\n",
    "    print(f'Number of posts for OCD: {len(df_reddit[df_reddit.target == 1])}')\n",
    "    print(f'Number of posts for Autism: {len(df_reddit[df_reddit.target == 0])}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Importing and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_48094/2788604670.py:3: DtypeWarning: Columns (5,27,50,51,53,54,56,57,60,61,63,67,68,75,76,77,80,81,82,83,84,85,87,88,89,90,91) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_ocd = pd.read_csv('../data/ocd_thread.csv')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_48094/2788604670.py:4: DtypeWarning: Columns (70,71,74,75,76,77,78,79,80,82,83,84,85,86,87,88) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_autism = pd.read_csv('../data/autism_thread.csv')\n"
     ]
    }
   ],
   "source": [
    "# Section 1: Importing and Loading Data\n",
    "# opening the scraped data saved in csv files and creating a dataframe for each\n",
    "df_ocd = pd.read_csv('../data/ocd_thread.csv')\n",
    "df_autism = pd.read_csv('../data/autism_thread.csv')\n",
    "\n",
    "# creating a target column for each dataframe\n",
    "df_ocd['target'] = 1\n",
    "df_autism['target'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section One. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions before dropping columns with more than 50% missing values: (41449, 93) for OCD and (25750, 90) for Autism\n",
      "Dimensions after dropping columns with more than 50% missing values: (41449, 51) for OCD and (25750, 52) for Autism\n",
      "columns in df_ocd: Index(['author', 'author_flair_richtext', 'author_flair_type', 'can_mod_post',\n",
      "       'contest_mode', 'created_utc', 'domain', 'full_link', 'id',\n",
      "       'is_crosspostable', 'is_reddit_media_domain', 'is_self', 'is_video',\n",
      "       'link_flair_richtext', 'link_flair_text_color', 'link_flair_type',\n",
      "       'locked', 'num_comments', 'num_crossposts', 'over_18',\n",
      "       'parent_whitelist_status', 'permalink', 'pinned', 'retrieved_on',\n",
      "       'score', 'selftext', 'spoiler', 'stickied', 'subreddit', 'subreddit_id',\n",
      "       'subreddit_type', 'thumbnail', 'title', 'url', 'whitelist_status',\n",
      "       'send_replies', 'no_follow', 'subreddit_subscribers',\n",
      "       'is_original_content', 'pwls', 'wls', 'media_only', 'is_meta',\n",
      "       'author_fullname', 'gildings', 'is_robot_indexable',\n",
      "       'author_patreon_flair', 'all_awardings', 'total_awards_received',\n",
      "       'allow_live_comments', 'target'],\n",
      "      dtype='object')\n",
      "Dimensions before dropping columns that are not in the lists above: (41449, 51) for OCD and (25750, 52) for Autism\n",
      "Dimensions after dropping columns that are not in the lists above: (41449, 12) for OCD and (25750, 12) for Autism\n",
      "Dimensions before removing posts where `is_video` or `media_only` columns are True: (41449, 12) for OCD and (25750, 12) for Autism\n",
      "Dimensions after removing posts where `is_video` or `media_only` columns are True: (37323, 12) for OCD and (25540, 12) for Autism\n",
      "Dropped the `is_video` and `media_only` columns\n",
      "Median length of title and selftext columns combined for OCD: 652.0\n",
      "Median length of title and selftext columns combined for Autism: 470.0\n",
      "Acceptable number of OCD posts: 16343\n",
      "Acceptable number of Autism posts: 9021\n",
      "Number of authors that are in both dataframes: 1\n",
      "List of authors that are in both dataframes: ['[deleted]']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_48094/929567222.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Section 2: Data Cleaning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_ocd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_autism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancel_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mocd_columns_to_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautism_columns_to_keep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# calling the preprocess function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_48094/193422053.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(df_ocd, df_autism, cancel_words, ocd_columns_to_keep, autism_columns_to_keep)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# apply the censor_words function to the selftext column of each dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mdf_ocd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selftext'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_ocd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selftext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcensor_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;31m# remove punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4772\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4773\u001b[0m         \"\"\"\n\u001b[0;32m-> 4774\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4776\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1149\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1152\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_48094/193422053.py\u001b[0m in \u001b[0;36mcensor_words\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcensor_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Remove all words that begin with 'aut' from the sentence and return the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# regex pattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Section 2: Data Cleaning\n",
    "preprocess(df_ocd, df_autism, cancel_words, ocd_columns_to_keep, autism_columns_to_keep) # calling the preprocess function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A preview of each dataframe after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_autism.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ocd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many authors are in each dataframe?\n",
    "print(f'Number of authors in df_ocd: {len(df_ocd.author.unique())}')\n",
    "print(f'Number of authors in df_autism: {len(df_autism.author.unique())}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
