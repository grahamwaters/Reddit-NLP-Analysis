{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "import re\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas_profiling\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# adaboost imports\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "# import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from math import floor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Tree imports\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from alive_progress import alive_bar\n",
    "\n",
    "# import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def censor_words(text):\n",
    "    # Remove all words that begin with 'aut' from the sentence and return the result\n",
    "    # regex pattern\n",
    "    pattern = r'aut(.*?)[^a-zA-Z]' # aut followed by any number of characters then ending in any character that is not a letter\n",
    "    # replace those pattern matches with '' (nothing)\n",
    "    text =  re.sub(pattern, '', text) # replace the pattern matches with '' (nothing)\n",
    "    \n",
    "    # pattern 2 - remove all words that begin with 'ocd' from the sentence and return the result\n",
    "    pattern = r'ocd(.*?)[^a-zA-Z]' # ocd followed by any number of characters then ending in any character that is not a letter\n",
    "    # replace those pattern matches with '' (nothing)\n",
    "    text =  re.sub(pattern, '', text) # replace the pattern matches with '' (nothing)\n",
    "\n",
    "    # pattern 3 - remove all words that begin with 'obsess' from the sentence and return the result\n",
    "    pattern = r'obsess|compuls(.*?)[^a-zA-Z]' # obsess followed by any number of characters then ending in any character that is not a letter\n",
    "    # replace those pattern matches with '' (nothing)\n",
    "    text =  re.sub(pattern, '', text) # replace the pattern matches with '' (nothing)\n",
    "    #  remove nonalphanumeric characters\n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "    \n",
    "    return text # return the result\n",
    "\n",
    "def get_keywords(post):\n",
    "    \"\"\"Get the keywords from a post\"\"\"\n",
    "    # Get the keywords from the post\n",
    "    keywords = set()\n",
    "    for word in re.split(\"\\W+\", post.text):\n",
    "        if word in keywords:\n",
    "            continue\n",
    "        else:\n",
    "            keywords.add(word)\n",
    "    return keywords\n",
    "\n",
    "\n",
    "# define the stop words list\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Remove Punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from a string\"\"\"\n",
    "    return ''.join(ch for ch in text if ch not in stop_words)\n",
    "\n",
    "# Lower Case\n",
    "def lowercase(text):\n",
    "    \"\"\"Lower case a string\"\"\"\n",
    "    return text.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/1681350667.py:2: DtypeWarning: Columns (5,27,50,51,53,54,56,57,60,61,63,67,68,75,76,77,80,81,82,83,84,85,87,88,89,90,91) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_ocd = pd.read_csv('../data/ocd_thread.csv')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/1681350667.py:3: DtypeWarning: Columns (70,71,74,75,76,77,78,79,80,82,83,84,85,86,87,88) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_autism = pd.read_csv('../data/autism_thread.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions before dropping columns with more than 50% missing values: (41449, 93) for OCD and (25750, 90) for Autism\n",
      "Dimensions after dropping columns with more than 50% missing values: (41449, 51) for OCD and (25750, 52) for Autism\n",
      "columns in df_ocd: Index(['author', 'author_flair_richtext', 'author_flair_type', 'can_mod_post',\n",
      "       'contest_mode', 'created_utc', 'domain', 'full_link', 'id',\n",
      "       'is_crosspostable', 'is_reddit_media_domain', 'is_self', 'is_video',\n",
      "       'link_flair_richtext', 'link_flair_text_color', 'link_flair_type',\n",
      "       'locked', 'num_comments', 'num_crossposts', 'over_18',\n",
      "       'parent_whitelist_status', 'permalink', 'pinned', 'retrieved_on',\n",
      "       'score', 'selftext', 'spoiler', 'stickied', 'subreddit', 'subreddit_id',\n",
      "       'subreddit_type', 'thumbnail', 'title', 'url', 'whitelist_status',\n",
      "       'send_replies', 'no_follow', 'subreddit_subscribers',\n",
      "       'is_original_content', 'pwls', 'wls', 'media_only', 'is_meta',\n",
      "       'author_fullname', 'gildings', 'is_robot_indexable',\n",
      "       'author_patreon_flair', 'all_awardings', 'total_awards_received',\n",
      "       'allow_live_comments', 'target'],\n",
      "      dtype='object')\n",
      "Dimensions before dropping columns that are not in the lists above: (41449, 51) for OCD and (25750, 52) for Autism\n",
      "Dimensions after dropping columns that are not in the lists above: (41449, 12) for OCD and (25750, 12) for Autism\n",
      "Dimensions before removing posts where `is_video` or `media_only` columns are True: (41449, 12) for OCD and (25750, 12) for Autism\n",
      "Dimensions after removing posts where `is_video` or `media_only` columns are True: (37323, 12) for OCD and (25540, 12) for Autism\n",
      "Dropped the `is_video` and `media_only` columns\n",
      "Median length of title and selftext columns combined for OCD: 652.0\n",
      "Median length of title and selftext columns combined for Autism: 470.0\n",
      "Acceptable number of OCD posts: 16343\n",
      "Acceptable number of Autism posts: 9021\n",
      "Dimensions before: (37323, 10) for OCD and (25540, 10) for Autism\n",
      "Dimensions before: (16343, 10) for OCD and (9021, 10) for Autism\n",
      "Number of authors in df_ocd: 7688\n",
      "Number of authors in df_autism: 2897\n"
     ]
    }
   ],
   "source": [
    "# drop rows where `is_video` or `media_only` columns are True\n",
    "df_ocd = df_ocd[(df_ocd['is_video'] == False) & (df_ocd['media_only'] == False)]\n",
    "df_autism = df_autism[(df_autism['is_video'] == False) & (df_autism['media_only'] == False)]\n",
    "print(f'Dimensions after removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "\n",
    "# opening the scraped data saved in csv files and creating a dataframe for each\n",
    "df_ocd = pd.read_csv('../data/ocd_thread.csv')\n",
    "df_autism = pd.read_csv('../data/autism_thread.csv')\n",
    "\n",
    "# creating a target column for each dataframe\n",
    "df_ocd['target'] = 1\n",
    "df_autism['target'] = 0\n",
    "\n",
    "# drop columns with more than 50% missing values from the dataframes\n",
    "print(f'Dimensions before dropping columns with more than 50% missing values: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "df_ocd = df_ocd.dropna(thresh=0.5*len(df_ocd), axis=1)\n",
    "df_autism = df_autism.dropna(thresh=0.5*len(df_autism), axis=1)\n",
    "print(f'Dimensions after dropping columns with more than 50% missing values: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "\n",
    "print(f'columns in df_ocd: {df_ocd.columns}')\n",
    "#* Only keep the columns in these two dataframes that are in both dataframes and are in the lists below\n",
    "autism_columns_to_keep = ['author', 'author_flair_richtext', 'author_flair_type','created_utc', 'id', 'is_video', 'selftext', 'title', 'is_original_content','media_only', 'author_fullname','target']\n",
    "ocd_columns_to_keep = ['author', 'author_flair_richtext', 'author_flair_type','created_utc', 'id', 'is_video', 'selftext', 'title', 'is_original_content','media_only', 'author_fullname','target']\n",
    "\n",
    "# drop columns that are not in the lists above\n",
    "print(f'Dimensions before dropping columns that are not in the lists above: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "df_ocd = df_ocd[ocd_columns_to_keep] \n",
    "df_autism = df_autism[autism_columns_to_keep]\n",
    "print(f'Dimensions after dropping columns that are not in the lists above: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "\n",
    "# Now remove any posts from these dataframes where the `is_video` or `media_only` columns are True\n",
    "print(f'Dimensions before removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "df_ocd = df_ocd[(df_ocd['is_video'] == False) & (df_ocd['media_only'] == False)]\n",
    "df_autism = df_autism[(df_autism['is_video'] == False) & (df_autism['media_only'] == False)]\n",
    "print(f'Dimensions after removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "\n",
    "# and now we can drop the `is_video` and `media_only` columns\n",
    "df_ocd = df_ocd.drop(columns=['is_video', 'media_only'])\n",
    "df_autism = df_autism.drop(columns=['is_video', 'media_only'])\n",
    "print(f'Dropped the `is_video` and `media_only` columns')\n",
    "\n",
    "# some posts are in the title column and some are in the selftext column so we need to combine these columns into one column if they are long enough.\n",
    "# find the median length of the title and selftext columns combined for each dataframe\n",
    "med_len_title_selftext_ocd = df_ocd.title.str.len().add(df_ocd.selftext.str.len()).median()\n",
    "med_len_title_selftext_autism = df_autism.title.str.len().add(df_autism.selftext.str.len()).median()\n",
    "print(f'Median length of title and selftext columns combined for OCD: {med_len_title_selftext_ocd}')\n",
    "print(f'Median length of title and selftext columns combined for Autism: {med_len_title_selftext_autism}')\n",
    "\n",
    "\n",
    "# how many posts have a title and selftext combined that are longer than the median length of the title and selftext columns combined for each dataframe?\n",
    "print(f'Acceptable number of OCD posts: {len(df_ocd[df_ocd.title.str.len().add(df_ocd.selftext.str.len()) > med_len_title_selftext_ocd])}')\n",
    "print(f'Acceptable number of Autism posts: {len(df_autism[df_autism.title.str.len().add(df_autism.selftext.str.len()) > med_len_title_selftext_autism])}')\n",
    "\n",
    "# remove posts where the title and selftext combined are shorter than the median length of the title and selftext columns combined for each dataframe\n",
    "print(f'Dimensions before: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "df_ocd = df_ocd[df_ocd.title.str.len().add(df_ocd.selftext.str.len()) > med_len_title_selftext_ocd]\n",
    "df_autism = df_autism[df_autism.title.str.len().add(df_autism.selftext.str.len()) > med_len_title_selftext_autism]\n",
    "print(f'Dimensions before: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "\n",
    "# drop author_flair_richtext\n",
    "df_ocd = df_ocd.drop(columns=['author_flair_richtext'])\n",
    "df_autism = df_autism.drop(columns=['author_flair_richtext'])\n",
    "\n",
    "# how many authors are in each dataframe?\n",
    "print(f'Number of authors in df_ocd: {len(df_ocd.author.unique())}')\n",
    "print(f'Number of authors in df_autism: {len(df_autism.author.unique())}')\n",
    "\n",
    "\n",
    "# how many posts are there for the top 100 authors in each dataframe?\n",
    "top_authors_ocd = df_ocd.author.value_counts().head(100)\n",
    "top_authors_byfullname_ocd = df_ocd.author_fullname.value_counts().head(100)\n",
    "top_authors_autism = df_autism.author.value_counts().head(100)\n",
    "top_authors_byfullname_autism = df_autism.author_fullname.value_counts().head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Userur      143\n",
       "corinaah     44\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_authors_ocd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Jupiter642           47\n",
       "anonaskingaccount    32\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_authors_autism.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors that are in both dataframes: 0\n",
      "List of authors that are in both dataframes: []\n"
     ]
    }
   ],
   "source": [
    "# are there any authors that are in both dataframes?\n",
    "print(f'Number of authors that are in both dataframes: {len(set(top_authors_ocd.index).intersection(set(top_authors_autism.index)))}')\n",
    "list_of_cross_posters = list(set(top_authors_ocd.index).intersection(set(top_authors_autism.index)))\n",
    "print(f'List of authors that are in both dataframes: {list_of_cross_posters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random OCD post: Why do some people hate Brain Lock? - My main struggle is with the compulsion to ruminate and so I am trying to discover new techniques to lessen this. I stumbled upon Schwartz and his 4 step plan which goes pretty much like this: Relabel the thought as OCD, reattribute the cause of OCD to a mental disorder instead of yourself, refocus on another activity, revalue the OCD thoughts as meaningless. In another variation, three steps are applied and pretty much go like this: label the thought as a thought/urge/feeling, determine if it is helpful to you right now in the present, refocus on another activity. I also discovered that many people do not use the 4 step plan because it relies on saying “it’s not me, it’s OCD.” Which method, if any, should I use?\n",
      "====================================================================================================\n",
      "Random Autism post: Has anyone else experienced this? I'm a new step father to a child with autism - Just a quick question, more curious than anything. My wife's son/my step son is 7 years old with autism. Very high functioning, he's extremely intelligent, but he unfortunately has a couple other neurological issues that exacerbate things. He's a little butt, but such a sweet child. \n",
      "\n",
      "I'm reading and learning more as I go, but something I just want y'all's opinions on.. he sends WAY better behaved when it's just me and him. So much more well behaved that it's very noticeable. He's as close to perfectly behaved as possible when it's just us (except for the very rare meltdown). But as soon as my wife enters the picture, or he's around her dad, then the bad behavior starts... Which tells me that at least to some extent he knows what he can or can't do and what is right and wrong. Basically I'm just wondering if anyone knows why that is? I thought it was because I'm firm with him, I don't let him get away with saying bad words, throwing, etc (if he does these he has to go to time out/home base and use his calm down techniques until he's ready). My wife tends to give him way more leeway and chances... Like if he's saying bad words, she'll tell him if he says it once more he'll get his iPad taken away, but she won't actually take it until he says it 15-20 more times...\n",
      "\n",
      "Anyways, just wondering if anyone has any thoughts on it, or maybe techniques y'all have used to help curb some behavioral issues. Thank you in advance!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/1701731589.py:26: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_ocd['selftext'] = df_ocd['selftext'].str.replace('[^\\w\\s]','')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/1701731589.py:28: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_ocd['selftext'] = df_ocd['selftext'].str.replace('\\d+', '')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/1701731589.py:30: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_ocd['selftext'] = df_ocd['selftext'].str.replace('\\s+', ' ')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/1701731589.py:35: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_autism['selftext'] = df_autism['selftext'].str.replace('[^\\w\\s]','')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/1701731589.py:37: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_autism['selftext'] = df_autism['selftext'].str.replace('\\d+', '')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/1701731589.py:39: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_autism['selftext'] = df_autism['selftext'].str.replace('\\s+', ' ')\n"
     ]
    }
   ],
   "source": [
    "# drop author_flair_type and author_fullname columns from both dataframes\n",
    "df_ocd = df_ocd.drop(columns=['author_flair_type', 'author_fullname'])\n",
    "df_autism = df_autism.drop(columns=['author_flair_type', 'author_fullname'])\n",
    "# combine the title and self text columns into one column with the format `title - selftext`\n",
    "df_ocd['title_selftext'] = df_ocd.title + ' - ' + df_ocd.selftext\n",
    "df_autism['title_selftext'] = df_autism.title + ' - ' + df_autism.selftext\n",
    "# drop the title and selftext columns\n",
    "df_ocd = df_ocd.drop(columns=['title', 'selftext'])\n",
    "df_autism = df_autism.drop(columns=['title', 'selftext'])\n",
    "\n",
    "# rename the `title_selftext` column to `selftext`\n",
    "df_ocd = df_ocd.rename(columns={'title_selftext': 'selftext'})\n",
    "df_autism = df_autism.rename(columns={'title_selftext': 'selftext'})\n",
    "\n",
    "# randomly sample one post from each dataframe and print it\n",
    "print(f'Random OCD post: {df_ocd.sample(1).selftext.values[0]}')\n",
    "print('='*100)\n",
    "print(f'Random Autism post: {df_autism.sample(1).selftext.values[0]}')\n",
    "\n",
    "cancel_words = ['ocd','aut*','autism','obsess*','compuls*','disorder','executive dysfunction','adhd','diagnosis','ive been taking','spectrum','intrusive thoughts','germaphobes','depression']\n",
    "\n",
    "# apply the censor_words function to the selftext column of each dataframe\n",
    "df_ocd['selftext'] = df_ocd['selftext'].apply(censor_words)\n",
    "\n",
    "# remove punctuation\n",
    "df_ocd['selftext'] = df_ocd['selftext'].str.replace('[^\\w\\s]','')\n",
    "# remove numbers\n",
    "df_ocd['selftext'] = df_ocd['selftext'].str.replace('\\d+', '')\n",
    "# remove whitespace\n",
    "df_ocd['selftext'] = df_ocd['selftext'].str.replace('\\s+', ' ')\n",
    "\n",
    "# do the same for the autism dataframe\n",
    "df_autism['selftext'] = df_autism['selftext'].apply(censor_words)\n",
    "# remove punctuation\n",
    "df_autism['selftext'] = df_autism['selftext'].str.replace('[^\\w\\s]','')\n",
    "# remove numbers\n",
    "df_autism['selftext'] = df_autism['selftext'].str.replace('\\d+', '')\n",
    "# remove whitespace\n",
    "df_autism['selftext'] = df_autism['selftext'].str.replace('\\s+', ' ')\n",
    "\n",
    "# remove words from posts that are in the cancel_words list. There are regex patterns in the cancel_words list so we need to use the `regex=True` parameter\n",
    "\n",
    "# then remove double spaces\n",
    "df_ocd['selftext'] = df_ocd['selftext'].str.replace('  ', ' ')\n",
    "df_autism['selftext'] = df_autism['selftext'].str.replace('  ', ' ')\n",
    "\n",
    "# make a new dataframe called df_reddit that combines the two dataframes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equilibrium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the new dataframe: (18042, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/1551032149.py:16: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  df_reddit = pd.concat([df_reddit, longer_df], axis=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>target</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HyperWendingo</td>\n",
       "      <td>1547408376</td>\n",
       "      <td>afmxb9</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>romance sucks i honestly am starting to believ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest_purple</td>\n",
       "      <td>1551164597</td>\n",
       "      <td>auwfm7</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>how to cope with a simple change and frustrati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alternativesnek</td>\n",
       "      <td>1588998087</td>\n",
       "      <td>gg8o90</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>the guilt behind the rules i hate how much unn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>maxturbated</td>\n",
       "      <td>1568069149</td>\n",
       "      <td>d1yi2z</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>my newtype theory my newtype theorynote skip i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ladychick84</td>\n",
       "      <td>1555951440</td>\n",
       "      <td>bg4ag0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>help need advice for asd year old hi my daught...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author created_utc      id is_original_content target  \\\n",
       "0    HyperWendingo  1547408376  afmxb9               False      0   \n",
       "1    forest_purple  1551164597  auwfm7               False      0   \n",
       "2  alternativesnek  1588998087  gg8o90               False      1   \n",
       "3      maxturbated  1568069149  d1yi2z               False      0   \n",
       "4      ladychick84  1555951440  bg4ag0               False      0   \n",
       "\n",
       "                                            selftext  \n",
       "0  romance sucks i honestly am starting to believ...  \n",
       "1  how to cope with a simple change and frustrati...  \n",
       "2  the guilt behind the rules i hate how much unn...  \n",
       "3  my newtype theory my newtype theorynote skip i...  \n",
       "4  help need advice for asd year old hi my daught...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit = pd.DataFrame(columns=df_ocd.columns)\n",
    "# what is the length of the shorter dataframe?\n",
    "if len(df_ocd) < len(df_autism): # if the OCD dataframe is shorter\n",
    "    shorter_df = df_ocd # set the shorter dataframe to the OCD dataframe\n",
    "    longer_df = df_autism # set the longer dataframe to the Autism dataframe\n",
    "    df_reddit.append()\n",
    "else: # if the Autism dataframe is shorter\n",
    "    shorter_df = df_autism\n",
    "    longer_df = df_ocd\n",
    "\n",
    "# add the shorter dataframe to the new dataframe using concat\n",
    "df_reddit = pd.concat([df_reddit, shorter_df], axis=0)\n",
    "# shorten the longer dataframe to the length of the shorter dataframe\n",
    "longer_df = longer_df.head(len(shorter_df))\n",
    "# add the shortened longer dataframe to the new dataframe using concat\n",
    "df_reddit = pd.concat([df_reddit, longer_df], axis=0)\n",
    "\n",
    "# reset the index\n",
    "df_reddit = df_reddit.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# shuffle the dataframe\n",
    "df_reddit = df_reddit.sample(frac=1).reset_index(drop=True)\n",
    "# check the dimensions of the new dataframe\n",
    "print(f'Dimensions of the new dataframe: {df_reddit.shape}')\n",
    "df_reddit.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts for OCD: 9021\n",
      "Number of posts for Autism: 9021\n"
     ]
    }
   ],
   "source": [
    "# double check that the number of posts for each subreddit is the same\n",
    "print(f'Number of posts for OCD: {len(df_reddit[df_reddit.target == 1])}')\n",
    "print(f'Number of posts for Autism: {len(df_reddit[df_reddit.target == 0])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>target</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>mindkingdom</td>\n",
       "      <td>1527063603</td>\n",
       "      <td>8lhprn</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>thoughts that things could get corrupted or br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>SteelSlayer7</td>\n",
       "      <td>1527074859</td>\n",
       "      <td>8lil9v</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>intrusive thoughts devaluing myself i am plagu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author  created_utc      id is_original_content  target  \\\n",
       "2016   mindkingdom   1527063603  8lhprn               False       1   \n",
       "2020  SteelSlayer7   1527074859  8lil9v               False       1   \n",
       "\n",
       "                                               selftext  \n",
       "2016  thoughts that things could get corrupted or br...  \n",
       "2020  intrusive thoughts devaluing myself i am plagu...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ocd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>target</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Equadex</td>\n",
       "      <td>1546777579</td>\n",
       "      <td>ad56om</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>how can people be considered equals if they ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>936R</td>\n",
       "      <td>1546786707</td>\n",
       "      <td>ad68am</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>questions for females with hi there im looking...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author  created_utc      id  is_original_content  target  \\\n",
       "0  Equadex   1546777579  ad56om                False       0   \n",
       "5     936R   1546786707  ad68am                False       0   \n",
       "\n",
       "                                            selftext  \n",
       "0  how can people be considered equals if they ar...  \n",
       "5  questions for females with hi there im looking...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_autism.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of medications: 3047\n",
      "Number of posts that contain a medication: 18039\n"
     ]
    }
   ],
   "source": [
    "# find any of the medications in the selftext column that are in the data/drug_info.csv file under the Medication Name column and replace them with ' ' (empty string)\n",
    "drug_info = pd.read_csv('../data/drug_info.csv')\n",
    "drug_info['Medication Name'] = drug_info['Medication Name'].str.lower()\n",
    "# create a list of the medications\n",
    "medications = drug_info['Medication Name'].tolist()\n",
    "print(f'Number of medications: {len(medications)}')\n",
    "# how many posts contain a medication?\n",
    "print(f'Number of posts that contain a medication: {len(df_reddit[df_reddit.selftext.str.contains(\"|\".join(medications), regex=True)])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abacavir sulfate'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medications[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/2232114972.py:31: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace('[^\\w\\s]','')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/2232114972.py:33: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace('\\d+', '')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/2232114972.py:37: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/2232114972.py:39: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\n', ' ')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/2232114972.py:41: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'http\\S+', '')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/2232114972.py:43: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'<.*?>', '')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/2232114972.py:45: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+', ' ')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/2232114972.py:47: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'^\\s+', '')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_14895/2232114972.py:49: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+$', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataframe: (12544, 33)\n"
     ]
    }
   ],
   "source": [
    "medications = [med for med in medications if len(med) > 5]\n",
    "# create a list of rows and the medications mentioned in each row\n",
    "import os\n",
    "medications_mentioned = []\n",
    "if os.path.exists('../data/cleaned_reddit.csv'):\n",
    "    pass\n",
    "else:\n",
    "    # with alive_bar (len(df_reddit)) as bar:\n",
    "    for index, row in df_reddit.iterrows(): # iterate through each row in the dataframe\n",
    "        # use regex to find all of the medications in the selftext column\n",
    "        meds = re.findall(r'\\b(?:{})\\b'.format('|'.join(medications)), row['selftext'])\n",
    "        if len(meds) > 0: # if there are medications mentioned in the post\n",
    "            # replace the medications with ' ' (empty string)\n",
    "            row['selftext'] = re.sub(r'\\b(?:{})\\b'.format('|'.join(medications)), ' ', row['selftext'])\n",
    "            medications_mentioned.extend(meds) # add the medications to the medications_mentioned list\n",
    "            # remove duplicate medications\n",
    "            medications_mentioned = list(set(medications_mentioned))\n",
    "            # bar()\n",
    "# remove the words from the selftext column that are in the medications list\n",
    "# if the file does not already exist, create it\n",
    "if os.path.exists('../data/cleaned_reddit.csv'):\n",
    "    # load the file\n",
    "    df_reddit = pd.read_csv('../data/cleaned_reddit.csv')\n",
    "else:\n",
    "    print('File does not exist. Creating it now. Before meds removed from selftext the length of the dataframe is: ', len(df_reddit))\n",
    "    print(f' Removed {len(medications_mentioned)} medications from the selftext column')\n",
    "    # save the dataframe to a csv file\n",
    "    df_reddit.to_csv('../data/cleaned_reddit.csv', index=False)\n",
    "# Now we want to clean the text in the self text column\n",
    "# remove punctuation\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace('[^\\w\\s]','')\n",
    "# remove numbers\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace('\\d+', '')\n",
    "# remove double spaces\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace('  ', ' ')\n",
    "# remove single characters\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n",
    "# remove newlines\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\n', ' ')\n",
    "# remove urls\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'http\\S+', '')\n",
    "# remove html tags\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'<.*?>', '')\n",
    "# remove extra spaces\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+', ' ')\n",
    "# remove extra spaces at the beginning of the string\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'^\\s+', '')\n",
    "# remove extra spaces at the end of the string\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+$', '')\n",
    "\n",
    "# read the file into a dataframe\n",
    "df_reddit = pd.read_csv('../data/cleaned_reddit.csv')\n",
    "# remove any rows that have a null value in the selftext column\n",
    "df_reddit = df_reddit.dropna(subset=['selftext'])\n",
    "# reset the index\n",
    "df_reddit = df_reddit.reset_index(drop=True)\n",
    "# check the dimensions of the dataframe\n",
    "print(f'Dimensions of the dataframe: {df_reddit.shape}')\n",
    "df_reddit.head(5)\n",
    "\n",
    "\n",
    "def num_distinct_words(text,df):\n",
    "    # for this text, find the words that do not appear in any other text in the dataframe column 'selftext'\n",
    "    # split the text into a list of words\n",
    "    if type(text) == str:\n",
    "        text = text.split(' ')\n",
    "        # find the number of words that are not in any other text in the dataframe\n",
    "    else:\n",
    "        # the text is a list of words\n",
    "        words = text\n",
    "        pass\n",
    "    #words = text.split(' ')\n",
    "    # find the number of words that are not in any other text in the dataframe\n",
    "    distinct_words = [word for word in words if word not in df['selftext'].str.split(' ').sum()]\n",
    "    number_distinct_words = len(distinct_words) # find the number of distinct words\n",
    "    return number_distinct_words, distinct_words\n",
    "\n",
    "\n",
    "\n",
    "df = df_reddit.copy() # make a copy of the dataframe\n",
    "\n",
    "#* flag\n",
    "block_active = False # do not run this section of code unless the flag is set to True.\n",
    "\n",
    "\n",
    "# if ../data/distinct_words_and_extras.csv does not exist, then create it with the following code\n",
    "\n",
    "if block_active:\n",
    "    if os.path.exists('../data/distinct_words_and_extras.csv'):\n",
    "        # load the file as df\n",
    "        df = pd.read_csv('../data/distinct_words_and_extras.csv')\n",
    "        print(f'Loaded the file ../data/distinct_words_and_extras.csv')\n",
    "        print(f'   the included features are {df.columns}')\n",
    "    else:\n",
    "        # add selftext_length as a column\n",
    "        df['selftext_length'] = df['selftext'].str.len()\n",
    "        # add number of words as a column (split the selftext column on spaces)\n",
    "        df['num_words'] = df['selftext'].str.split(' ').str.len()\n",
    "        # add number of sentences as a column (split the selftext column on periods)\n",
    "        df['num_sentences'] = df['selftext'].str.split('.').str.len()\n",
    "        # add number of distinct words as a column (words that only this user used)\n",
    "\n",
    "        df['num_distinct_words'] = df['selftext'].str.split(' ').apply(lambda x: num_distinct_words(x,df)[0]) # add the number of distinct words as a column\n",
    "        df['distinct_words'] = df['selftext'].str.split(' ').apply(lambda x: num_distinct_words(x,df)[1]) # add the distinct words as a column\n",
    "\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        df['selftext_length'] = scaler.fit_transform(df['selftext_length'].values.reshape(-1, 1))\n",
    "\n",
    "        print(f'Scaled the new \"selftext_length\" column. The mean is {df[\"selftext_length\"].mean()} and the standard deviation is {df[\"selftext_length\"].std()}')\n",
    "        print(f' The median is {df[\"selftext_length\"].median()} and the max is {df[\"selftext_length\"].max()}')\n",
    "\n",
    "\n",
    "        # save these columns to a csv file\n",
    "        df.to_csv('../data/distinct_words_and_extras.csv', index=False)\n",
    "\n",
    "        print(f'Saved the distinct words used by each user to a csv file \"distinct_words_and_extras.csv\" in the data folder')\n",
    "        print(f'   this file also includes the number of words, sentences, scaled selftext_length, and distinct words used by each user')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "* [X_df.csv](./data/X_df.csv) - The features used in the model.\n",
    "* [autism_thread.csv](./data/autism_thread.csv) - The raw data from the autism thread.\n",
    "* [best_scores.csv](./data/best_scores.csv) - The best scores from the model.\n",
    "* [cleaned_reddit.csv](./data/cleaned_reddit.csv) - The cleaned data from the autism thread.\n",
    "* [cleaned_reddit_withsentiment.csv](./data/cleaned_reddit_withsentiment.csv) - The cleaned data from the autism thread with sentiment analysis. Also includes the OCD thread.\n",
    "* [cvec.csv](./data/cvec.csv) - The cvec data. Count Vectorized data. This is used in the model.\n",
    "* [cvec_vocab.txt](./data/cvec_vocab.txt) - The vocabulary used in the cvec data.\n",
    "* [df_after_feature_engineering.csv](./data/df_after_feature_engineering.csv) - The data after feature engineering. This may not be used in the model.\n",
    "* [df_cleaned.csv](./data/df_cleaned.csv) - The cleaned data from the autism and OCD threads.\n",
    "* [drug_info.csv](./data/drug_info.csv) - The drug information from the drugbank database.\n",
    "* [global_variables.csv](./data/global_variables.csv) - The global variables used in the model.\n",
    "* [master_results_dataframe.csv](./data/master_results_dataframe.csv) - The master results dataframe.\n",
    "* [ocd_thread.csv](./data/ocd_thread.csv) - The raw data from the OCD thread.\n",
    "* [reddit_threads.csv](./data/reddit_threads.csv) - The raw data from the autism and OCD threads.\n",
    "* [tfidf.csv](./data/tfidf.csv) - The tfidf data. Term Frequency Inverse Document Frequency data. This is used in the model.\n",
    "* [tfidf_vocab.txt](./data/tfidf_vocab.txt) - The vocabulary used in the tfidf data.\n",
    "* [y.csv](./data/y.csv) - The target variable used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files and their contents\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('master_env_temp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "666dfdc1c3434f60b2f64fdb13e0b185df8f5878ddae1a39c75e1eb2af091f89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
