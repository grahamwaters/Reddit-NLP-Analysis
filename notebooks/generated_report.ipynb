{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e7ab50",
   "metadata": {},
   "source": [
    "Project Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6871c8cb",
   "metadata": {},
   "source": [
    "Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2b992c",
   "metadata": {},
   "source": [
    "Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41a7cb4",
   "metadata": {},
   "source": [
    "Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ebd4d",
   "metadata": {},
   "source": [
    "```markdown\n",
    " **Problem Statement:** A wealthy donor with a track record of philanthropic contributions to both Autism and OCD research organizations contacted our organization, asking for a model that they can utilize to identify post characteristics on Reddit. The purposes of this study (towards those ends) are to:  1) Use Pushshift API to scrape Reddit posts from the Autism and OCD subreddits. 2) To build a predictive model that can accurately predict whether a post is from the Autism or OCD subreddit  To accomplish these goals, we hypothesize that count vectorization, and Logistic Regression, Adaboost, or Decision Trees can be used to build a model that accurately can predict whether a post is from the Autism or OCD subreddit. Success in this study would mean that our model has a misclassification rate of less than 10% and an accuracy score of greater than 90% on the test data set.  [presentation](https://www.canva.com/design/DAFOTzanP1s/gXPNiG_2EAQ7svxm0Y6dPQ/edit?utm_content=DAFOTzanP1s&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73a23b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fa1cb30",
   "metadata": {},
   "source": [
    "About the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9115621",
   "metadata": {},
   "source": [
    "```markdown\n",
    " Pushshift's API is relatively straightforward. For example, if I want the posts from [`/r/boardgames`](https://www.reddit.com/r/boardgames), all I have to do is use the following URL: https://api.pushshift.io/reddit/search/submission?subreddit=boardgames  # Data Collection To gather data for this analysis, I scraped Reddit for posts on two threads, the `r/Autism` thread, and the `r/OCD` thread.  Before getting too deep into the project and eliminating rows/columns from the data, I want to get a bird's eye view of what I have scraped from these subreddits.  We want the data-cleaning process to be as scalable as possible for future research, so I will use a function to clean the data. This function will clean the data for both the `Autism` and `OCD` subreddits.  # Files Provided and their Sequence The files are ordered as follows:  1. feature_engineering.ipynb # This file contains the code for the feature engineering process. 2. data_cleaning.py # data cleaning functions that I consolidated from a data cleaning notebook for space optimization. 3. data_exploration.ipynb # The exploration of the data. 4. modeling.ipynb # the first iteration of modeling and analysis 5. modeling_beta.ipynb # A streamlined version of modeling.ipynb with added lemmatized models.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8c2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e07b12fd",
   "metadata": {},
   "source": [
    "Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb91f1",
   "metadata": {},
   "source": [
    "```markdown\n",
    "To gather data for this analysis, I scraped Reddit for posts on two threads, the `r/Autism` thread, and the `r/OCD` thread.  Before getting too deep into the project and eliminating rows/columns from the data, I want to get a bird's eye view of what I have scraped from these subreddits.  We want the data-cleaning process to be as scalable as possible for future research, so I will use a function to clean the data. This function will clean the data for both the `Autism` and `OCD` subreddits.  # Files Provided and their Sequence The files are ordered as follows:  1. feature_engineering.ipynb # This file contains the code for the feature engineering process. 2. data_cleaning.py # data cleaning functions that I consolidated from a data cleaning notebook for space optimization. 3. data_exploration.ipynb # The exploration of the data. 4. modeling.ipynb # the first iteration of modeling and analysis 5. modeling_beta.ipynb # A streamlined version of modeling.ipynb with added lemmatized models.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e159dcab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9481d46f",
   "metadata": {},
   "source": [
    "Files Provided and their Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85dcce",
   "metadata": {},
   "source": [
    "```markdown\n",
    "The files are ordered as follows:  1. feature_engineering.ipynb # This file contains the code for the feature engineering process. 2. data_cleaning.py # data cleaning functions that I consolidated from a data cleaning notebook for space optimization. 3. data_exploration.ipynb # The exploration of the data. 4. modeling.ipynb # the first iteration of modeling and analysis 5. modeling_beta.ipynb # A streamlined version of modeling.ipynb with added lemmatized models.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0cc39b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c62c41dd",
   "metadata": {},
   "source": [
    "Model Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f593f4",
   "metadata": {},
   "source": [
    "```markdown\n",
    " The models are saved as pickle files in the `models` folder. The models are named as follows: 1. `logreg.pkl` 2. `adaboost.pkl` 3. `decision_tree.pkl` 4. `lemmatized_logreg.pkl` 5. `lemmatized_adaboost.pkl` 6. `lemmatized_decision_tree.pkl`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9875f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aa39fcf",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dafdbee",
   "metadata": {},
   "source": [
    "```markdown\n",
    " This process took the most time and thought out of any of the sections of this study. During the first iteration and early stages of the project, scores were very high (around 0.98 R2) with little to no data cleaning. However, the model was overfitting and was not generalizable to new data. I had to go back and clean the data to get a more generalizable model. I also identified data leakage due to the words `OCD` and `autism` being present in the titles and selftext fields of the posts. I removed these words from the titles etc., and re-ran the model. This resulted in a decrease in the R2 score.  STIC is an acronym that repeatedly appears in autism posts. For future research I would eliminate this from the analysis or add it to the stop words. In addition I would also remove sensory processing disorder and anything to do with sensory systems. It's also possible that terms like stimming and sm, or tantrum Could be overpowered when it comes to a pure word analysis and may lead to overfitting. I accounted for some of this in my analysis but further analysis would benefit from a more rigorous application of regex and data cleaning to identify new word patterns that could be skewing results one way or the other. Accuracy can be a misleading metric and depending on the goals of the project we may want to optimize for something like an F1 score in the future as opposed to pure accuracy and a comparison to baseline.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f635dabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75b91b73",
   "metadata": {},
   "source": [
    "Example Austim Post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d4b880",
   "metadata": {},
   "source": [
    "```markdown\n",
    " The following post is an example of a post from the `r/Autism` subreddit. The post is not perfectly cleaned to give an idea of some of the cleaning challenges that exist in this dataset. Any mentions of \"autism\" explicitly are removed from the examples in the following sections.   ```output \"Workplace undergoing renovations and I'm drowning. My workplace workspace is undergoing renovations. They built up floor to ceiling temporary walls around my working area while they renovate the space around us. The walls were supposed to absorb sound while construction is going on but it does nothing. I hear banging, drilling, vacuuming and workers screaming at each other. There are 15 people any any time in probably a 100 year old femaleemalet by 100 year old femaleemalet enclosed space. We had open concept before. I am drowning here and having a breakdown. Everyone is poo-pooing my issues. They tell me it is temporary (it will go on 4-6 months). They tell me I'm not the only have to put up with that. They do not know that I am stic. The walls are keeping the sound in and the air out. I have a lot of sensory dysfunction. It is too hot in here because there is no ventilation (this might be temporary until they work out the airlow) and I'm extremely sensitive to heat, and it is making me extremely dizzy. I am dealing with a lot of extra noise and people. My is through the roof. People eat at their desks and in this enclosed space it is creating a lot of extra scents that, while not unpleasant, is strong and I cannot deal with. The construction is creating extra dust and allergens. I feel that these walls are closing in on me. People pick up the phone constantly and it is amplified by the enclosed walls. The list goes on. I want to take short term disability (my company provides it, full base salary for 3 months) while this construction is going on, but I don't know how I can even get a medical professional to approve it. Now I am a person who has not called in sick a single day in the 20+ years My family doctor refuses to deal with mental health issues and referred me to a shrink which I can only see next year. I currently see a registered social worker for but since she is not a medical professional, she cannot write me a disability letter. I don't know how to get help. I'm in Ontario, Canada if anyone can offer me any advice.\" ```  # Example OCD Post ## Example 1 ```output 'Asymmetrical physical exercises are setting me off. I was recently assessed by a physical therapist for an issue with my pelvis where it is rotated somewhat off kilter. Apparently it means my right side of my hip is lower than my left and the left is tilted backwards while the right is more forwards. ```  ## Example 2 ```output \"I think i have  I often get a sudden urge to touch, tap or stroke different objects or surfaces(no not that) on specific places or just the whole thing. If i dont do it i get a very wierd tense feeling in my fingertips and palms. I also get the need to beat the insides of my hands nd fingers very often to get the same kind of feeling to go away. Tapping my fingers againts a solid surface in a rythmic way usually helps with the stress from these instances.     Please tell me if there is a diagnosis for this behaviour. None of the people i ask about this know anything about it and think it's weird.\" ```   ## Balancing the Data  To balance the classes between the OCD and autism subreddits, I sequentially dropped values from the majority class until equilibrium was reached. The resulting dataframe contains balanced distributions of OCD and autism.  Build the classification baseline model to predict the majority class. This is how we get this for our classification problem.   ## User Top Word Analysis  During the data exploration phase, I process the users in the OCD thread to determine their most common words and the top 50 words used for each author. I then use these values as annotations on a Matplotlib visual shown below.  ```python # add 'I' to the list of stopwords # dict of users and their most common word users_aut = df_aut['author'].value_counts().index users_aut_most_common_word = {} for user in users_aut:     user_df = df_aut[df_aut['author'] == user]     user_df = user_df['selftext'].str.split(expand=True).stack().value_counts()[:50].sort_values(ascending=True)     # most common words are in user_df.index, go from most to least common, and pick the first one that is not in the stop words list     for word in user_df.index:         if word not in stopwords_list: # if the word is not in the stop words list             users_aut_most_common_word[user] = word             #print(f'{user} most common word: {word}')             break # break out of the for loop ```    ## OCD and Autism Keywords  In the process of working through these data-cleaning stages, I found that many users use the names of medications in their posts. This could also be a form of data leakage as they are often indicative of a diagnosis. It could be worth examining to see if users that are posting on the autism thread use medication names more or less than those on the ocd thread. Should this be considered a form of data leakage?  ## Medication Leakage  Due to misspellings and abbreviations, I elected to remove the primary medications used to treat individuals diagnosed with obsessive-compulsive disorder or syndrome. This removed a lot of the medication names and other words that were not helpful to the model.  To eliminate disparities between the length of the title and selftext fields, I also combined the two fields into one field. This was done by adding a space between the two fields and concatenating them together as the new `selftext`.  ## Steps for Data Cleaning in this Study 1. Combine title and selftext fields into one field. 2. Generate title-specific features and add them to the dataframe.    1. Is the title a question?    2. Are there all-caps words in the title? How many?    3. What is the length of the title?    4. What is the title's Sentiment Polarity?       1. Positive       2. Negative       3. Neutral       4. Compound 3. Before changing the `selftext` field at all run feature_engineering.ipynb to generate features that illustrate reading time... etc. 4. Clean the `selftext` field.    1. Remove URLs    2. Remove Special Characters    3. Remove OCD and Autism from the `selftext` field.    4. Remove all words related to sex or the activity of physical intimacy, as these are overrepresented in the `r/OCD` thread and will result in overfitting.       1. The one caveat to this step is that any terms that relate to a sexual orientation such as homosexuality or heterosexuality are not removed as they occur evenly in both subreddits.     5. Eliminate any medication names from the text as well as dosage amounts. This is done to eliminate data leakage.     6. Remove all words that are not in the English dictionary.    5. Save the results to `df_clean.csv` for use in the modeling, and data expl 5. Drop rows with a selftext length of less than ten words. 6. Lowercase all words in the `selftext` field. 7. Instantiate a Lemmatizer and Lemmatize all words in the `selftext` field, save this new field as `selftext_lemmatized`. 8. Instantiate a Stemmer and Stem all words in the `selftext` field; save this new field as `selftext_stemmed`.   What words are the most frequent unique words in each of the threads? Create a visual to analyze unique words count versus post length in the autism thread to examine vocabulary density and diversity. Do the same for the `r/OCD` thread.  # Feature Engineering 1. `word_count` - the number of words in each post 2. `unique_word_count` - the number of unique words in each post 3. `post_length` - the length of the post in characters 4. `title_length` - the length of the title in characters 5. `title_word_count` - the number of words in the title 6. `title_unique_word_count` - the number of unique words in the title   It would be interesting to determine which of the two subreddits has a higher percentage of LGBTQ+ terms. This is beyond the scope of this study.  I noticed significant numbers of posters using words like \"hyperactive\" when discussing their children versus themselves. * \"her/his brother/sister\" * \"my son/daughter\" * \"my child\" * \"as parent*\" used by parents. * \"year old\" is usually used by parents describing their child.  These are simply interesting observations and are not meant to inform the model. Instead, they simply augment the analysis with context.  During the data cleaning process, I create two data frames, one for the r/ocd posts and the other for the r/autism posts. These are named df_ocd and df_aut respectively. Finally, I loop through the words in the self-text lemmatized field to determine all words that start with I and are less than or equal to four characters in length to append to my stop words list.   ### Visualizing the Data  I used a combination of `matplotlib` and `seaborn` to visualize the results of the study. I wanted to see how the features were distributed and how they were related to each other. What I was looking for was the presence of outliers and any other anomalies that would need to be addressed before modeling.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4632eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15ab1551",
   "metadata": {},
   "source": [
    "Example OCD Post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9933846",
   "metadata": {},
   "source": [
    "```markdown\n",
    "## Example 1 ```output 'Asymmetrical physical exercises are setting me off. I was recently assessed by a physical therapist for an issue with my pelvis where it is rotated somewhat off kilter. Apparently it means my right side of my hip is lower than my left and the left is tilted backwards while the right is more forwards. ```  ## Example 2 ```output \"I think i have  I often get a sudden urge to touch, tap or stroke different objects or surfaces(no not that) on specific places or just the whole thing. If i dont do it i get a very wierd tense feeling in my fingertips and palms. I also get the need to beat the insides of my hands nd fingers very often to get the same kind of feeling to go away. Tapping my fingers againts a solid surface in a rythmic way usually helps with the stress from these instances.     Please tell me if there is a diagnosis for this behaviour. None of the people i ask about this know anything about it and think it's weird.\" ```   ## Balancing the Data  To balance the classes between the OCD and autism subreddits, I sequentially dropped values from the majority class until equilibrium was reached. The resulting dataframe contains balanced distributions of OCD and autism.  Build the classification baseline model to predict the majority class. This is how we get this for our classification problem.   ## User Top Word Analysis  During the data exploration phase, I process the users in the OCD thread to determine their most common words and the top 50 words used for each author. I then use these values as annotations on a Matplotlib visual shown below.  ```python # add 'I' to the list of stopwords # dict of users and their most common word users_aut = df_aut['author'].value_counts().index users_aut_most_common_word = {} for user in users_aut:     user_df = df_aut[df_aut['author'] == user]     user_df = user_df['selftext'].str.split(expand=True).stack().value_counts()[:50].sort_values(ascending=True)     # most common words are in user_df.index, go from most to least common, and pick the first one that is not in the stop words list     for word in user_df.index:         if word not in stopwords_list: # if the word is not in the stop words list             users_aut_most_common_word[user] = word             #print(f'{user} most common word: {word}')             break # break out of the for loop ```    ## OCD and Autism Keywords  In the process of working through these data-cleaning stages, I found that many users use the names of medications in their posts. This could also be a form of data leakage as they are often indicative of a diagnosis. It could be worth examining to see if users that are posting on the autism thread use medication names more or less than those on the ocd thread. Should this be considered a form of data leakage?  ## Medication Leakage  Due to misspellings and abbreviations, I elected to remove the primary medications used to treat individuals diagnosed with obsessive-compulsive disorder or syndrome. This removed a lot of the medication names and other words that were not helpful to the model.  To eliminate disparities between the length of the title and selftext fields, I also combined the two fields into one field. This was done by adding a space between the two fields and concatenating them together as the new `selftext`.  ## Steps for Data Cleaning in this Study 1. Combine title and selftext fields into one field. 2. Generate title-specific features and add them to the dataframe.    1. Is the title a question?    2. Are there all-caps words in the title? How many?    3. What is the length of the title?    4. What is the title's Sentiment Polarity?       1. Positive       2. Negative       3. Neutral       4. Compound 3. Before changing the `selftext` field at all run feature_engineering.ipynb to generate features that illustrate reading time... etc. 4. Clean the `selftext` field.    1. Remove URLs    2. Remove Special Characters    3. Remove OCD and Autism from the `selftext` field.    4. Remove all words related to sex or the activity of physical intimacy, as these are overrepresented in the `r/OCD` thread and will result in overfitting.       1. The one caveat to this step is that any terms that relate to a sexual orientation such as homosexuality or heterosexuality are not removed as they occur evenly in both subreddits.     5. Eliminate any medication names from the text as well as dosage amounts. This is done to eliminate data leakage.     6. Remove all words that are not in the English dictionary.    5. Save the results to `df_clean.csv` for use in the modeling, and data expl 5. Drop rows with a selftext length of less than ten words. 6. Lowercase all words in the `selftext` field. 7. Instantiate a Lemmatizer and Lemmatize all words in the `selftext` field, save this new field as `selftext_lemmatized`. 8. Instantiate a Stemmer and Stem all words in the `selftext` field; save this new field as `selftext_stemmed`.   What words are the most frequent unique words in each of the threads? Create a visual to analyze unique words count versus post length in the autism thread to examine vocabulary density and diversity. Do the same for the `r/OCD` thread.  # Feature Engineering 1. `word_count` - the number of words in each post 2. `unique_word_count` - the number of unique words in each post 3. `post_length` - the length of the post in characters 4. `title_length` - the length of the title in characters 5. `title_word_count` - the number of words in the title 6. `title_unique_word_count` - the number of unique words in the title   It would be interesting to determine which of the two subreddits has a higher percentage of LGBTQ+ terms. This is beyond the scope of this study.  I noticed significant numbers of posters using words like \"hyperactive\" when discussing their children versus themselves. * \"her/his brother/sister\" * \"my son/daughter\" * \"my child\" * \"as parent*\" used by parents. * \"year old\" is usually used by parents describing their child.  These are simply interesting observations and are not meant to inform the model. Instead, they simply augment the analysis with context.  During the data cleaning process, I create two data frames, one for the r/ocd posts and the other for the r/autism posts. These are named df_ocd and df_aut respectively. Finally, I loop through the words in the self-text lemmatized field to determine all words that start with I and are less than or equal to four characters in length to append to my stop words list.   ### Visualizing the Data  I used a combination of `matplotlib` and `seaborn` to visualize the results of the study. I wanted to see how the features were distributed and how they were related to each other. What I was looking for was the presence of outliers and any other anomalies that would need to be addressed before modeling.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa069a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15ab6825",
   "metadata": {},
   "source": [
    "Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4344fa",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62e448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2120cef9",
   "metadata": {},
   "source": [
    "Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae0e68",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34cefc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "666500da",
   "metadata": {},
   "source": [
    "Balancing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654bb88c",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a500493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bad45a0c",
   "metadata": {},
   "source": [
    "User Top Word Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a498cc1",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5547990f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e38de7b",
   "metadata": {},
   "source": [
    "add 'I' to the list of stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec70f81",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# dict of users and their most common word users_aut = df_aut['author'].value_counts().index users_aut_most_common_word = {} for user in users_aut:     user_df = df_aut[df_aut['author'] == user]     user_df = user_df['selftext'].str.split(expand=True).stack().value_counts()[:50].sort_values(ascending=True)     # most common words are in user_df.index, go from most to least common, and pick the first one that is not in the stop words list     for word in user_df.index:         if word not in stopwords_list: # if the word is not in the stop words list             users_aut_most_common_word[user] = word             #print(f'{user} most common word: {word}')             break # break out of the for loop ```    ## OCD and Autism Keywords  In the process of working through these data-cleaning stages, I found that many users use the names of medications in their posts. This could also be a form of data leakage as they are often indicative of a diagnosis. It could be worth examining to see if users that are posting on the autism thread use medication names more or less than those on the ocd thread. Should this be considered a form of data leakage?  ## Medication Leakage  Due to misspellings and abbreviations, I elected to remove the primary medications used to treat individuals diagnosed with obsessive-compulsive disorder or syndrome. This removed a lot of the medication names and other words that were not helpful to the model.  To eliminate disparities between the length of the title and selftext fields, I also combined the two fields into one field. This was done by adding a space between the two fields and concatenating them together as the new `selftext`.  ## Steps for Data Cleaning in this Study 1. Combine title and selftext fields into one field. 2. Generate title-specific features and add them to the dataframe.    1. Is the title a question?    2. Are there all-caps words in the title? How many?    3. What is the length of the title?    4. What is the title's Sentiment Polarity?       1. Positive       2. Negative       3. Neutral       4. Compound 3. Before changing the `selftext` field at all run feature_engineering.ipynb to generate features that illustrate reading time... etc. 4. Clean the `selftext` field.    1. Remove URLs    2. Remove Special Characters    3. Remove OCD and Autism from the `selftext` field.    4. Remove all words related to sex or the activity of physical intimacy, as these are overrepresented in the `r/OCD` thread and will result in overfitting.       1. The one caveat to this step is that any terms that relate to a sexual orientation such as homosexuality or heterosexuality are not removed as they occur evenly in both subreddits.     5. Eliminate any medication names from the text as well as dosage amounts. This is done to eliminate data leakage.     6. Remove all words that are not in the English dictionary.    5. Save the results to `df_clean.csv` for use in the modeling, and data expl 5. Drop rows with a selftext length of less than ten words. 6. Lowercase all words in the `selftext` field. 7. Instantiate a Lemmatizer and Lemmatize all words in the `selftext` field, save this new field as `selftext_lemmatized`. 8. Instantiate a Stemmer and Stem all words in the `selftext` field; save this new field as `selftext_stemmed`.   What words are the most frequent unique words in each of the threads? Create a visual to analyze unique words count versus post length in the autism thread to examine vocabulary density and diversity. Do the same for the `r/OCD` thread.  # Feature Engineering 1. `word_count` - the number of words in each post 2. `unique_word_count` - the number of unique words in each post 3. `post_length` - the length of the post in characters 4. `title_length` - the length of the title in characters 5. `title_word_count` - the number of words in the title 6. `title_unique_word_count` - the number of unique words in the title   It would be interesting to determine which of the two subreddits has a higher percentage of LGBTQ+ terms. This is beyond the scope of this study.  I noticed significant numbers of posters using words like \"hyperactive\" when discussing their children versus themselves. * \"her/his brother/sister\" * \"my son/daughter\" * \"my child\" * \"as parent*\" used by parents. * \"year old\" is usually used by parents describing their child.  These are simply interesting observations and are not meant to inform the model. Instead, they simply augment the analysis with context.  During the data cleaning process, I create two data frames, one for the r/ocd posts and the other for the r/autism posts. These are named df_ocd and df_aut respectively. Finally, I loop through the words in the self-text lemmatized field to determine all words that start with I and are less than or equal to four characters in length to append to my stop words list.   ### Visualizing the Data  I used a combination of `matplotlib` and `seaborn` to visualize the results of the study. I wanted to see how the features were distributed and how they were related to each other. What I was looking for was the presence of outliers and any other anomalies that would need to be addressed before modeling.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4567716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8641e17e",
   "metadata": {},
   "source": [
    "dict of users and their most common word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a58400a",
   "metadata": {},
   "source": [
    "```markdown\n",
    "users_aut = df_aut['author'].value_counts().index users_aut_most_common_word = {} for user in users_aut:     user_df = df_aut[df_aut['author'] == user]     user_df = user_df['selftext'].str.split(expand=True).stack().value_counts()[:50].sort_values(ascending=True)     # most common words are in user_df.index, go from most to least common, and pick the first one that is not in the stop words list     for word in user_df.index:         if word not in stopwords_list: # if the word is not in the stop words list             users_aut_most_common_word[user] = word             #print(f'{user} most common word: {word}')             break # break out of the for loop ```    ## OCD and Autism Keywords  In the process of working through these data-cleaning stages, I found that many users use the names of medications in their posts. This could also be a form of data leakage as they are often indicative of a diagnosis. It could be worth examining to see if users that are posting on the autism thread use medication names more or less than those on the ocd thread. Should this be considered a form of data leakage?  ## Medication Leakage  Due to misspellings and abbreviations, I elected to remove the primary medications used to treat individuals diagnosed with obsessive-compulsive disorder or syndrome. This removed a lot of the medication names and other words that were not helpful to the model.  To eliminate disparities between the length of the title and selftext fields, I also combined the two fields into one field. This was done by adding a space between the two fields and concatenating them together as the new `selftext`.  ## Steps for Data Cleaning in this Study 1. Combine title and selftext fields into one field. 2. Generate title-specific features and add them to the dataframe.    1. Is the title a question?    2. Are there all-caps words in the title? How many?    3. What is the length of the title?    4. What is the title's Sentiment Polarity?       1. Positive       2. Negative       3. Neutral       4. Compound 3. Before changing the `selftext` field at all run feature_engineering.ipynb to generate features that illustrate reading time... etc. 4. Clean the `selftext` field.    1. Remove URLs    2. Remove Special Characters    3. Remove OCD and Autism from the `selftext` field.    4. Remove all words related to sex or the activity of physical intimacy, as these are overrepresented in the `r/OCD` thread and will result in overfitting.       1. The one caveat to this step is that any terms that relate to a sexual orientation such as homosexuality or heterosexuality are not removed as they occur evenly in both subreddits.     5. Eliminate any medication names from the text as well as dosage amounts. This is done to eliminate data leakage.     6. Remove all words that are not in the English dictionary.    5. Save the results to `df_clean.csv` for use in the modeling, and data expl 5. Drop rows with a selftext length of less than ten words. 6. Lowercase all words in the `selftext` field. 7. Instantiate a Lemmatizer and Lemmatize all words in the `selftext` field, save this new field as `selftext_lemmatized`. 8. Instantiate a Stemmer and Stem all words in the `selftext` field; save this new field as `selftext_stemmed`.   What words are the most frequent unique words in each of the threads? Create a visual to analyze unique words count versus post length in the autism thread to examine vocabulary density and diversity. Do the same for the `r/OCD` thread.  # Feature Engineering 1. `word_count` - the number of words in each post 2. `unique_word_count` - the number of unique words in each post 3. `post_length` - the length of the post in characters 4. `title_length` - the length of the title in characters 5. `title_word_count` - the number of words in the title 6. `title_unique_word_count` - the number of unique words in the title   It would be interesting to determine which of the two subreddits has a higher percentage of LGBTQ+ terms. This is beyond the scope of this study.  I noticed significant numbers of posters using words like \"hyperactive\" when discussing their children versus themselves. * \"her/his brother/sister\" * \"my son/daughter\" * \"my child\" * \"as parent*\" used by parents. * \"year old\" is usually used by parents describing their child.  These are simply interesting observations and are not meant to inform the model. Instead, they simply augment the analysis with context.  During the data cleaning process, I create two data frames, one for the r/ocd posts and the other for the r/autism posts. These are named df_ocd and df_aut respectively. Finally, I loop through the words in the self-text lemmatized field to determine all words that start with I and are less than or equal to four characters in length to append to my stop words list.   ### Visualizing the Data  I used a combination of `matplotlib` and `seaborn` to visualize the results of the study. I wanted to see how the features were distributed and how they were related to each other. What I was looking for was the presence of outliers and any other anomalies that would need to be addressed before modeling.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3145c1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b454da4e",
   "metadata": {},
   "source": [
    "OCD and Autism Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7811183d",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa987c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bde3897",
   "metadata": {},
   "source": [
    "Medication Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573611a4",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b5b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76353337",
   "metadata": {},
   "source": [
    "Steps for Data Cleaning in this Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f6a6d9",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc15a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13bf2c38",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0dcc70",
   "metadata": {},
   "source": [
    "```markdown\n",
    "1. `word_count` - the number of words in each post 2. `unique_word_count` - the number of unique words in each post 3. `post_length` - the length of the post in characters 4. `title_length` - the length of the title in characters 5. `title_word_count` - the number of words in the title 6. `title_unique_word_count` - the number of unique words in the title   It would be interesting to determine which of the two subreddits has a higher percentage of LGBTQ+ terms. This is beyond the scope of this study.  I noticed significant numbers of posters using words like \"hyperactive\" when discussing their children versus themselves. * \"her/his brother/sister\" * \"my son/daughter\" * \"my child\" * \"as parent*\" used by parents. * \"year old\" is usually used by parents describing their child.  These are simply interesting observations and are not meant to inform the model. Instead, they simply augment the analysis with context.  During the data cleaning process, I create two data frames, one for the r/ocd posts and the other for the r/autism posts. These are named df_ocd and df_aut respectively. Finally, I loop through the words in the self-text lemmatized field to determine all words that start with I and are less than or equal to four characters in length to append to my stop words list.   ### Visualizing the Data  I used a combination of `matplotlib` and `seaborn` to visualize the results of the study. I wanted to see how the features were distributed and how they were related to each other. What I was looking for was the presence of outliers and any other anomalies that would need to be addressed before modeling.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a2ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32895efa",
   "metadata": {},
   "source": [
    "Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d84ad9",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eaccbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4228fd31",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a29e57",
   "metadata": {},
   "source": [
    "```markdown\n",
    " Getting into the details of these threads was a fascinating experience. I learned a lot about the kinds of posts one can find in these two forums. After doing background research on the topic I found one paper that coined the term `mood profile` (Gedanke, 2018) to describe a subreddit based on sentiment scores. I decided to use this term to describe the sentiment of the posts in the `r/Autism` and `r/OCD` subreddits.  Title Length Distribution (entire dataset)  ![title_length_distribution](./images/title_length_distribution.png)    Top 25 Users by Post Count in the `r/OCD` Thread  ![](./images/top_25_users_ocd_by_posts_with_word.png)  This reveals that there are a large number of posts that have the author deleted, which could mean a deactivated account. This is not a problem for the model as it will not use the author's name as a feature. It would be helpful to remove the posts with the author deleted, though, as they are not valid for the model.  Top 25 Users by Post Count in the `r/Autism` Thread   ![](./images/top_25_users_aut_by_posts_with_word.png)  There is no way to know which deleted accounts posted, and they are all added together. This makes the group very noisy and not valid for the model. Therefore, I will remove these posts from the dataset.  The visual below shows the post frequency by the hour for the two threads. They both pick up in the evening, but OCD has a notable increase in posts early in the day at around 7:00 AM compared to autism which has its minimum at 10:00 AM.  ![](./images/post_frequency_by_hour_of_day.png)   I recommend using named entity recognition as well to identify medication names in the raw text. I specifically would use SpaCy for this. SpaCy was trained on Reddit posts for this very purpose and Is a very robust tool for analysis. I also recommend. upgrading this to the higher quality large data set once the analysis becomes broader and includes different text sources. If the next phases of the project that are beyond the scope of this study involved more text it would be useful.  For now, we will run with the list of medications below and remove them from the text. This will eliminate data leakage and will also help the model generalize better. Medications used for OCD are myriad; however, a starting list from medical sources indicates the following. Clonidine, Quetiapine, Risperidone, Vyvanse, Adderall, Dexedrine, Wellbutrin, Focalin XR, Modafinil, Fluvoxamine, Serzone, Fluvoxamine, Prozac, Lexapro, Paxil, Celexa, Effexor, Zoloft, Cymbalta, Luvox, Pristiq, Remeron, Venlafaxine, Sarafem, Anafranil, Nortriptyline, Tofranil, Xanax, Klonopin, Ativan, Valium, Buspirone, Oxazepam, Aripiprazole, dextroamphetamine, and medications in the SSRI or SNRI families. These include: Antidepressants – Selective serotonin reuptake inhibitors (SSRIs), such as fluoxetine and paroxetine. Benzodiazepines – Diazepam, clonazepam, lorazepam, temazepam, alprazolam, chlordiazepoxide, flurazepam, oxazepam, triazolam, divalproex sodium, dronabinol, nabilone, and duloxetine. These will be filtered in the code to eliminate leakage.  (Negrini, 2021)  Bigrams and Trigrams are great ways to examine text data as well. ![](./images/top_25_bigrams_ocd.png) ![](./images/top_25_trigrams_ocd.png)  ![](./images/top_25_bigrams_aut.png) ![](./images/top_25_trigrams_aut.png)  ## Model 1. Alpha Model  Alpha Model is the first model that I built. It is a simple model that utilizes a pipeline with count vectorization and multiple logistic regression to classify posts from either thread. To optimize the model and \"tune\" it to give the most accurate results, I utilized sklearn's GridsearchCV package to optimize over a set of hyperparameters.  After my first models were trained, I had a very high r2 score. I revisited the data cleaning stage to narrow down the posts a little bit more before continuing with further iterations of my models.  See [modeling.ipynb](notebooks/modeling.ipynb) for this model   Now that the data has been cleaned I can move on to the next iteration of my model.  ## Model 2. Beta Models  ### Model 2.1. Logistic Regression  Training score: 0.991991643454039 Testing score: 0.9139972144846796 Best score: 0.8992571959145775 Best params: {'cvec__max_df': 0.9, 'cvec__max_features': 3000, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 1), 'logreg__C': 1, 'logreg__penalty': 'l2'} Best estimator: Pipeline(steps=[('cvec', CountVectorizer(max_df=0.9, max_features=3000, min_df=2)), ('logreg', LogisticRegression(C=1, solver='liblinear'))]) The training score is 99.1% while the score on the testing data is 91.3%. This means that. The model is good. But. It may be a little bit too good. The first model uses account vectorizer in conjunction with a simple logistic regression and I utilized a grid search to optimize the model over a set of parameters. The parameters are shown below.  ```output pipe_params_sent_len = {     'cvec__max_features': [1000, 2000, 3000],     'cvec__min_df': [2, 3],     'cvec__max_df': [.9, .95],     'cvec__ngram_range': [(1,1), (1,2)],     'logreg__penalty': ['l1','l2'],     'logreg__C': [1, 2, 3] } ```  Something to pay attention to in this set of parameters is the penalty terms L1 and L2 these correspond to lasso and ridge regressions. I also tested in grams in ranges one to two.   I elected to use three-fold cross-validation instead of 5/4 optimizing for a time. The client wanted results as quickly as possible and this seemed like the most logical way. A confusion matrix for the logistic regression model reveals that the true positives and true negatives are fairly even at 1312 and 1313 respectively interestingly the false positives and false negatives are also evenly matched at 123 and 124 respectively   ### Model 2.2. Adaboost  The second model that I tested was an AdaBoost model that has used logistic regression as its base estimator. My parameters for the ADA boost model are shown below, and I tested 50, 100, and 150 estimators with varying learning rates from .1 up to 1 I also instantiate a grid search on this Adaboost model with three-fold cross-validation and the results of this model were interesting. The score on the training data was .95 or 95.3% accurate while the score on the testing data was 91.8% accurate. This is an improvement of 41.8% over the baseline accuracy but I am still interested to see what further testing can show. The best parameters were a learning rate of 1 with 150 estimators 3000 Max features for the account correct riser and a Max DF for the count vectorizer of 0.90. This model performed the best in the end outperforming baseline accuracy by 41.8% and outperforming the logistic regression model by 0.5%.  ### Model 2.3 Decision Tree My third model was the decision tree model. I ended up with my best parameters for this model being a Max DF of 0.9, 2000 Max features for the count vectorizer and a one by two Ngram range When I first ran this code before I did a lot of data cleaning and realized that there was data leakage. At the first, I was getting a training score here of 1.0 and a testing score of 0.9. The scores on my training set for the final decision tree model are 99.4% and 81.9% on the testing set. This means that the decision tree model performed 31 percent better than guessing at random.  ## Models using Lemmatization  Results did not improve enough to warrant using lemmatization. I will leave the code in the repository for reference but will not be using it in the final model. The results for the lemmatized models are as follows:  * The adaboost model scored 89.1% on the testing set and 98.9% on the training set. * The logistic regression scored 80% on the testing and 99.4% on the training set. * The decision tree scored 88.9% on the testing and 90.2% on the training set.  Ultimately if we are looking to make statements about what kinds of words can lend weight to a post belonging to threads A or B (autism or OCD) and thus desire interpretability we would want to look at the logistic regression model as it could offer insight into inference while the others tend to be more opaque. Overall lemmatizeation did not seem to improve the scores on the testing set though in reality it might have improved the quality of the analysis.  # What posts were misclassified?  ## Misclassified posts  If we are applying this model to a clinical application we want to make sure that we would optimize for something like recall because recall it is focused on reducing the false negatives.  # Conclusions and Recommendations My initial hypothesis that a combination of key NLP techniques such as sentiment analysis and count vectorization, could be used to build a model that accurately is able to predict whether a post is from the Autism or OCD subreddit was not sufficiently proved by the analysis.  Our Alpha Model was good at predicting which subreddit a post belonged to, with accuracy scores around 90 to 99% on the training data. It also scored extremely high on the test data, which was an indicator that some features within the data were overpowering the model and causing it to overfit. One could stop there and be done with their analysis, saying that the model does technically predict which subreddit a post belongs to, but I wanted to see if I could improve the model for scalability. For this reason, I moved the next stage and created my Beta Model.  My Beta Models were able to predict with 91.8% accuracy (on the test set) whether a post was from the Autism or OCD subreddit. This is a significant improvement over the baseline accuracy of 50%. However, it is still not good enough to be used in a production environment. The model is still overfitting, and I believe that this is due to the fact that the data is not clean enough. There are still a lot of words that are not relevant to the model, and I believe that if I were to clean the data further, I would be able to improve the model.  My final recommendations are that we number one gather more data and consistently measure these two subreddits to gain a more holistic understanding of what these populations enjoy, what they participate in, what kinds of verbs they use, or nouns they prefer. I would also really like to explore one of the features that I created that has to do with questions. How many users post questions versus discussions and are these skewed towards one or the other forum?  There are 8949 unique users in the data frame and this is a large number when you consider the neuro diversity that exists not only on a spectrum but also on the side of OCD. This study opens many doors for future work and shows promising results though less in the area of linguistics and more in the area of simple prediction.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d413b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a309745",
   "metadata": {},
   "source": [
    "Model 1. Alpha Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca7477",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c2f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ff1a6fa",
   "metadata": {},
   "source": [
    "Model 2. Beta Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16870e0",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be69b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28c9d178",
   "metadata": {},
   "source": [
    "Model 2.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961d4fa1",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91120f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25580fa7",
   "metadata": {},
   "source": [
    "Model 2.2. Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e9f8a",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e6864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6c6cf1b",
   "metadata": {},
   "source": [
    "Model 2.3 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3791b8a",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd2786b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24ef316d",
   "metadata": {},
   "source": [
    "Models using Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e2cf7",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1dfd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a0689e3",
   "metadata": {},
   "source": [
    "What posts were misclassified?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e835183d",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980fd8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28b4a1b2",
   "metadata": {},
   "source": [
    "Misclassified posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f684810a",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a4ee1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cdfed12",
   "metadata": {},
   "source": [
    "Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2e9bf8",
   "metadata": {},
   "source": [
    "```markdown\n",
    "My initial hypothesis that a combination of key NLP techniques such as sentiment analysis and count vectorization, could be used to build a model that accurately is able to predict whether a post is from the Autism or OCD subreddit was not sufficiently proved by the analysis.  Our Alpha Model was good at predicting which subreddit a post belonged to, with accuracy scores around 90 to 99% on the training data. It also scored extremely high on the test data, which was an indicator that some features within the data were overpowering the model and causing it to overfit. One could stop there and be done with their analysis, saying that the model does technically predict which subreddit a post belongs to, but I wanted to see if I could improve the model for scalability. For this reason, I moved the next stage and created my Beta Model.  My Beta Models were able to predict with 91.8% accuracy (on the test set) whether a post was from the Autism or OCD subreddit. This is a significant improvement over the baseline accuracy of 50%. However, it is still not good enough to be used in a production environment. The model is still overfitting, and I believe that this is due to the fact that the data is not clean enough. There are still a lot of words that are not relevant to the model, and I believe that if I were to clean the data further, I would be able to improve the model.  My final recommendations are that we number one gather more data and consistently measure these two subreddits to gain a more holistic understanding of what these populations enjoy, what they participate in, what kinds of verbs they use, or nouns they prefer. I would also really like to explore one of the features that I created that has to do with questions. How many users post questions versus discussions and are these skewed towards one or the other forum?  There are 8949 unique users in the data frame and this is a large number when you consider the neuro diversity that exists not only on a spectrum but also on the side of OCD. This study opens many doors for future work and shows promising results though less in the area of linguistics and more in the area of simple prediction.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb3ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e277ee3a",
   "metadata": {},
   "source": [
    "Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0de2f3d",
   "metadata": {},
   "source": [
    "```markdown\n",
    " Future work could be done examining some of the features that the client requested, such as word count, unique word count, post link, title, length, title, and word count. Title unique word count as well as being able to determine distinctively whether the poster is a parent. Using analysis of words like hyperactive when discussing their children versus themselves.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada2bfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25e2eb47",
   "metadata": {},
   "source": [
    "Works Cited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df6eb8",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c0f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e329e4b",
   "metadata": {},
   "source": [
    "Appendix A. - Hyperparameters used for Model 1. Alpha Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e44ec3",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f67bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb02084f",
   "metadata": {},
   "source": [
    "Appendix B. - Hyperparameters used for Beta Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a7b08",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa82e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2c62be0",
   "metadata": {},
   "source": [
    "Appendix C. Exemplary posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7293a6a4",
   "metadata": {},
   "source": [
    "```markdown\n",
    "Section Text Not Found\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6ffbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
