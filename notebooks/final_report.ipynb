{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/grahamwaters/.local/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/grahamwaters/.local/lib/python3.9/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /Users/grahamwaters/.local/lib/python3.9/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in /Users/grahamwaters/.local/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /Users/grahamwaters/.local/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: bs4 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Requirement already satisfied: scipy in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy<1.26.0,>=1.18.5 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from scipy) (1.23.4)\n",
      "Requirement already satisfied: sklearn in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (0.0.post1)\n",
      "Requirement already satisfied: pandas in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: requests in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from requests) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from requests) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from requests) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: nltk in /Users/grahamwaters/.local/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in /Users/grahamwaters/.local/lib/python3.9/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in /Users/grahamwaters/.local/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /Users/grahamwaters/.local/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/grahamwaters/.local/lib/python3.9/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: seaborn in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (0.12.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from seaborn) (1.23.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from seaborn) (3.6.2)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from seaborn) (1.5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.38.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from pandas>=0.25->seaborn) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in /Users/grahamwaters/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk --user;\n",
    "!pip3 install bs4;\n",
    "!pip3 install scipy;\n",
    "!pip3 install sklearn;\n",
    "!pip3 install pandas;\n",
    "!pip3 install requests;\n",
    "!pip3 install nltk;\n",
    "!pip3 install seaborn;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "import re\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# adaboost imports\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Tree imports\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import export_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_keywords(post):\n",
    "    \"\"\"Get the keywords from a post\"\"\"\n",
    "    # Get the keywords from the post\n",
    "    keywords = set()\n",
    "    for word in re.split(\"\\W+\", post.text):\n",
    "        if word in keywords:\n",
    "            continue\n",
    "        else:\n",
    "            keywords.add(word)\n",
    "    return keywords\n",
    "\n",
    "\n",
    "# define the stop words list\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Remove Punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from a string\"\"\"\n",
    "    return ''.join(ch for ch in text if ch not in stop_words)\n",
    "\n",
    "# Lower Case\n",
    "def lowercase(text):\n",
    "    \"\"\"Lower case a string\"\"\"\n",
    "    return text.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_51534/972057233.py:2: DtypeWarning: Columns (5,27,50,51,53,54,56,57,60,61,63,67,68,75,76,77,80,81,82,83,84,85,87,88,89,90,91) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_ocd = pd.read_csv('../data/ocd_thread.csv')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_51534/972057233.py:3: DtypeWarning: Columns (70,71,74,75,76,77,78,79,80,82,83,84,85,86,87,88) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_autism = pd.read_csv('../data/autism_thread.csv')\n"
     ]
    }
   ],
   "source": [
    "# opening the scraped data saved in csv files and creating a dataframe for each\n",
    "df_ocd = pd.read_csv('../data/ocd_thread.csv')\n",
    "df_autism = pd.read_csv('../data/autism_thread.csv')\n",
    "\n",
    "# creating a target column for each dataframe\n",
    "df_ocd['target'] = 1\n",
    "df_autism['target'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions before dropping columns with more than 50% missing values: (41449, 93) for OCD and (25750, 90) for Autism\n",
      "Dimensions after dropping columns with more than 50% missing values: (41449, 51) for OCD and (25750, 52) for Autism\n"
     ]
    }
   ],
   "source": [
    "# drop columns with more than 50% missing values from the dataframes\n",
    "print(f'Dimensions before dropping columns with more than 50% missing values: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "df_ocd = df_ocd.dropna(thresh=0.5*len(df_ocd), axis=1)\n",
    "df_autism = df_autism.dropna(thresh=0.5*len(df_autism), axis=1)\n",
    "print(f'Dimensions after dropping columns with more than 50% missing values: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns in df_ocd: Index(['author', 'author_flair_richtext', 'author_flair_type', 'can_mod_post',\n",
      "       'contest_mode', 'created_utc', 'domain', 'full_link', 'id',\n",
      "       'is_crosspostable', 'is_reddit_media_domain', 'is_self', 'is_video',\n",
      "       'link_flair_richtext', 'link_flair_text_color', 'link_flair_type',\n",
      "       'locked', 'num_comments', 'num_crossposts', 'over_18',\n",
      "       'parent_whitelist_status', 'permalink', 'pinned', 'retrieved_on',\n",
      "       'score', 'selftext', 'spoiler', 'stickied', 'subreddit', 'subreddit_id',\n",
      "       'subreddit_type', 'thumbnail', 'title', 'url', 'whitelist_status',\n",
      "       'send_replies', 'no_follow', 'subreddit_subscribers',\n",
      "       'is_original_content', 'pwls', 'wls', 'media_only', 'is_meta',\n",
      "       'author_fullname', 'gildings', 'is_robot_indexable',\n",
      "       'author_patreon_flair', 'all_awardings', 'total_awards_received',\n",
      "       'allow_live_comments', 'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f'columns in df_ocd: {df_ocd.columns}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Only keep the columns in these two dataframes that are in both dataframes and are in the lists below\n",
    "autism_columns_to_keep = ['author', 'author_flair_richtext', 'author_flair_type','created_utc', 'id', 'is_video', 'selftext', 'title', 'is_original_content','media_only', 'author_fullname','target']\n",
    "ocd_columns_to_keep = ['author', 'author_flair_richtext', 'author_flair_type','created_utc', 'id', 'is_video', 'selftext', 'title', 'is_original_content','media_only', 'author_fullname','target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions before dropping columns that are not in the lists above: (41449, 51) for OCD and (25750, 52) for Autism\n",
      "Dimensions after dropping columns that are not in the lists above: (41449, 12) for OCD and (25750, 12) for Autism\n"
     ]
    }
   ],
   "source": [
    "# drop columns that are not in the lists above\n",
    "print(f'Dimensions before dropping columns that are not in the lists above: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "df_ocd = df_ocd[ocd_columns_to_keep] \n",
    "df_autism = df_autism[autism_columns_to_keep]\n",
    "print(f'Dimensions after dropping columns that are not in the lists above: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions before removing posts where `is_video` or `media_only` columns are True: (41449, 12) for OCD and (25750, 12) for Autism\n",
      "Dimensions after removing posts where `is_video` or `media_only` columns are True: (37323, 12) for OCD and (25540, 12) for Autism\n",
      "Dropped the `is_video` and `media_only` columns\n"
     ]
    }
   ],
   "source": [
    "# Now remove any posts from these dataframes where the `is_video` or `media_only` columsn are True\n",
    "print(f'Dimensions before removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "df_ocd = df_ocd[(df_ocd['is_video'] == False) & (df_ocd['media_only'] == False)]\n",
    "df_autism = df_autism[(df_autism['is_video'] == False) & (df_autism['media_only'] == False)]\n",
    "print(f'Dimensions after removing posts where `is_video` or `media_only` columns are True: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "\n",
    "# and now we can drop the `is_video` and `media_only` columns\n",
    "df_ocd = df_ocd.drop(columns=['is_video', 'media_only'])\n",
    "df_autism = df_autism.drop(columns=['is_video', 'media_only'])\n",
    "print(f'Dropped the `is_video` and `media_only` columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median length of title and selftext columns combined for OCD: 652.0\n",
      "Median length of title and selftext columns combined for Autism: 470.0\n"
     ]
    }
   ],
   "source": [
    "# some posts are in the title column and some are in the selftext column so we need to combine these columns into one column if they are long enough.\n",
    "# find the median length of the title and selftext columns combined for each dataframe\n",
    "med_len_title_selftext_ocd = df_ocd.title.str.len().add(df_ocd.selftext.str.len()).median()\n",
    "med_len_title_selftext_autism = df_autism.title.str.len().add(df_autism.selftext.str.len()).median()\n",
    "print(f'Median length of title and selftext columns combined for OCD: {med_len_title_selftext_ocd}')\n",
    "print(f'Median length of title and selftext columns combined for Autism: {med_len_title_selftext_autism}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceptable number of OCD posts: 16343\n",
      "Acceptable number of Autism posts: 9021\n"
     ]
    }
   ],
   "source": [
    "# how many posts have a title and selftext combined that are longer than the median length of the title and selftext columns combined for each dataframe?\n",
    "print(f'Acceptable number of OCD posts: {len(df_ocd[df_ocd.title.str.len().add(df_ocd.selftext.str.len()) > med_len_title_selftext_ocd])}')\n",
    "print(f'Acceptable number of Autism posts: {len(df_autism[df_autism.title.str.len().add(df_autism.selftext.str.len()) > med_len_title_selftext_autism])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions before: (37323, 10) for OCD and (25540, 10) for Autism\n",
      "Dimensions before: (16343, 10) for OCD and (9021, 10) for Autism\n"
     ]
    }
   ],
   "source": [
    "# remove posts where the title and selftext combined are shorter than the median length of the title and selftext columns combined for each dataframe\n",
    "print(f'Dimensions before: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')\n",
    "df_ocd = df_ocd[df_ocd.title.str.len().add(df_ocd.selftext.str.len()) > med_len_title_selftext_ocd]\n",
    "df_autism = df_autism[df_autism.title.str.len().add(df_autism.selftext.str.len()) > med_len_title_selftext_autism]\n",
    "print(f'Dimensions before: {df_ocd.shape} for OCD and {df_autism.shape} for Autism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop author_flair_richtext\n",
    "df_ocd = df_ocd.drop(columns=['author_flair_richtext'])\n",
    "df_autism = df_autism.drop(columns=['author_flair_richtext'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors in df_ocd: 7688\n",
      "Number of authors in df_autism: 2897\n"
     ]
    }
   ],
   "source": [
    "# how many authors are in each dataframe?\n",
    "print(f'Number of authors in df_ocd: {len(df_ocd.author.unique())}')\n",
    "print(f'Number of authors in df_autism: {len(df_autism.author.unique())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many posts are there for the top 100 authors in each dataframe?\n",
    "top_authors_ocd = df_ocd.author.value_counts().head(100)\n",
    "top_authors_byfullname_ocd = df_ocd.author_fullname.value_counts().head(100)\n",
    "top_authors_autism = df_autism.author.value_counts().head(100)\n",
    "top_authors_byfullname_autism = df_autism.author_fullname.value_counts().head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Userur      143\n",
       "corinaah     44\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_authors_ocd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Jupiter642           47\n",
       "anonaskingaccount    32\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_authors_autism.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors that are in both dataframes: 0\n",
      "List of authors that are in both dataframes: []\n"
     ]
    }
   ],
   "source": [
    "# are there any authors that are in both dataframes?\n",
    "print(f'Number of authors that are in both dataframes: {len(set(top_authors_ocd.index).intersection(set(top_authors_autism.index)))}')\n",
    "list_of_cross_posters = list(set(top_authors_ocd.index).intersection(set(top_authors_autism.index)))\n",
    "print(f'List of authors that are in both dataframes: {list_of_cross_posters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop author_flair_type and author_fullname columns from both dataframes\n",
    "df_ocd = df_ocd.drop(columns=['author_flair_type', 'author_fullname'])\n",
    "df_autism = df_autism.drop(columns=['author_flair_type', 'author_fullname'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the title and self text columns into one column with the format `title - selftext`\n",
    "df_ocd['title_selftext'] = df_ocd.title + ' - ' + df_ocd.selftext\n",
    "df_autism['title_selftext'] = df_autism.title + ' - ' + df_autism.selftext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the title and selftext columns\n",
    "df_ocd = df_ocd.drop(columns=['title', 'selftext'])\n",
    "df_autism = df_autism.drop(columns=['title', 'selftext'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the `title_selftext` column to `selftext`\n",
    "df_ocd = df_ocd.rename(columns={'title_selftext': 'selftext'})\n",
    "df_autism = df_autism.rename(columns={'title_selftext': 'selftext'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first five selftext posts for each dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning out the post data (to remove biasing factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancel_words = ['ocd','aut*','autism','obsess*','compuls*','disorder','diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_51534/2951519817.py:24: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_ocd['selftext'] = df_ocd['selftext'].str.replace('[^\\w\\s]','')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_51534/2951519817.py:26: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_ocd['selftext'] = df_ocd['selftext'].str.replace('\\d+', '')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_51534/2951519817.py:28: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_ocd['selftext'] = df_ocd['selftext'].str.replace('\\s+', ' ')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_51534/2951519817.py:33: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_autism['selftext'] = df_autism['selftext'].str.replace('[^\\w\\s]','')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_51534/2951519817.py:35: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_autism['selftext'] = df_autism['selftext'].str.replace('\\d+', '')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_51534/2951519817.py:37: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_autism['selftext'] = df_autism['selftext'].str.replace('\\s+', ' ')\n"
     ]
    }
   ],
   "source": [
    "def censor_words(text):\n",
    "    text = text.lower()\n",
    "    # Remove all words that begin with 'aut' from the sentence and return the result\n",
    "    # regex pattern\n",
    "    pattern = r'aut(.*?)[^a-zA-Z]' # aut followed by any number of characters then ending in any character that is not a letter\n",
    "    # replace those pattern matches with '' (nothing)\n",
    "    text =  re.sub(pattern, '', text) # replace the pattern matches with '' (nothing)\n",
    "    \n",
    "    # pattern 2 - remove all words that begin with 'ocd' from the sentence and return the result\n",
    "    pattern = r'ocd(.*?)[^a-zA-Z]' # ocd followed by any number of characters then ending in any character that is not a letter\n",
    "    # replace those pattern matches with '' (nothing)\n",
    "    text =  re.sub(pattern, '', text) # replace the pattern matches with '' (nothing)\n",
    "\n",
    "    # pattern 3 - remove all words that begin with 'obsess' from the sentence and return the result\n",
    "    pattern = r'obsess|compuls(.*?)[^a-zA-Z]' # obsess followed by any number of characters then ending in any character that is not a letter\n",
    "    # replace those pattern matches with '' (nothing)\n",
    "    text =  re.sub(pattern, '', text) # replace the pattern matches with '' (nothing)\n",
    "    return text # return the result\n",
    "\n",
    "# apply the censor_words function to the selftext column of each dataframe\n",
    "df_ocd['selftext'] = df_ocd['selftext'].apply(censor_words)\n",
    "\n",
    "# remove punctuation\n",
    "df_ocd['selftext'] = df_ocd['selftext'].str.replace('[^\\w\\s]','')\n",
    "# remove numbers\n",
    "df_ocd['selftext'] = df_ocd['selftext'].str.replace('\\d+', '')\n",
    "# remove whitespace\n",
    "df_ocd['selftext'] = df_ocd['selftext'].str.replace('\\s+', ' ')\n",
    "\n",
    "# do the same for the autism dataframe\n",
    "df_autism['selftext'] = df_autism['selftext'].apply(censor_words)\n",
    "# remove punctuation\n",
    "df_autism['selftext'] = df_autism['selftext'].str.replace('[^\\w\\s]','')\n",
    "# remove numbers\n",
    "df_autism['selftext'] = df_autism['selftext'].str.replace('\\d+', '')\n",
    "# remove whitespace\n",
    "df_autism['selftext'] = df_autism['selftext'].str.replace('\\s+', ' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words from posts that are in the cancel_words list. There are regex patterns in the cancel_words list so we need to use the `regex=True` parameter\n",
    "\n",
    "# then remove double spaces\n",
    "df_ocd['selftext'] = df_ocd['selftext'].str.replace('  ', ' ')\n",
    "df_autism['selftext'] = df_autism['selftext'].str.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_51534/2826576949.py:18: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  df_reddit = pd.concat([df_reddit, longer_df], axis=0)\n"
     ]
    }
   ],
   "source": [
    "# make a new dataframe called df_reddit that combines the two dataframes\n",
    "\n",
    "df_reddit = pd.DataFrame(columns=df_ocd.columns)\n",
    "# what is the length of the shorter dataframe?\n",
    "if len(df_ocd) < len(df_autism): # if the OCD dataframe is shorter\n",
    "    shorter_df = df_ocd # set the shorter dataframe to the OCD dataframe\n",
    "    longer_df = df_autism # set the longer dataframe to the Autism dataframe\n",
    "    df_reddit.append()\n",
    "else: # if the Autism dataframe is shorter\n",
    "    shorter_df = df_autism\n",
    "    longer_df = df_ocd\n",
    "\n",
    "# add the shorter dataframe to the new dataframe using concat\n",
    "df_reddit = pd.concat([df_reddit, shorter_df], axis=0)\n",
    "# shorten the longer dataframe to the length of the shorter dataframe\n",
    "longer_df = longer_df.head(len(shorter_df))\n",
    "# add the shortened longer dataframe to the new dataframe using concat\n",
    "df_reddit = pd.concat([df_reddit, longer_df], axis=0)\n",
    "\n",
    "# reset the index\n",
    "df_reddit = df_reddit.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the new dataframe: (18042, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>target</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>squeakybeeper</td>\n",
       "      <td>1571153444</td>\n",
       "      <td>di9ivs</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>is it ok for me to tell people that im probabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YourLocalNewsSource</td>\n",
       "      <td>1581513407</td>\n",
       "      <td>f2r2aq</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>sister triggered my mental illnesses i posted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transcruples</td>\n",
       "      <td>1534891375</td>\n",
       "      <td>997u3g</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>does anyone else have pure spiking in regards ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zettoraius</td>\n",
       "      <td>1579377981</td>\n",
       "      <td>eqljtc</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>please help i do not feel well i ended up havi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EPJ327</td>\n",
       "      <td>1554538241</td>\n",
       "      <td>ba27l9</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>help i need your advice hello people of r i ne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author created_utc      id is_original_content target  \\\n",
       "0        squeakybeeper  1571153444  di9ivs               False      0   \n",
       "1  YourLocalNewsSource  1581513407  f2r2aq               False      1   \n",
       "2         Transcruples  1534891375  997u3g               False      1   \n",
       "3           Zettoraius  1579377981  eqljtc               False      1   \n",
       "4               EPJ327  1554538241  ba27l9               False      0   \n",
       "\n",
       "                                            selftext  \n",
       "0  is it ok for me to tell people that im probabl...  \n",
       "1  sister triggered my mental illnesses i posted ...  \n",
       "2  does anyone else have pure spiking in regards ...  \n",
       "3  please help i do not feel well i ended up havi...  \n",
       "4  help i need your advice hello people of r i ne...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle the dataframe\n",
    "df_reddit = df_reddit.sample(frac=1).reset_index(drop=True)\n",
    "# check the dimensions of the new dataframe\n",
    "print(f'Dimensions of the new dataframe: {df_reddit.shape}')\n",
    "df_reddit.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts for OCD: 9021\n",
      "Number of posts for Autism: 9021\n"
     ]
    }
   ],
   "source": [
    "# double check that the number of posts for each subreddit is the same\n",
    "print(f'Number of posts for OCD: {len(df_reddit[df_reddit.target == 1])}')\n",
    "print(f'Number of posts for Autism: {len(df_reddit[df_reddit.target == 0])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Cleaning Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>target</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>mindkingdom</td>\n",
       "      <td>1527063603</td>\n",
       "      <td>8lhprn</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>thoughts that things could get corrupted or br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>SteelSlayer7</td>\n",
       "      <td>1527074859</td>\n",
       "      <td>8lil9v</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>intrusive thoughts devaluing myself i am plagu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author  created_utc      id is_original_content  target  \\\n",
       "2016   mindkingdom   1527063603  8lhprn               False       1   \n",
       "2020  SteelSlayer7   1527074859  8lil9v               False       1   \n",
       "\n",
       "                                               selftext  \n",
       "2016  thoughts that things could get corrupted or br...  \n",
       "2020  intrusive thoughts devaluing myself i am plagu...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ocd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>target</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Equadex</td>\n",
       "      <td>1546777579</td>\n",
       "      <td>ad56om</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>how can people be considered equals if they ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>936R</td>\n",
       "      <td>1546786707</td>\n",
       "      <td>ad68am</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>questions for females with hi there im looking...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author  created_utc      id  is_original_content  target  \\\n",
       "0  Equadex   1546777579  ad56om                False       0   \n",
       "5     936R   1546786707  ad68am                False       0   \n",
       "\n",
       "                                            selftext  \n",
       "0  how can people be considered equals if they ar...  \n",
       "5  questions for females with hi there im looking...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_autism.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find any of the medications in the selftext column that are in the data/drug_info.csv file under the Medication Name column and replace them with ' ' (empty string)\n",
    "drug_info = pd.read_csv('../data/drug_info.csv')\n",
    "drug_info['Medication Name'] = drug_info['Medication Name'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of medications: 3047\n"
     ]
    }
   ],
   "source": [
    "# create a list of the medications\n",
    "medications = drug_info['Medication Name'].tolist()\n",
    "print(f'Number of medications: {len(medications)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist. Creating it now. Before meds removed from selftext the length of the dataframe is:  18042\n"
     ]
    }
   ],
   "source": [
    "# remove the words from the selftext column that are in the medications list\n",
    "import os\n",
    "# if the file does not already exist, create it\n",
    "if os.path.exists('../data/cleaned_reddit.csv'):\n",
    "    # load the file\n",
    "    df_reddit = pd.read_csv('../data/cleaned_reddit.csv')\n",
    "else:\n",
    "    print('File does not exist. Creating it now. Before meds removed from selftext the length of the dataframe is: ', len(df_reddit))\n",
    "    df_reddit['selftext'] = df_reddit['selftext'].str.replace('|'.join(medications), ' ', regex=True)\n",
    "    print(f' Removed {len(medications)} medications from the selftext column')\n",
    "    # save the dataframe to a csv file\n",
    "    df_reddit.to_csv('../data/cleaned_reddit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_85018/1545294563.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace('[^\\w\\s]','')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_85018/1545294563.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace('\\d+', '')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_85018/1545294563.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_85018/1545294563.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\n', ' ')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_85018/1545294563.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'http\\S+', '')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_85018/1545294563.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'<.*?>', '')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_85018/1545294563.py:17: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+', ' ')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_85018/1545294563.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'^\\s+', '')\n",
      "/var/folders/34/d1tlq3k91hb0lj6x90xpzb4r0000gn/T/ipykernel_85018/1545294563.py:21: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+$', '')\n"
     ]
    }
   ],
   "source": [
    "# Now we want to clean the text in the self text column\n",
    "# remove punctuation\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace('[^\\w\\s]','')\n",
    "# remove numbers\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace('\\d+', '')\n",
    "# remove double spaces\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace('  ', ' ')\n",
    "# remove single characters\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n",
    "# remove newlines\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\n', ' ')\n",
    "# remove urls\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'http\\S+', '')\n",
    "# remove html tags\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'<.*?>', '')\n",
    "# remove extra spaces\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+', ' ')\n",
    "# remove extra spaces at the beginning of the string\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'^\\s+', '')\n",
    "# remove extra spaces at the end of the string\n",
    "df_reddit['selftext'] = df_reddit['selftext'].str.replace(r'\\s+$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'is this my issues with other peoples breathe is ruining kissing and sex for me please help both of my long term relationships ive slowly developed an issue with the persons breathe the first time i thought maybe it was just that guy now its happening a second time i dont know what to do and it seems like no one else suffers with this i never noticed any kind of bad breathe in my bf the first few months we were dating then it started creeping up on me i spoke with him about it and being the wonderful partner he is taking steps to fix it on his end he brushes and scrapes his tongue and uses this special non alcoholic mouthwash still the bad breath he doesnt floss and wont because of the pain and refuses to acknowledge he could have tonsil stones i dont see them anyway its like his bad breath is coming from deep inside though he drinks a lot of beer and probiotic drinks im at a loss im very attracted to this man and love him dearly why do i keep having this issue how can i fix it my previous partner it got to the point where i couldnt even kiss him beyond a peck if he didnt have gum or a mint in his mouth i dont want it to get to this point im really stressed about this what should i do is this something cognitive behavioral therapy can fix ive thought about it and while i do think my bf has slightly worse breath than average i do think that no matter the person i would eventually start to feel this way because peoples used up hot air coming out of their mouths with all their smells triggers my very sensitive system and i become disgusted or neaseaus at the thought of it'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [291], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(df_ocd[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselftext\u001b[39m\u001b[38;5;124m'\u001b[39m], df_ocd[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Predict the model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1138\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     _dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32]\n\u001b[0;32m-> 1138\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1139\u001b[0m     X,\n\u001b[1;32m   1140\u001b[0m     y,\n\u001b[1;32m   1141\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1142\u001b[0m     dtype\u001b[39m=\u001b[39;49m_dtype,\n\u001b[1;32m   1143\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1144\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49msolver \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m [\u001b[39m\"\u001b[39;49m\u001b[39mliblinear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msag\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1145\u001b[0m )\n\u001b[1;32m   1146\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages/sklearn/base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    595\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages/sklearn/utils/validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1070\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1071\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[0;32m-> 1074\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1075\u001b[0m     X,\n\u001b[1;32m   1076\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1077\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1078\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1079\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1080\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1081\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1082\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1083\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1084\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1085\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1086\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1087\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1088\u001b[0m )\n\u001b[1;32m   1090\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1092\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages/sklearn/utils/validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    854\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    855\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    857\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    858\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    860\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/EmersonWriter/envs/master_env_temp/lib/python3.9/site-packages/pandas/core/series.py:893\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    847\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \u001b[39m    Return the values as a NumPy array.\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39m          dtype='datetime64[ns]')\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'is this my issues with other peoples breathe is ruining kissing and sex for me please help both of my long term relationships ive slowly developed an issue with the persons breathe the first time i thought maybe it was just that guy now its happening a second time i dont know what to do and it seems like no one else suffers with this i never noticed any kind of bad breathe in my bf the first few months we were dating then it started creeping up on me i spoke with him about it and being the wonderful partner he is taking steps to fix it on his end he brushes and scrapes his tongue and uses this special non alcoholic mouthwash still the bad breath he doesnt floss and wont because of the pain and refuses to acknowledge he could have tonsil stones i dont see them anyway its like his bad breath is coming from deep inside though he drinks a lot of beer and probiotic drinks im at a loss im very attracted to this man and love him dearly why do i keep having this issue how can i fix it my previous partner it got to the point where i couldnt even kiss him beyond a peck if he didnt have gum or a mint in his mouth i dont want it to get to this point im really stressed about this what should i do is this something cognitive behavioral therapy can fix ive thought about it and while i do think my bf has slightly worse breath than average i do think that no matter the person i would eventually start to feel this way because peoples used up hot air coming out of their mouths with all their smells triggers my very sensitive system and i become disgusted or neaseaus at the thought of it'"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Define the Train and Validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_ocd['selftext'], df_ocd['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict the model\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "# Print the accuracy score\n",
    "print(\"Accuracy: \", accuracy_score(y_val, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix: \", confusion_matrix(y_val, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "print(\"Classification Report: \", classification_report(y_val, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.title('ROC Curve')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel base (Python 3.9.6) is not usable. Check the Jupyter output tab for more information. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "adaboost = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "adaboost.fit(df_ocd.drop(columns=('self_text', 'author'), axis=1), df_ocd.target)\n",
    "accuracy = accuracy_score(adaboost.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(df_ocd.drop(columns=('self_text', 'author'), axis=1), df_ocd.target)\n",
    "accuracy = accuracy_score(dt.predict(df_ocd.drop(columns=('self_text', 'author'), axis=1)), df_ocd.target)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_ocd.drop(columns=('self_text', 'author'), axis=1).astype(str))\n",
    "tf_vocab = Counter().most_common(len(stop_words)+5)\n",
    "for i in range(10):\n",
    "    print(i, len(set(tokens)))\n",
    "    for j, k in enumerate(tf_vocab):\n",
    "        if k >= 5:\n",
    "            break\n",
    "        X_k = np.array(np.where(X == k))\n",
    "        X_j = np.zeros((0, 0))\n",
    "        for x in X_k:\n",
    "            X_j += 1 * x\n",
    "        X = np.vstack((X, X_j))\n",
    "    print(i + 1, len(set(tokens)))\n",
    "    for j, k in enumerate(tf_vocab):\n",
    "        if k > 4:\n",
    "            break\n",
    "        X_k = np.array(np.where(X == k))\n",
    "        X_j = np.zeros((0, 0))\n",
    "        for x in X_k:\n",
    "            X_j += 1 * x\n",
    "        X = np.hstack((X, X_j))\n",
    "        print(i + 2, len(set(tokens)))\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "x = df_ocd.target.values\n",
    "y = lr.predict(x)\n",
    "bar_width = .35\n",
    "color_map = sns.light_palette(\"Greens\", 10)\n",
    "colors = color_map.as_hex()\n",
    "labels = list(range(len(df_ocd.target.index)))\n",
    "rects = ax.bar(labels, y, width=bar_width, label='Predicted Probability', edgecolor=None, align=\"center\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_yticks(np.arange(0, 1.05, .25))\n",
    "ax.set_xticklabels(list(df_ocd.target.index))\n",
    "ax.invert_yaxis()\n",
    "ax.set_title(\"Logistic Regression Predictions\")\n",
    "fig = plt.gcf()\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "confusion_matrix = pd.crosstab(df_ocd.target, df_ocd.prediction)\n",
    "cm = confusion_matrix(df_ocd.target, df_ocd.prediction)\n",
    "num_classes = cm.sum(1).max()+1\n",
    "class_names = list(range(num_classes))\n",
    "row_positions = np.argsort(-df_ocd.target)\n",
    "col_indices = np.argpartition(df_ocd.target, -df_ocd.target.size-1)\n",
    "for row_number, col_name in zip(row_positions, class_names):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(2, num_classes, row_number)\n",
    "    cnt = conf_matrix.iloc.get_value(row_position=row_number, column_label=col_name)\n",
    "    ax.barh(class_names, cnt)\n",
    "    ax.set_ylabel(col_name)\n",
    "    ax.set_xlim(0, num_classes)\n",
    "    ax.set_xticks(np.arange(0, num_classes, 1))\n",
    "    ax.grid()\n",
    "    plt.savefig('images/logreg_confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the Dendogram\n",
    "linkage_obj = linkage(distance_func=euclidean)\n",
    "dendro = linkage_obj.apply(X)\n",
    "plt.figure()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('master_env_temp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "666dfdc1c3434f60b2f64fdb13e0b185df8f5878ddae1a39c75e1eb2af091f89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
